{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the performance of the different models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models with normalizing flows: MADE MAF RealNVP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf normalizing_flows\n",
    "!rm -rf results/normalizing_flows\n",
    "!git clone https://github.com/kamenbliznashki/normalizing_flows.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move our simulation datasets into the repo normalizing_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = ['circle', 'torus', 'involute']\n",
    "\n",
    "for data_name in data_names:\n",
    "    source_file1 = os.path.join('datasets', data_name + '.py')\n",
    "    source_file2 = os.path.join('datasets', data_name, 'B.csv')\n",
    "    destination_folder =  os.path.join('normalizing_flows', 'datasets')\n",
    "    shutil.copy2(source_file1, destination_folder)\n",
    "    os.makedirs(os.path.join(destination_folder, data_name))\n",
    "    shutil.copy2(source_file2, os.path.join(destination_folder, data_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revise some lines in the file data.py and maf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fileinput\n",
    "\n",
    "file_path = os.path.join('normalizing_flows', 'data.py')\n",
    "\n",
    "new_line_65 = '    elif dataset_name in [\\'TOY\\', \\'MOONS\\', \\'CIRCLE\\', \\'TORUS\\', \\'INVOLUTE\\']:  # use own constructors\\n'\n",
    "new_line_66=  '        train_dataset = load_dataset(dataset_name)()\\n'\n",
    "new_line_67=  '        test_dataset = load_dataset(dataset_name)()\\n'\n",
    "new_line_106= '    kwargs = {\\'num_workers\\': 1, \\'pin_memory\\': True} if device.type == \\'cuda\\' else {}\\n'\n",
    "\n",
    "# Replace the line in the file\n",
    "with fileinput.FileInput(file_path, inplace=True) as file:\n",
    "    for i, line in enumerate(file, 1):\n",
    "        if i == 65:\n",
    "            print(new_line_65, end='')\n",
    "        elif i == 66:\n",
    "            print(new_line_66, end='')\n",
    "        elif i == 67:\n",
    "            print(new_line_67, end='')\n",
    "        elif i == 106:\n",
    "            print(new_line_106, end='')\n",
    "        else:\n",
    "            print(line, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './normalizing_flows/maf.py'\n",
    "start_line = 740\n",
    "\n",
    "replacement_code = '''\n",
    "    if args.generate:\n",
    "        if args.dataset == 'TOY':\n",
    "            base_dist = train_dataloader.dataset.base_dist\n",
    "            plot_sample_and_density(model, base_dist, args, ranges_density=[[-15,4],[-3,3]], ranges_sample=[[-1.5,1.5],[-3,3]])\n",
    "        elif args.dataset in ['CIRCLE', 'TORUS', 'INVOLUTE']:\n",
    "            u = model.base_dist.sample((10000, args.n_components)).squeeze()\n",
    "            samples, _ = model.inverse(u)\n",
    "            samples = samples.data.cpu().numpy()\n",
    "            import numpy as np\n",
    "            import os\n",
    "            save_dir = os.path.join(args.output_dir,args.dataset+'_sample.csv')\n",
    "\n",
    "            np.savetxt(save_dir, samples, delimiter=',')\n",
    "'''\n",
    "\n",
    "# Rewrite the specified part of the file\n",
    "with fileinput.FileInput(file_path, inplace=True) as f:\n",
    "    for line in f:\n",
    "        if f.filelineno() >= start_line:\n",
    "            # Print the replacement code instead of the original line\n",
    "            print(replacement_code)\n",
    "            break\n",
    "        elif f.filelineno() == 667:\n",
    "            print('    args.output_dir = os.path.join(\\'./results/\\',args.model)\\n', end='')\n",
    "        else:\n",
    "            # Print the original line as it is\n",
    "            print(line, end='')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'CIRCLE',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': False,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 2,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 2,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'made',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 200,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': False,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/made',\n",
      " 'restore_file': None,\n",
      " 'results_file': './results/made\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 0,\n",
      " 'train': True}\n",
      "MADE(\n",
      "  (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "  (net): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch   0 / 200, step    0 / 100; loss 2.3699\n",
      "Evaluate (epoch 0) -- logp(x) = -2.186 +/- 0.004\n",
      "epoch   1 / 200, step    0 / 100; loss 2.1931\n",
      "Evaluate (epoch 1) -- logp(x) = -2.138 +/- 0.005\n",
      "epoch   2 / 200, step    0 / 100; loss 2.1203\n",
      "Evaluate (epoch 2) -- logp(x) = -2.095 +/- 0.006\n",
      "epoch   3 / 200, step    0 / 100; loss 2.0780\n",
      "Evaluate (epoch 3) -- logp(x) = -2.059 +/- 0.006\n",
      "epoch   4 / 200, step    0 / 100; loss 2.1022\n",
      "Evaluate (epoch 4) -- logp(x) = -2.035 +/- 0.007\n",
      "epoch   5 / 200, step    0 / 100; loss 2.0227\n",
      "Evaluate (epoch 5) -- logp(x) = -2.021 +/- 0.007\n",
      "epoch   6 / 200, step    0 / 100; loss 2.0215\n",
      "Evaluate (epoch 6) -- logp(x) = -2.011 +/- 0.007\n",
      "epoch   7 / 200, step    0 / 100; loss 2.0056\n",
      "Evaluate (epoch 7) -- logp(x) = -2.002 +/- 0.008\n",
      "epoch   8 / 200, step    0 / 100; loss 2.0044\n",
      "Evaluate (epoch 8) -- logp(x) = -1.993 +/- 0.008\n",
      "epoch   9 / 200, step    0 / 100; loss 1.9919\n",
      "Evaluate (epoch 9) -- logp(x) = -1.986 +/- 0.008\n",
      "epoch  10 / 200, step    0 / 100; loss 2.0116\n",
      "Evaluate (epoch 10) -- logp(x) = -1.978 +/- 0.008\n",
      "epoch  11 / 200, step    0 / 100; loss 1.9573\n",
      "Evaluate (epoch 11) -- logp(x) = -1.971 +/- 0.008\n",
      "epoch  12 / 200, step    0 / 100; loss 1.8737\n",
      "Evaluate (epoch 12) -- logp(x) = -1.964 +/- 0.008\n",
      "epoch  13 / 200, step    0 / 100; loss 1.9490\n",
      "Evaluate (epoch 13) -- logp(x) = -1.958 +/- 0.008\n",
      "epoch  14 / 200, step    0 / 100; loss 1.9308\n",
      "Evaluate (epoch 14) -- logp(x) = -1.953 +/- 0.008\n",
      "epoch  15 / 200, step    0 / 100; loss 1.9377\n",
      "Evaluate (epoch 15) -- logp(x) = -1.947 +/- 0.008\n",
      "epoch  16 / 200, step    0 / 100; loss 1.9651\n",
      "Evaluate (epoch 16) -- logp(x) = -1.942 +/- 0.008\n",
      "epoch  17 / 200, step    0 / 100; loss 1.9247\n",
      "Evaluate (epoch 17) -- logp(x) = -1.938 +/- 0.008\n",
      "epoch  18 / 200, step    0 / 100; loss 1.9220\n",
      "Evaluate (epoch 18) -- logp(x) = -1.932 +/- 0.008\n",
      "epoch  19 / 200, step    0 / 100; loss 1.8415\n",
      "Evaluate (epoch 19) -- logp(x) = -1.928 +/- 0.008\n",
      "epoch  20 / 200, step    0 / 100; loss 1.8693\n",
      "Evaluate (epoch 20) -- logp(x) = -1.925 +/- 0.008\n",
      "epoch  21 / 200, step    0 / 100; loss 1.9764\n",
      "Evaluate (epoch 21) -- logp(x) = -1.921 +/- 0.008\n",
      "epoch  22 / 200, step    0 / 100; loss 2.0007\n",
      "Evaluate (epoch 22) -- logp(x) = -1.917 +/- 0.008\n",
      "epoch  23 / 200, step    0 / 100; loss 1.8828\n",
      "Evaluate (epoch 23) -- logp(x) = -1.914 +/- 0.008\n",
      "epoch  24 / 200, step    0 / 100; loss 1.8877\n",
      "Evaluate (epoch 24) -- logp(x) = -1.912 +/- 0.008\n",
      "epoch  25 / 200, step    0 / 100; loss 1.9362\n",
      "Evaluate (epoch 25) -- logp(x) = -1.908 +/- 0.008\n",
      "epoch  26 / 200, step    0 / 100; loss 1.8316\n",
      "Evaluate (epoch 26) -- logp(x) = -1.905 +/- 0.008\n",
      "epoch  27 / 200, step    0 / 100; loss 1.9181\n",
      "Evaluate (epoch 27) -- logp(x) = -1.903 +/- 0.008\n",
      "epoch  28 / 200, step    0 / 100; loss 1.9484\n",
      "Evaluate (epoch 28) -- logp(x) = -1.900 +/- 0.008\n",
      "epoch  29 / 200, step    0 / 100; loss 1.9162\n",
      "Evaluate (epoch 29) -- logp(x) = -1.898 +/- 0.008\n",
      "epoch  30 / 200, step    0 / 100; loss 1.9120\n",
      "Evaluate (epoch 30) -- logp(x) = -1.897 +/- 0.008\n",
      "epoch  31 / 200, step    0 / 100; loss 1.9356\n",
      "Evaluate (epoch 31) -- logp(x) = -1.894 +/- 0.008\n",
      "epoch  32 / 200, step    0 / 100; loss 1.8566\n",
      "Evaluate (epoch 32) -- logp(x) = -1.892 +/- 0.008\n",
      "epoch  33 / 200, step    0 / 100; loss 1.9112\n",
      "Evaluate (epoch 33) -- logp(x) = -1.890 +/- 0.008\n",
      "epoch  34 / 200, step    0 / 100; loss 1.8688\n",
      "Evaluate (epoch 34) -- logp(x) = -1.889 +/- 0.008\n",
      "epoch  35 / 200, step    0 / 100; loss 1.9023\n",
      "Evaluate (epoch 35) -- logp(x) = -1.888 +/- 0.008\n",
      "epoch  36 / 200, step    0 / 100; loss 1.8968\n",
      "Evaluate (epoch 36) -- logp(x) = -1.886 +/- 0.008\n",
      "epoch  37 / 200, step    0 / 100; loss 1.8913\n",
      "Evaluate (epoch 37) -- logp(x) = -1.885 +/- 0.008\n",
      "epoch  38 / 200, step    0 / 100; loss 1.8820\n",
      "Evaluate (epoch 38) -- logp(x) = -1.884 +/- 0.008\n",
      "epoch  39 / 200, step    0 / 100; loss 1.8651\n",
      "Evaluate (epoch 39) -- logp(x) = -1.883 +/- 0.008\n",
      "epoch  40 / 200, step    0 / 100; loss 1.8955\n",
      "Evaluate (epoch 40) -- logp(x) = -1.882 +/- 0.008\n",
      "epoch  41 / 200, step    0 / 100; loss 1.8728\n",
      "Evaluate (epoch 41) -- logp(x) = -1.881 +/- 0.008\n",
      "epoch  42 / 200, step    0 / 100; loss 1.9172\n",
      "Evaluate (epoch 42) -- logp(x) = -1.880 +/- 0.008\n",
      "epoch  43 / 200, step    0 / 100; loss 1.8649\n",
      "Evaluate (epoch 43) -- logp(x) = -1.880 +/- 0.008\n",
      "epoch  44 / 200, step    0 / 100; loss 1.8820\n",
      "Evaluate (epoch 44) -- logp(x) = -1.878 +/- 0.008\n",
      "epoch  45 / 200, step    0 / 100; loss 1.8646\n",
      "Evaluate (epoch 45) -- logp(x) = -1.877 +/- 0.008\n",
      "epoch  46 / 200, step    0 / 100; loss 1.9523\n",
      "Evaluate (epoch 46) -- logp(x) = -1.877 +/- 0.008\n",
      "epoch  47 / 200, step    0 / 100; loss 1.8984\n",
      "Evaluate (epoch 47) -- logp(x) = -1.876 +/- 0.008\n",
      "epoch  48 / 200, step    0 / 100; loss 1.9138\n",
      "Evaluate (epoch 48) -- logp(x) = -1.876 +/- 0.008\n",
      "epoch  49 / 200, step    0 / 100; loss 1.8310\n",
      "Evaluate (epoch 49) -- logp(x) = -1.877 +/- 0.008\n",
      "epoch  50 / 200, step    0 / 100; loss 1.9272\n",
      "Evaluate (epoch 50) -- logp(x) = -1.874 +/- 0.008\n",
      "epoch  51 / 200, step    0 / 100; loss 1.9025\n",
      "Evaluate (epoch 51) -- logp(x) = -1.875 +/- 0.008\n",
      "epoch  52 / 200, step    0 / 100; loss 1.8650\n",
      "Evaluate (epoch 52) -- logp(x) = -1.873 +/- 0.008\n",
      "epoch  53 / 200, step    0 / 100; loss 1.8420\n",
      "Evaluate (epoch 53) -- logp(x) = -1.874 +/- 0.009\n",
      "epoch  54 / 200, step    0 / 100; loss 1.9092\n",
      "Evaluate (epoch 54) -- logp(x) = -1.872 +/- 0.009\n",
      "epoch  55 / 200, step    0 / 100; loss 1.8210\n",
      "Evaluate (epoch 55) -- logp(x) = -1.872 +/- 0.009\n",
      "epoch  56 / 200, step    0 / 100; loss 1.8837\n",
      "Evaluate (epoch 56) -- logp(x) = -1.871 +/- 0.009\n",
      "epoch  57 / 200, step    0 / 100; loss 1.8168\n",
      "Evaluate (epoch 57) -- logp(x) = -1.872 +/- 0.009\n",
      "epoch  58 / 200, step    0 / 100; loss 1.8915\n",
      "Evaluate (epoch 58) -- logp(x) = -1.871 +/- 0.009\n",
      "epoch  59 / 200, step    0 / 100; loss 1.8434\n",
      "Evaluate (epoch 59) -- logp(x) = -1.870 +/- 0.009\n",
      "epoch  60 / 200, step    0 / 100; loss 1.9164\n",
      "Evaluate (epoch 60) -- logp(x) = -1.870 +/- 0.009\n",
      "epoch  61 / 200, step    0 / 100; loss 1.8477\n",
      "Evaluate (epoch 61) -- logp(x) = -1.870 +/- 0.009\n",
      "epoch  62 / 200, step    0 / 100; loss 1.8563\n",
      "Evaluate (epoch 62) -- logp(x) = -1.870 +/- 0.009\n",
      "epoch  63 / 200, step    0 / 100; loss 1.8248\n",
      "Evaluate (epoch 63) -- logp(x) = -1.871 +/- 0.009\n",
      "epoch  64 / 200, step    0 / 100; loss 1.8869\n",
      "Evaluate (epoch 64) -- logp(x) = -1.869 +/- 0.009\n",
      "epoch  65 / 200, step    0 / 100; loss 1.9064\n",
      "Evaluate (epoch 65) -- logp(x) = -1.869 +/- 0.009\n",
      "epoch  66 / 200, step    0 / 100; loss 1.8344\n",
      "Evaluate (epoch 66) -- logp(x) = -1.868 +/- 0.009\n",
      "epoch  67 / 200, step    0 / 100; loss 1.8795\n",
      "Evaluate (epoch 67) -- logp(x) = -1.868 +/- 0.009\n",
      "epoch  68 / 200, step    0 / 100; loss 1.8087\n",
      "Evaluate (epoch 68) -- logp(x) = -1.867 +/- 0.009\n",
      "epoch  69 / 200, step    0 / 100; loss 1.9143\n",
      "Evaluate (epoch 69) -- logp(x) = -1.867 +/- 0.009\n",
      "epoch  70 / 200, step    0 / 100; loss 1.8854\n",
      "Evaluate (epoch 70) -- logp(x) = -1.867 +/- 0.009\n",
      "epoch  71 / 200, step    0 / 100; loss 1.8483\n",
      "Evaluate (epoch 71) -- logp(x) = -1.866 +/- 0.009\n",
      "epoch  72 / 200, step    0 / 100; loss 1.8999\n",
      "Evaluate (epoch 72) -- logp(x) = -1.867 +/- 0.009\n",
      "epoch  73 / 200, step    0 / 100; loss 1.8951\n",
      "Evaluate (epoch 73) -- logp(x) = -1.866 +/- 0.009\n",
      "epoch  74 / 200, step    0 / 100; loss 1.8134\n",
      "Evaluate (epoch 74) -- logp(x) = -1.868 +/- 0.009\n",
      "epoch  75 / 200, step    0 / 100; loss 1.8127\n",
      "Evaluate (epoch 75) -- logp(x) = -1.868 +/- 0.009\n",
      "epoch  76 / 200, step    0 / 100; loss 1.8679\n",
      "Evaluate (epoch 76) -- logp(x) = -1.865 +/- 0.009\n",
      "epoch  77 / 200, step    0 / 100; loss 1.9311\n",
      "Evaluate (epoch 77) -- logp(x) = -1.866 +/- 0.009\n",
      "epoch  78 / 200, step    0 / 100; loss 1.8452\n",
      "Evaluate (epoch 78) -- logp(x) = -1.866 +/- 0.009\n",
      "epoch  79 / 200, step    0 / 100; loss 1.8025\n",
      "Evaluate (epoch 79) -- logp(x) = -1.865 +/- 0.009\n",
      "epoch  80 / 200, step    0 / 100; loss 1.8123\n",
      "Evaluate (epoch 80) -- logp(x) = -1.865 +/- 0.009\n",
      "epoch  81 / 200, step    0 / 100; loss 1.9078\n",
      "Evaluate (epoch 81) -- logp(x) = -1.865 +/- 0.009\n",
      "epoch  82 / 200, step    0 / 100; loss 1.8967\n",
      "Evaluate (epoch 82) -- logp(x) = -1.865 +/- 0.009\n",
      "epoch  83 / 200, step    0 / 100; loss 1.8728\n",
      "Evaluate (epoch 83) -- logp(x) = -1.864 +/- 0.009\n",
      "epoch  84 / 200, step    0 / 100; loss 1.8635\n",
      "Evaluate (epoch 84) -- logp(x) = -1.864 +/- 0.009\n",
      "epoch  85 / 200, step    0 / 100; loss 1.9013\n",
      "Evaluate (epoch 85) -- logp(x) = -1.864 +/- 0.009\n",
      "epoch  86 / 200, step    0 / 100; loss 1.9259\n",
      "Evaluate (epoch 86) -- logp(x) = -1.864 +/- 0.009\n",
      "epoch  87 / 200, step    0 / 100; loss 1.9205\n",
      "Evaluate (epoch 87) -- logp(x) = -1.864 +/- 0.009\n",
      "epoch  88 / 200, step    0 / 100; loss 1.8227\n",
      "Evaluate (epoch 88) -- logp(x) = -1.864 +/- 0.009\n",
      "epoch  89 / 200, step    0 / 100; loss 1.7810\n",
      "Evaluate (epoch 89) -- logp(x) = -1.864 +/- 0.009\n",
      "epoch  90 / 200, step    0 / 100; loss 1.8575\n",
      "Evaluate (epoch 90) -- logp(x) = -1.865 +/- 0.009\n",
      "epoch  91 / 200, step    0 / 100; loss 1.8626\n",
      "Evaluate (epoch 91) -- logp(x) = -1.863 +/- 0.009\n",
      "epoch  92 / 200, step    0 / 100; loss 1.8442\n",
      "Evaluate (epoch 92) -- logp(x) = -1.864 +/- 0.009\n",
      "epoch  93 / 200, step    0 / 100; loss 1.9035\n",
      "Evaluate (epoch 93) -- logp(x) = -1.863 +/- 0.009\n",
      "epoch  94 / 200, step    0 / 100; loss 1.8371\n",
      "Evaluate (epoch 94) -- logp(x) = -1.863 +/- 0.009\n",
      "epoch  95 / 200, step    0 / 100; loss 1.8581\n",
      "Evaluate (epoch 95) -- logp(x) = -1.863 +/- 0.009\n",
      "epoch  96 / 200, step    0 / 100; loss 1.8711\n",
      "Evaluate (epoch 96) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch  97 / 200, step    0 / 100; loss 1.8540\n",
      "Evaluate (epoch 97) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch  98 / 200, step    0 / 100; loss 1.8597\n",
      "Evaluate (epoch 98) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch  99 / 200, step    0 / 100; loss 1.8754\n",
      "Evaluate (epoch 99) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 100 / 200, step    0 / 100; loss 1.8623\n",
      "Evaluate (epoch 100) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 101 / 200, step    0 / 100; loss 1.8621\n",
      "Evaluate (epoch 101) -- logp(x) = -1.864 +/- 0.009\n",
      "epoch 102 / 200, step    0 / 100; loss 1.8850\n",
      "Evaluate (epoch 102) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 103 / 200, step    0 / 100; loss 1.9301\n",
      "Evaluate (epoch 103) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 104 / 200, step    0 / 100; loss 1.9344\n",
      "Evaluate (epoch 104) -- logp(x) = -1.863 +/- 0.009\n",
      "epoch 105 / 200, step    0 / 100; loss 1.7550\n",
      "Evaluate (epoch 105) -- logp(x) = -1.863 +/- 0.009\n",
      "epoch 106 / 200, step    0 / 100; loss 1.9202\n",
      "Evaluate (epoch 106) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 107 / 200, step    0 / 100; loss 1.8959\n",
      "Evaluate (epoch 107) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 108 / 200, step    0 / 100; loss 1.8128\n",
      "Evaluate (epoch 108) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 109 / 200, step    0 / 100; loss 1.8423\n",
      "Evaluate (epoch 109) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 110 / 200, step    0 / 100; loss 1.8929\n",
      "Evaluate (epoch 110) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 111 / 200, step    0 / 100; loss 1.7922\n",
      "Evaluate (epoch 111) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 112 / 200, step    0 / 100; loss 1.8656\n",
      "Evaluate (epoch 112) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 113 / 200, step    0 / 100; loss 1.9143\n",
      "Evaluate (epoch 113) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 114 / 200, step    0 / 100; loss 1.8817\n",
      "Evaluate (epoch 114) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 115 / 200, step    0 / 100; loss 1.8210\n",
      "Evaluate (epoch 115) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 116 / 200, step    0 / 100; loss 1.9025\n",
      "Evaluate (epoch 116) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 117 / 200, step    0 / 100; loss 1.8481\n",
      "Evaluate (epoch 117) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 118 / 200, step    0 / 100; loss 1.7964\n",
      "Evaluate (epoch 118) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 119 / 200, step    0 / 100; loss 1.8767\n",
      "Evaluate (epoch 119) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 120 / 200, step    0 / 100; loss 1.8446\n",
      "Evaluate (epoch 120) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 121 / 200, step    0 / 100; loss 1.8280\n",
      "Evaluate (epoch 121) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 122 / 200, step    0 / 100; loss 1.8253\n",
      "Evaluate (epoch 122) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 123 / 200, step    0 / 100; loss 1.8919\n",
      "Evaluate (epoch 123) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 124 / 200, step    0 / 100; loss 1.9170\n",
      "Evaluate (epoch 124) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 125 / 200, step    0 / 100; loss 1.9264\n",
      "Evaluate (epoch 125) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 126 / 200, step    0 / 100; loss 1.8852\n",
      "Evaluate (epoch 126) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 127 / 200, step    0 / 100; loss 1.9496\n",
      "Evaluate (epoch 127) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 128 / 200, step    0 / 100; loss 1.8623\n",
      "Evaluate (epoch 128) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 129 / 200, step    0 / 100; loss 1.8185\n",
      "Evaluate (epoch 129) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 130 / 200, step    0 / 100; loss 1.7782\n",
      "Evaluate (epoch 130) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 131 / 200, step    0 / 100; loss 1.8492\n",
      "Evaluate (epoch 131) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 132 / 200, step    0 / 100; loss 1.8458\n",
      "Evaluate (epoch 132) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 133 / 200, step    0 / 100; loss 1.8555\n",
      "Evaluate (epoch 133) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 134 / 200, step    0 / 100; loss 1.8721\n",
      "Evaluate (epoch 134) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 135 / 200, step    0 / 100; loss 1.8147\n",
      "Evaluate (epoch 135) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 136 / 200, step    0 / 100; loss 1.9012\n",
      "Evaluate (epoch 136) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 137 / 200, step    0 / 100; loss 1.8579\n",
      "Evaluate (epoch 137) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 138 / 200, step    0 / 100; loss 1.8815\n",
      "Evaluate (epoch 138) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 139 / 200, step    0 / 100; loss 1.8848\n",
      "Evaluate (epoch 139) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 140 / 200, step    0 / 100; loss 1.8165\n",
      "Evaluate (epoch 140) -- logp(x) = -1.862 +/- 0.009\n",
      "epoch 141 / 200, step    0 / 100; loss 1.8677\n",
      "Evaluate (epoch 141) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 142 / 200, step    0 / 100; loss 1.9086\n",
      "Evaluate (epoch 142) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 143 / 200, step    0 / 100; loss 1.8985\n",
      "Evaluate (epoch 143) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 144 / 200, step    0 / 100; loss 1.7999\n",
      "Evaluate (epoch 144) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 145 / 200, step    0 / 100; loss 1.8523\n",
      "Evaluate (epoch 145) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 146 / 200, step    0 / 100; loss 1.8495\n",
      "Evaluate (epoch 146) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 147 / 200, step    0 / 100; loss 1.8773\n",
      "Evaluate (epoch 147) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 148 / 200, step    0 / 100; loss 1.9463\n",
      "Evaluate (epoch 148) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 149 / 200, step    0 / 100; loss 1.8881\n",
      "Evaluate (epoch 149) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 150 / 200, step    0 / 100; loss 1.8408\n",
      "Evaluate (epoch 150) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 151 / 200, step    0 / 100; loss 1.8440\n",
      "Evaluate (epoch 151) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 152 / 200, step    0 / 100; loss 1.8845\n",
      "Evaluate (epoch 152) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 153 / 200, step    0 / 100; loss 1.8694\n",
      "Evaluate (epoch 153) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 154 / 200, step    0 / 100; loss 1.8935\n",
      "Evaluate (epoch 154) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 155 / 200, step    0 / 100; loss 1.9604\n",
      "Evaluate (epoch 155) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 156 / 200, step    0 / 100; loss 1.8844\n",
      "Evaluate (epoch 156) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 157 / 200, step    0 / 100; loss 1.7885\n",
      "Evaluate (epoch 157) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 158 / 200, step    0 / 100; loss 1.8848\n",
      "Evaluate (epoch 158) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 159 / 200, step    0 / 100; loss 1.8923\n",
      "Evaluate (epoch 159) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 160 / 200, step    0 / 100; loss 1.8999\n",
      "Evaluate (epoch 160) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 161 / 200, step    0 / 100; loss 1.9240\n",
      "Evaluate (epoch 161) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 162 / 200, step    0 / 100; loss 1.9316\n",
      "Evaluate (epoch 162) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 163 / 200, step    0 / 100; loss 1.7915\n",
      "Evaluate (epoch 163) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 164 / 200, step    0 / 100; loss 1.8698\n",
      "Evaluate (epoch 164) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 165 / 200, step    0 / 100; loss 1.8293\n",
      "Evaluate (epoch 165) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 166 / 200, step    0 / 100; loss 1.8795\n",
      "Evaluate (epoch 166) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 167 / 200, step    0 / 100; loss 1.8649\n",
      "Evaluate (epoch 167) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 168 / 200, step    0 / 100; loss 1.8800\n",
      "Evaluate (epoch 168) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 169 / 200, step    0 / 100; loss 1.8639\n",
      "Evaluate (epoch 169) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 170 / 200, step    0 / 100; loss 1.9008\n",
      "Evaluate (epoch 170) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 171 / 200, step    0 / 100; loss 1.9789\n",
      "Evaluate (epoch 171) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 172 / 200, step    0 / 100; loss 1.9243\n",
      "Evaluate (epoch 172) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 173 / 200, step    0 / 100; loss 1.8554\n",
      "Evaluate (epoch 173) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 174 / 200, step    0 / 100; loss 1.8379\n",
      "Evaluate (epoch 174) -- logp(x) = -1.861 +/- 0.009\n",
      "epoch 175 / 200, step    0 / 100; loss 1.8413\n",
      "Evaluate (epoch 175) -- logp(x) = -1.859 +/- 0.009\n",
      "epoch 176 / 200, step    0 / 100; loss 1.8446\n",
      "Evaluate (epoch 176) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 177 / 200, step    0 / 100; loss 1.8373\n",
      "Evaluate (epoch 177) -- logp(x) = -1.859 +/- 0.009\n",
      "epoch 178 / 200, step    0 / 100; loss 1.8798\n",
      "Evaluate (epoch 178) -- logp(x) = -1.859 +/- 0.009\n",
      "epoch 179 / 200, step    0 / 100; loss 1.8921\n",
      "Evaluate (epoch 179) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 180 / 200, step    0 / 100; loss 1.9132\n",
      "Evaluate (epoch 180) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 181 / 200, step    0 / 100; loss 1.8513\n",
      "Evaluate (epoch 181) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 182 / 200, step    0 / 100; loss 1.8968\n",
      "Evaluate (epoch 182) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 183 / 200, step    0 / 100; loss 1.8545\n",
      "Evaluate (epoch 183) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 184 / 200, step    0 / 100; loss 1.8469\n",
      "Evaluate (epoch 184) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 185 / 200, step    0 / 100; loss 1.8674\n",
      "Evaluate (epoch 185) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 186 / 200, step    0 / 100; loss 1.8534\n",
      "Evaluate (epoch 186) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 187 / 200, step    0 / 100; loss 1.8433\n",
      "Evaluate (epoch 187) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 188 / 200, step    0 / 100; loss 1.9068\n",
      "Evaluate (epoch 188) -- logp(x) = -1.859 +/- 0.009\n",
      "epoch 189 / 200, step    0 / 100; loss 1.8515\n",
      "Evaluate (epoch 189) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 190 / 200, step    0 / 100; loss 1.8286\n",
      "Evaluate (epoch 190) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 191 / 200, step    0 / 100; loss 1.8483\n",
      "Evaluate (epoch 191) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 192 / 200, step    0 / 100; loss 1.8866\n",
      "Evaluate (epoch 192) -- logp(x) = -1.859 +/- 0.009\n",
      "epoch 193 / 200, step    0 / 100; loss 1.8203\n",
      "Evaluate (epoch 193) -- logp(x) = -1.859 +/- 0.009\n",
      "epoch 194 / 200, step    0 / 100; loss 1.8734\n",
      "Evaluate (epoch 194) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 195 / 200, step    0 / 100; loss 1.8020\n",
      "Evaluate (epoch 195) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 196 / 200, step    0 / 100; loss 1.9274\n",
      "Evaluate (epoch 196) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 197 / 200, step    0 / 100; loss 1.8621\n",
      "Evaluate (epoch 197) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 198 / 200, step    0 / 100; loss 1.9177\n",
      "Evaluate (epoch 198) -- logp(x) = -1.860 +/- 0.009\n",
      "epoch 199 / 200, step    0 / 100; loss 1.9081\n",
      "Evaluate (epoch 199) -- logp(x) = -1.860 +/- 0.009\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'CIRCLE',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': True,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 2,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 2,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'made',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 50,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': False,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/made',\n",
      " 'restore_file': './results/made/best_model_checkpoint.pt',\n",
      " 'results_file': './results/made\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 194,\n",
      " 'train': False}\n",
      "MADE(\n",
      "  (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "  (net): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'INVOLUTE',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': False,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 2,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 2,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'realnvp',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 200,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': True,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/realnvp',\n",
      " 'restore_file': None,\n",
      " 'results_file': './results/realnvp\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 0,\n",
      " 'train': True}\n",
      "RealNVP(\n",
      "  (net): FlowSequential(\n",
      "    (0): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch   0 / 200, step    0 / 100; loss 2.0048\n",
      "Evaluate (epoch 0) -- logp(x) = -1.042 +/- 0.019\n",
      "epoch   1 / 200, step    0 / 100; loss 1.1514\n",
      "Evaluate (epoch 1) -- logp(x) = -1.035 +/- 0.018\n",
      "epoch   2 / 200, step    0 / 100; loss 1.1046\n",
      "Evaluate (epoch 2) -- logp(x) = -1.033 +/- 0.019\n",
      "epoch   3 / 200, step    0 / 100; loss 1.0245\n",
      "Evaluate (epoch 3) -- logp(x) = -1.026 +/- 0.017\n",
      "epoch   4 / 200, step    0 / 100; loss 0.9594\n",
      "Evaluate (epoch 4) -- logp(x) = -1.023 +/- 0.018\n",
      "epoch   5 / 200, step    0 / 100; loss 0.9735\n",
      "Evaluate (epoch 5) -- logp(x) = -1.022 +/- 0.016\n",
      "epoch   6 / 200, step    0 / 100; loss 1.0305\n",
      "Evaluate (epoch 6) -- logp(x) = -1.017 +/- 0.016\n",
      "epoch   7 / 200, step    0 / 100; loss 1.0457\n",
      "Evaluate (epoch 7) -- logp(x) = -1.009 +/- 0.017\n",
      "epoch   8 / 200, step    0 / 100; loss 0.8977\n",
      "Evaluate (epoch 8) -- logp(x) = -1.006 +/- 0.016\n",
      "epoch   9 / 200, step    0 / 100; loss 0.8721\n",
      "Evaluate (epoch 9) -- logp(x) = -1.002 +/- 0.016\n",
      "epoch  10 / 200, step    0 / 100; loss 1.0113\n",
      "Evaluate (epoch 10) -- logp(x) = -0.980 +/- 0.015\n",
      "epoch  11 / 200, step    0 / 100; loss 1.0001\n",
      "Evaluate (epoch 11) -- logp(x) = -0.949 +/- 0.015\n",
      "epoch  12 / 200, step    0 / 100; loss 0.9089\n",
      "Evaluate (epoch 12) -- logp(x) = -0.943 +/- 0.016\n",
      "epoch  13 / 200, step    0 / 100; loss 0.7700\n",
      "Evaluate (epoch 13) -- logp(x) = -0.902 +/- 0.015\n",
      "epoch  14 / 200, step    0 / 100; loss 0.9218\n",
      "Evaluate (epoch 14) -- logp(x) = -0.897 +/- 0.014\n",
      "epoch  15 / 200, step    0 / 100; loss 0.7650\n",
      "Evaluate (epoch 15) -- logp(x) = -0.879 +/- 0.015\n",
      "epoch  16 / 200, step    0 / 100; loss 0.8271\n",
      "Evaluate (epoch 16) -- logp(x) = -0.846 +/- 0.014\n",
      "epoch  17 / 200, step    0 / 100; loss 0.8781\n",
      "Evaluate (epoch 17) -- logp(x) = -0.827 +/- 0.015\n",
      "epoch  18 / 200, step    0 / 100; loss 0.8775\n",
      "Evaluate (epoch 18) -- logp(x) = -0.812 +/- 0.015\n",
      "epoch  19 / 200, step    0 / 100; loss 0.9507\n",
      "Evaluate (epoch 19) -- logp(x) = -0.791 +/- 0.016\n",
      "epoch  20 / 200, step    0 / 100; loss 0.6520\n",
      "Evaluate (epoch 20) -- logp(x) = -0.792 +/- 0.017\n",
      "epoch  21 / 200, step    0 / 100; loss 0.7461\n",
      "Evaluate (epoch 21) -- logp(x) = -0.827 +/- 0.016\n",
      "epoch  22 / 200, step    0 / 100; loss 0.8508\n",
      "Evaluate (epoch 22) -- logp(x) = -0.992 +/- 0.018\n",
      "epoch  23 / 200, step    0 / 100; loss 1.1888\n",
      "Evaluate (epoch 23) -- logp(x) = -0.867 +/- 0.017\n",
      "epoch  24 / 200, step    0 / 100; loss 0.9136\n",
      "Evaluate (epoch 24) -- logp(x) = -0.832 +/- 0.017\n",
      "epoch  25 / 200, step    0 / 100; loss 0.8078\n",
      "Evaluate (epoch 25) -- logp(x) = -0.792 +/- 0.017\n",
      "epoch  26 / 200, step    0 / 100; loss 0.7643\n",
      "Evaluate (epoch 26) -- logp(x) = -0.755 +/- 0.016\n",
      "epoch  27 / 200, step    0 / 100; loss 0.7704\n",
      "Evaluate (epoch 27) -- logp(x) = -0.749 +/- 0.017\n",
      "epoch  28 / 200, step    0 / 100; loss 0.8689\n",
      "Evaluate (epoch 28) -- logp(x) = -0.698 +/- 0.017\n",
      "epoch  29 / 200, step    0 / 100; loss 0.6281\n",
      "Evaluate (epoch 29) -- logp(x) = -0.708 +/- 0.017\n",
      "epoch  30 / 200, step    0 / 100; loss 0.6449\n",
      "Evaluate (epoch 30) -- logp(x) = -0.667 +/- 0.019\n",
      "epoch  31 / 200, step    0 / 100; loss 0.5454\n",
      "Evaluate (epoch 31) -- logp(x) = -0.663 +/- 0.019\n",
      "epoch  32 / 200, step    0 / 100; loss 0.6977\n",
      "Evaluate (epoch 32) -- logp(x) = -0.652 +/- 0.019\n",
      "epoch  33 / 200, step    0 / 100; loss 0.7901\n",
      "Evaluate (epoch 33) -- logp(x) = -0.649 +/- 0.019\n",
      "epoch  34 / 200, step    0 / 100; loss 0.5965\n",
      "Evaluate (epoch 34) -- logp(x) = -0.611 +/- 0.020\n",
      "epoch  35 / 200, step    0 / 100; loss 0.9764\n",
      "Evaluate (epoch 35) -- logp(x) = -0.619 +/- 0.019\n",
      "epoch  36 / 200, step    0 / 100; loss 0.5519\n",
      "Evaluate (epoch 36) -- logp(x) = -0.608 +/- 0.020\n",
      "epoch  37 / 200, step    0 / 100; loss 0.6919\n",
      "Evaluate (epoch 37) -- logp(x) = -0.727 +/- 0.019\n",
      "epoch  38 / 200, step    0 / 100; loss 0.7600\n",
      "Evaluate (epoch 38) -- logp(x) = -0.616 +/- 0.019\n",
      "epoch  39 / 200, step    0 / 100; loss 0.6546\n",
      "Evaluate (epoch 39) -- logp(x) = -0.622 +/- 0.022\n",
      "epoch  40 / 200, step    0 / 100; loss 0.4881\n",
      "Evaluate (epoch 40) -- logp(x) = -0.608 +/- 0.020\n",
      "epoch  41 / 200, step    0 / 100; loss 0.6564\n",
      "Evaluate (epoch 41) -- logp(x) = -0.579 +/- 0.020\n",
      "epoch  42 / 200, step    0 / 100; loss 0.6182\n",
      "Evaluate (epoch 42) -- logp(x) = -0.556 +/- 0.020\n",
      "epoch  43 / 200, step    0 / 100; loss 0.5787\n",
      "Evaluate (epoch 43) -- logp(x) = -0.535 +/- 0.020\n",
      "epoch  44 / 200, step    0 / 100; loss 0.6871\n",
      "Evaluate (epoch 44) -- logp(x) = -0.522 +/- 0.021\n",
      "epoch  45 / 200, step    0 / 100; loss 0.5300\n",
      "Evaluate (epoch 45) -- logp(x) = -0.495 +/- 0.022\n",
      "epoch  46 / 200, step    0 / 100; loss 0.5186\n",
      "Evaluate (epoch 46) -- logp(x) = -0.535 +/- 0.021\n",
      "epoch  47 / 200, step    0 / 100; loss 0.4394\n",
      "Evaluate (epoch 47) -- logp(x) = -0.520 +/- 0.022\n",
      "epoch  48 / 200, step    0 / 100; loss 0.6011\n",
      "Evaluate (epoch 48) -- logp(x) = -0.484 +/- 0.023\n",
      "epoch  49 / 200, step    0 / 100; loss 0.7039\n",
      "Evaluate (epoch 49) -- logp(x) = -0.545 +/- 0.024\n",
      "epoch  50 / 200, step    0 / 100; loss 0.5767\n",
      "Evaluate (epoch 50) -- logp(x) = -0.505 +/- 0.022\n",
      "epoch  51 / 200, step    0 / 100; loss 0.5349\n",
      "Evaluate (epoch 51) -- logp(x) = -0.432 +/- 0.024\n",
      "epoch  52 / 200, step    0 / 100; loss 0.3185\n",
      "Evaluate (epoch 52) -- logp(x) = -0.568 +/- 0.029\n",
      "epoch  53 / 200, step    0 / 100; loss 0.5035\n",
      "Evaluate (epoch 53) -- logp(x) = -0.414 +/- 0.024\n",
      "epoch  54 / 200, step    0 / 100; loss 0.5642\n",
      "Evaluate (epoch 54) -- logp(x) = -0.410 +/- 0.024\n",
      "epoch  55 / 200, step    0 / 100; loss 0.6521\n",
      "Evaluate (epoch 55) -- logp(x) = -0.429 +/- 0.025\n",
      "epoch  56 / 200, step    0 / 100; loss 0.2635\n",
      "Evaluate (epoch 56) -- logp(x) = -0.411 +/- 0.025\n",
      "epoch  57 / 200, step    0 / 100; loss 0.2473\n",
      "Evaluate (epoch 57) -- logp(x) = -0.468 +/- 0.023\n",
      "epoch  58 / 200, step    0 / 100; loss 0.4515\n",
      "Evaluate (epoch 58) -- logp(x) = -0.384 +/- 0.024\n",
      "epoch  59 / 200, step    0 / 100; loss 0.3033\n",
      "Evaluate (epoch 59) -- logp(x) = -0.388 +/- 0.025\n",
      "epoch  60 / 200, step    0 / 100; loss 0.2568\n",
      "Evaluate (epoch 60) -- logp(x) = -0.466 +/- 0.024\n",
      "epoch  61 / 200, step    0 / 100; loss 0.7624\n",
      "Evaluate (epoch 61) -- logp(x) = -0.394 +/- 0.024\n",
      "epoch  62 / 200, step    0 / 100; loss 0.4317\n",
      "Evaluate (epoch 62) -- logp(x) = -0.377 +/- 0.025\n",
      "epoch  63 / 200, step    0 / 100; loss 0.3246\n",
      "Evaluate (epoch 63) -- logp(x) = -0.391 +/- 0.024\n",
      "epoch  64 / 200, step    0 / 100; loss 0.4362\n",
      "Evaluate (epoch 64) -- logp(x) = -0.452 +/- 0.026\n",
      "epoch  65 / 200, step    0 / 100; loss 0.4931\n",
      "Evaluate (epoch 65) -- logp(x) = -0.430 +/- 0.025\n",
      "epoch  66 / 200, step    0 / 100; loss 0.4029\n",
      "Evaluate (epoch 66) -- logp(x) = -0.362 +/- 0.024\n",
      "epoch  67 / 200, step    0 / 100; loss 0.3787\n",
      "Evaluate (epoch 67) -- logp(x) = -0.421 +/- 0.025\n",
      "epoch  68 / 200, step    0 / 100; loss 0.4903\n",
      "Evaluate (epoch 68) -- logp(x) = -0.408 +/- 0.025\n",
      "epoch  69 / 200, step    0 / 100; loss 0.6073\n",
      "Evaluate (epoch 69) -- logp(x) = -0.418 +/- 0.027\n",
      "epoch  70 / 200, step    0 / 100; loss 0.3679\n",
      "Evaluate (epoch 70) -- logp(x) = -0.549 +/- 0.026\n",
      "epoch  71 / 200, step    0 / 100; loss 0.7847\n",
      "Evaluate (epoch 71) -- logp(x) = -0.354 +/- 0.026\n",
      "epoch  72 / 200, step    0 / 100; loss 0.3363\n",
      "Evaluate (epoch 72) -- logp(x) = -0.365 +/- 0.027\n",
      "epoch  73 / 200, step    0 / 100; loss 0.3266\n",
      "Evaluate (epoch 73) -- logp(x) = -0.393 +/- 0.028\n",
      "epoch  74 / 200, step    0 / 100; loss 0.3371\n",
      "Evaluate (epoch 74) -- logp(x) = -0.364 +/- 0.029\n",
      "epoch  75 / 200, step    0 / 100; loss 0.2960\n",
      "Evaluate (epoch 75) -- logp(x) = -0.424 +/- 0.026\n",
      "epoch  76 / 200, step    0 / 100; loss 0.4707\n",
      "Evaluate (epoch 76) -- logp(x) = -0.331 +/- 0.024\n",
      "epoch  77 / 200, step    0 / 100; loss 0.3530\n",
      "Evaluate (epoch 77) -- logp(x) = -0.375 +/- 0.027\n",
      "epoch  78 / 200, step    0 / 100; loss 0.2056\n",
      "Evaluate (epoch 78) -- logp(x) = -0.379 +/- 0.026\n",
      "epoch  79 / 200, step    0 / 100; loss 0.3374\n",
      "Evaluate (epoch 79) -- logp(x) = -0.300 +/- 0.025\n",
      "epoch  80 / 200, step    0 / 100; loss 0.0926\n",
      "Evaluate (epoch 80) -- logp(x) = -0.337 +/- 0.027\n",
      "epoch  81 / 200, step    0 / 100; loss 0.5997\n",
      "Evaluate (epoch 81) -- logp(x) = -0.305 +/- 0.025\n",
      "epoch  82 / 200, step    0 / 100; loss 0.3161\n",
      "Evaluate (epoch 82) -- logp(x) = -0.307 +/- 0.027\n",
      "epoch  83 / 200, step    0 / 100; loss 0.4011\n",
      "Evaluate (epoch 83) -- logp(x) = -0.478 +/- 0.027\n",
      "epoch  84 / 200, step    0 / 100; loss 0.4599\n",
      "Evaluate (epoch 84) -- logp(x) = -0.380 +/- 0.025\n",
      "epoch  85 / 200, step    0 / 100; loss 0.5501\n",
      "Evaluate (epoch 85) -- logp(x) = -0.308 +/- 0.026\n",
      "epoch  86 / 200, step    0 / 100; loss 0.3453\n",
      "Evaluate (epoch 86) -- logp(x) = -0.301 +/- 0.025\n",
      "epoch  87 / 200, step    0 / 100; loss 0.1075\n",
      "Evaluate (epoch 87) -- logp(x) = -0.321 +/- 0.025\n",
      "epoch  88 / 200, step    0 / 100; loss 0.4516\n",
      "Evaluate (epoch 88) -- logp(x) = -0.339 +/- 0.026\n",
      "epoch  89 / 200, step    0 / 100; loss 0.2645\n",
      "Evaluate (epoch 89) -- logp(x) = -0.277 +/- 0.026\n",
      "epoch  90 / 200, step    0 / 100; loss 0.3995\n",
      "Evaluate (epoch 90) -- logp(x) = -0.295 +/- 0.026\n",
      "epoch  91 / 200, step    0 / 100; loss 0.0107\n",
      "Evaluate (epoch 91) -- logp(x) = -0.294 +/- 0.024\n",
      "epoch  92 / 200, step    0 / 100; loss 0.2128\n",
      "Evaluate (epoch 92) -- logp(x) = -0.289 +/- 0.024\n",
      "epoch  93 / 200, step    0 / 100; loss 0.2417\n",
      "Evaluate (epoch 93) -- logp(x) = -0.274 +/- 0.025\n",
      "epoch  94 / 200, step    0 / 100; loss 0.2742\n",
      "Evaluate (epoch 94) -- logp(x) = -0.298 +/- 0.026\n",
      "epoch  95 / 200, step    0 / 100; loss 0.2377\n",
      "Evaluate (epoch 95) -- logp(x) = -0.262 +/- 0.026\n",
      "epoch  96 / 200, step    0 / 100; loss 0.2044\n",
      "Evaluate (epoch 96) -- logp(x) = -0.250 +/- 0.027\n",
      "epoch  97 / 200, step    0 / 100; loss 0.3621\n",
      "Evaluate (epoch 97) -- logp(x) = -0.299 +/- 0.026\n",
      "epoch  98 / 200, step    0 / 100; loss 0.3260\n",
      "Evaluate (epoch 98) -- logp(x) = -0.300 +/- 0.024\n",
      "epoch  99 / 200, step    0 / 100; loss 0.3042\n",
      "Evaluate (epoch 99) -- logp(x) = -0.264 +/- 0.025\n",
      "epoch 100 / 200, step    0 / 100; loss 0.2695\n",
      "Evaluate (epoch 100) -- logp(x) = -0.343 +/- 0.024\n",
      "epoch 101 / 200, step    0 / 100; loss 0.2362\n",
      "Evaluate (epoch 101) -- logp(x) = -0.307 +/- 0.023\n",
      "epoch 102 / 200, step    0 / 100; loss 0.4718\n",
      "Evaluate (epoch 102) -- logp(x) = -0.267 +/- 0.025\n",
      "epoch 103 / 200, step    0 / 100; loss 0.4129\n",
      "Evaluate (epoch 103) -- logp(x) = -0.328 +/- 0.027\n",
      "epoch 104 / 200, step    0 / 100; loss 0.2407\n",
      "Evaluate (epoch 104) -- logp(x) = -0.333 +/- 0.031\n",
      "epoch 105 / 200, step    0 / 100; loss 0.5160\n",
      "Evaluate (epoch 105) -- logp(x) = -0.288 +/- 0.026\n",
      "epoch 106 / 200, step    0 / 100; loss 0.1191\n",
      "Evaluate (epoch 106) -- logp(x) = -0.303 +/- 0.024\n",
      "epoch 107 / 200, step    0 / 100; loss 0.3000\n",
      "Evaluate (epoch 107) -- logp(x) = -0.350 +/- 0.027\n",
      "epoch 108 / 200, step    0 / 100; loss 0.3590\n",
      "Evaluate (epoch 108) -- logp(x) = -0.283 +/- 0.025\n",
      "epoch 109 / 200, step    0 / 100; loss 0.1983\n",
      "Evaluate (epoch 109) -- logp(x) = -0.382 +/- 0.028\n",
      "epoch 110 / 200, step    0 / 100; loss 0.4046\n",
      "Evaluate (epoch 110) -- logp(x) = -0.331 +/- 0.027\n",
      "epoch 111 / 200, step    0 / 100; loss 0.1793\n",
      "Evaluate (epoch 111) -- logp(x) = -0.271 +/- 0.025\n",
      "epoch 112 / 200, step    0 / 100; loss 0.2709\n",
      "Evaluate (epoch 112) -- logp(x) = -0.263 +/- 0.025\n",
      "epoch 113 / 200, step    0 / 100; loss 0.1671\n",
      "Evaluate (epoch 113) -- logp(x) = -0.350 +/- 0.027\n",
      "epoch 114 / 200, step    0 / 100; loss 0.2611\n",
      "Evaluate (epoch 114) -- logp(x) = -0.275 +/- 0.026\n",
      "epoch 115 / 200, step    0 / 100; loss 0.4797\n",
      "Evaluate (epoch 115) -- logp(x) = -0.239 +/- 0.028\n",
      "epoch 116 / 200, step    0 / 100; loss 0.4443\n",
      "Evaluate (epoch 116) -- logp(x) = -0.245 +/- 0.028\n",
      "epoch 117 / 200, step    0 / 100; loss 0.0460\n",
      "Evaluate (epoch 117) -- logp(x) = -0.267 +/- 0.025\n",
      "epoch 118 / 200, step    0 / 100; loss 0.1802\n",
      "Evaluate (epoch 118) -- logp(x) = -0.231 +/- 0.026\n",
      "epoch 119 / 200, step    0 / 100; loss 0.6084\n",
      "Evaluate (epoch 119) -- logp(x) = -0.242 +/- 0.027\n",
      "epoch 120 / 200, step    0 / 100; loss 0.3732\n",
      "Evaluate (epoch 120) -- logp(x) = -0.242 +/- 0.028\n",
      "epoch 121 / 200, step    0 / 100; loss 0.3408\n",
      "Evaluate (epoch 121) -- logp(x) = -0.239 +/- 0.025\n",
      "epoch 122 / 200, step    0 / 100; loss 0.3456\n",
      "Evaluate (epoch 122) -- logp(x) = -0.273 +/- 0.025\n",
      "epoch 123 / 200, step    0 / 100; loss 0.2198\n",
      "Evaluate (epoch 123) -- logp(x) = -0.223 +/- 0.025\n",
      "epoch 124 / 200, step    0 / 100; loss 0.2690\n",
      "Evaluate (epoch 124) -- logp(x) = -0.238 +/- 0.026\n",
      "epoch 125 / 200, step    0 / 100; loss 0.2833\n",
      "Evaluate (epoch 125) -- logp(x) = -0.220 +/- 0.026\n",
      "epoch 126 / 200, step    0 / 100; loss 0.2101\n",
      "Evaluate (epoch 126) -- logp(x) = -0.312 +/- 0.027\n",
      "epoch 127 / 200, step    0 / 100; loss 0.2124\n",
      "Evaluate (epoch 127) -- logp(x) = -0.212 +/- 0.026\n",
      "epoch 128 / 200, step    0 / 100; loss 0.3331\n",
      "Evaluate (epoch 128) -- logp(x) = -0.292 +/- 0.026\n",
      "epoch 129 / 200, step    0 / 100; loss 0.3679\n",
      "Evaluate (epoch 129) -- logp(x) = -0.224 +/- 0.026\n",
      "epoch 130 / 200, step    0 / 100; loss 0.3053\n",
      "Evaluate (epoch 130) -- logp(x) = -0.210 +/- 0.026\n",
      "epoch 131 / 200, step    0 / 100; loss 0.1038\n",
      "Evaluate (epoch 131) -- logp(x) = -0.220 +/- 0.026\n",
      "epoch 132 / 200, step    0 / 100; loss 0.4404\n",
      "Evaluate (epoch 132) -- logp(x) = -0.227 +/- 0.025\n",
      "epoch 133 / 200, step    0 / 100; loss 0.2202\n",
      "Evaluate (epoch 133) -- logp(x) = -0.190 +/- 0.027\n",
      "epoch 134 / 200, step    0 / 100; loss 0.1311\n",
      "Evaluate (epoch 134) -- logp(x) = -0.338 +/- 0.025\n",
      "epoch 135 / 200, step    0 / 100; loss 0.3270\n",
      "Evaluate (epoch 135) -- logp(x) = -0.206 +/- 0.025\n",
      "epoch 136 / 200, step    0 / 100; loss 0.2954\n",
      "Evaluate (epoch 136) -- logp(x) = -0.185 +/- 0.026\n",
      "epoch 137 / 200, step    0 / 100; loss 0.2123\n",
      "Evaluate (epoch 137) -- logp(x) = -0.225 +/- 0.026\n",
      "epoch 138 / 200, step    0 / 100; loss 0.3851\n",
      "Evaluate (epoch 138) -- logp(x) = -0.241 +/- 0.025\n",
      "epoch 139 / 200, step    0 / 100; loss 0.2182\n",
      "Evaluate (epoch 139) -- logp(x) = -0.207 +/- 0.026\n",
      "epoch 140 / 200, step    0 / 100; loss 0.0956\n",
      "Evaluate (epoch 140) -- logp(x) = -0.176 +/- 0.026\n",
      "epoch 141 / 200, step    0 / 100; loss 0.1239\n",
      "Evaluate (epoch 141) -- logp(x) = -0.205 +/- 0.026\n",
      "epoch 142 / 200, step    0 / 100; loss 0.1359\n",
      "Evaluate (epoch 142) -- logp(x) = -0.177 +/- 0.028\n",
      "epoch 143 / 200, step    0 / 100; loss 0.0439\n",
      "Evaluate (epoch 143) -- logp(x) = -0.175 +/- 0.028\n",
      "epoch 144 / 200, step    0 / 100; loss 0.2558\n",
      "Evaluate (epoch 144) -- logp(x) = -0.218 +/- 0.026\n",
      "epoch 145 / 200, step    0 / 100; loss 0.2926\n",
      "Evaluate (epoch 145) -- logp(x) = -0.194 +/- 0.027\n",
      "epoch 146 / 200, step    0 / 100; loss 0.1589\n",
      "Evaluate (epoch 146) -- logp(x) = -0.220 +/- 0.027\n",
      "epoch 147 / 200, step    0 / 100; loss 0.1184\n",
      "Evaluate (epoch 147) -- logp(x) = -0.168 +/- 0.027\n",
      "epoch 148 / 200, step    0 / 100; loss 0.2193\n",
      "Evaluate (epoch 148) -- logp(x) = -0.264 +/- 0.027\n",
      "epoch 149 / 200, step    0 / 100; loss 0.3518\n",
      "Evaluate (epoch 149) -- logp(x) = -0.261 +/- 0.026\n",
      "epoch 150 / 200, step    0 / 100; loss 0.3791\n",
      "Evaluate (epoch 150) -- logp(x) = -0.259 +/- 0.030\n",
      "epoch 151 / 200, step    0 / 100; loss 0.1695\n",
      "Evaluate (epoch 151) -- logp(x) = -0.225 +/- 0.025\n",
      "epoch 152 / 200, step    0 / 100; loss 0.2053\n",
      "Evaluate (epoch 152) -- logp(x) = -0.231 +/- 0.027\n",
      "epoch 153 / 200, step    0 / 100; loss 0.2426\n",
      "Evaluate (epoch 153) -- logp(x) = -0.308 +/- 0.027\n",
      "epoch 154 / 200, step    0 / 100; loss 0.2479\n",
      "Evaluate (epoch 154) -- logp(x) = -0.294 +/- 0.028\n",
      "epoch 155 / 200, step    0 / 100; loss 0.3076\n",
      "Evaluate (epoch 155) -- logp(x) = -0.213 +/- 0.026\n",
      "epoch 156 / 200, step    0 / 100; loss 0.2647\n",
      "Evaluate (epoch 156) -- logp(x) = -0.201 +/- 0.027\n",
      "epoch 157 / 200, step    0 / 100; loss 0.0036\n",
      "Evaluate (epoch 157) -- logp(x) = -0.195 +/- 0.027\n",
      "epoch 158 / 200, step    0 / 100; loss 0.0795\n",
      "Evaluate (epoch 158) -- logp(x) = -0.331 +/- 0.027\n",
      "epoch 159 / 200, step    0 / 100; loss 0.3129\n",
      "Evaluate (epoch 159) -- logp(x) = -0.216 +/- 0.028\n",
      "epoch 160 / 200, step    0 / 100; loss 0.1930\n",
      "Evaluate (epoch 160) -- logp(x) = -0.243 +/- 0.025\n",
      "epoch 161 / 200, step    0 / 100; loss 0.1063\n",
      "Evaluate (epoch 161) -- logp(x) = -0.296 +/- 0.026\n",
      "epoch 162 / 200, step    0 / 100; loss 0.0378\n",
      "Evaluate (epoch 162) -- logp(x) = -0.150 +/- 0.028\n",
      "epoch 163 / 200, step    0 / 100; loss 0.4909\n",
      "Evaluate (epoch 163) -- logp(x) = -0.177 +/- 0.027\n",
      "epoch 164 / 200, step    0 / 100; loss 0.2398\n",
      "Evaluate (epoch 164) -- logp(x) = -0.177 +/- 0.027\n",
      "epoch 165 / 200, step    0 / 100; loss 0.0345\n",
      "Evaluate (epoch 165) -- logp(x) = -0.195 +/- 0.027\n",
      "epoch 166 / 200, step    0 / 100; loss 0.3998\n",
      "Evaluate (epoch 166) -- logp(x) = -0.266 +/- 0.027\n",
      "epoch 167 / 200, step    0 / 100; loss 0.1734\n",
      "Evaluate (epoch 167) -- logp(x) = -0.260 +/- 0.027\n",
      "epoch 168 / 200, step    0 / 100; loss 0.1924\n",
      "Evaluate (epoch 168) -- logp(x) = -0.166 +/- 0.028\n",
      "epoch 169 / 200, step    0 / 100; loss 0.4922\n",
      "Evaluate (epoch 169) -- logp(x) = -0.142 +/- 0.028\n",
      "epoch 170 / 200, step    0 / 100; loss 0.0246\n",
      "Evaluate (epoch 170) -- logp(x) = -0.155 +/- 0.030\n",
      "epoch 171 / 200, step    0 / 100; loss 0.0456\n",
      "Evaluate (epoch 171) -- logp(x) = -0.175 +/- 0.028\n",
      "epoch 172 / 200, step    0 / 100; loss 0.0755\n",
      "Evaluate (epoch 172) -- logp(x) = -0.220 +/- 0.027\n",
      "epoch 173 / 200, step    0 / 100; loss 0.1166\n",
      "Evaluate (epoch 173) -- logp(x) = -0.160 +/- 0.026\n",
      "epoch 174 / 200, step    0 / 100; loss 0.2117\n",
      "Evaluate (epoch 174) -- logp(x) = -0.196 +/- 0.027\n",
      "epoch 175 / 200, step    0 / 100; loss 0.1372\n",
      "Evaluate (epoch 175) -- logp(x) = -0.166 +/- 0.026\n",
      "epoch 176 / 200, step    0 / 100; loss 0.4709\n",
      "Evaluate (epoch 176) -- logp(x) = -0.189 +/- 0.029\n",
      "epoch 177 / 200, step    0 / 100; loss 0.1418\n",
      "Evaluate (epoch 177) -- logp(x) = -0.180 +/- 0.028\n",
      "epoch 178 / 200, step    0 / 100; loss -0.0563\n",
      "Evaluate (epoch 178) -- logp(x) = -0.153 +/- 0.026\n",
      "epoch 179 / 200, step    0 / 100; loss 0.0397\n",
      "Evaluate (epoch 179) -- logp(x) = -0.192 +/- 0.030\n",
      "epoch 180 / 200, step    0 / 100; loss 0.0902\n",
      "Evaluate (epoch 180) -- logp(x) = -0.143 +/- 0.027\n",
      "epoch 181 / 200, step    0 / 100; loss 0.0894\n",
      "Evaluate (epoch 181) -- logp(x) = -0.161 +/- 0.027\n",
      "epoch 182 / 200, step    0 / 100; loss 0.3211\n",
      "Evaluate (epoch 182) -- logp(x) = -0.337 +/- 0.027\n",
      "epoch 183 / 200, step    0 / 100; loss 0.3480\n",
      "Evaluate (epoch 183) -- logp(x) = -0.281 +/- 0.029\n",
      "epoch 184 / 200, step    0 / 100; loss 0.2949\n",
      "Evaluate (epoch 184) -- logp(x) = -0.268 +/- 0.027\n",
      "epoch 185 / 200, step    0 / 100; loss 0.1039\n",
      "Evaluate (epoch 185) -- logp(x) = -0.257 +/- 0.031\n",
      "epoch 186 / 200, step    0 / 100; loss 0.1901\n",
      "Evaluate (epoch 186) -- logp(x) = -0.315 +/- 0.030\n",
      "epoch 187 / 200, step    0 / 100; loss 0.5299\n",
      "Evaluate (epoch 187) -- logp(x) = -0.199 +/- 0.029\n",
      "epoch 188 / 200, step    0 / 100; loss 0.2481\n",
      "Evaluate (epoch 188) -- logp(x) = -0.263 +/- 0.027\n",
      "epoch 189 / 200, step    0 / 100; loss 0.6394\n",
      "Evaluate (epoch 189) -- logp(x) = -0.289 +/- 0.033\n",
      "epoch 190 / 200, step    0 / 100; loss 0.1972\n",
      "Evaluate (epoch 190) -- logp(x) = -0.232 +/- 0.026\n",
      "epoch 191 / 200, step    0 / 100; loss 0.3478\n",
      "Evaluate (epoch 191) -- logp(x) = -0.211 +/- 0.028\n",
      "epoch 192 / 200, step    0 / 100; loss -0.0370\n",
      "Evaluate (epoch 192) -- logp(x) = -0.210 +/- 0.028\n",
      "epoch 193 / 200, step    0 / 100; loss 0.2199\n",
      "Evaluate (epoch 193) -- logp(x) = -0.188 +/- 0.031\n",
      "epoch 194 / 200, step    0 / 100; loss 0.1752\n",
      "Evaluate (epoch 194) -- logp(x) = -0.199 +/- 0.028\n",
      "epoch 195 / 200, step    0 / 100; loss 0.1544\n",
      "Evaluate (epoch 195) -- logp(x) = -0.194 +/- 0.029\n",
      "epoch 196 / 200, step    0 / 100; loss 0.3532\n",
      "Evaluate (epoch 196) -- logp(x) = -0.190 +/- 0.032\n",
      "epoch 197 / 200, step    0 / 100; loss 0.1663\n",
      "Evaluate (epoch 197) -- logp(x) = -0.188 +/- 0.027\n",
      "epoch 198 / 200, step    0 / 100; loss 0.1107\n",
      "Evaluate (epoch 198) -- logp(x) = -0.189 +/- 0.029\n",
      "epoch 199 / 200, step    0 / 100; loss 0.3097\n",
      "Evaluate (epoch 199) -- logp(x) = -0.230 +/- 0.028\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'TORUS',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': False,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 3,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 3,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'made',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 200,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': False,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/made',\n",
      " 'restore_file': None,\n",
      " 'results_file': './results/made\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 0,\n",
      " 'train': True}\n",
      "MADE(\n",
      "  (net_input): MaskedLinear(in_features=3, out_features=100, bias=True)\n",
      "  (net): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaskedLinear(in_features=100, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch   0 / 200, step    0 / 100; loss 3.2624\n",
      "Evaluate (epoch 0) -- logp(x) = -2.900 +/- 0.007\n",
      "epoch   1 / 200, step    0 / 100; loss 2.9440\n",
      "Evaluate (epoch 1) -- logp(x) = -2.691 +/- 0.010\n",
      "epoch   2 / 200, step    0 / 100; loss 2.6498\n",
      "Evaluate (epoch 2) -- logp(x) = -2.664 +/- 0.011\n",
      "epoch   3 / 200, step    0 / 100; loss 2.5258\n",
      "Evaluate (epoch 3) -- logp(x) = -2.655 +/- 0.011\n",
      "epoch   4 / 200, step    0 / 100; loss 2.6318\n",
      "Evaluate (epoch 4) -- logp(x) = -2.648 +/- 0.011\n",
      "epoch   5 / 200, step    0 / 100; loss 2.5469\n",
      "Evaluate (epoch 5) -- logp(x) = -2.642 +/- 0.012\n",
      "epoch   6 / 200, step    0 / 100; loss 2.6506\n",
      "Evaluate (epoch 6) -- logp(x) = -2.636 +/- 0.011\n",
      "epoch   7 / 200, step    0 / 100; loss 2.5991\n",
      "Evaluate (epoch 7) -- logp(x) = -2.631 +/- 0.011\n",
      "epoch   8 / 200, step    0 / 100; loss 2.6170\n",
      "Evaluate (epoch 8) -- logp(x) = -2.627 +/- 0.012\n",
      "epoch   9 / 200, step    0 / 100; loss 2.5723\n",
      "Evaluate (epoch 9) -- logp(x) = -2.622 +/- 0.012\n",
      "epoch  10 / 200, step    0 / 100; loss 2.6262\n",
      "Evaluate (epoch 10) -- logp(x) = -2.618 +/- 0.012\n",
      "epoch  11 / 200, step    0 / 100; loss 2.6333\n",
      "Evaluate (epoch 11) -- logp(x) = -2.616 +/- 0.012\n",
      "epoch  12 / 200, step    0 / 100; loss 2.6521\n",
      "Evaluate (epoch 12) -- logp(x) = -2.611 +/- 0.012\n",
      "epoch  13 / 200, step    0 / 100; loss 2.5933\n",
      "Evaluate (epoch 13) -- logp(x) = -2.608 +/- 0.012\n",
      "epoch  14 / 200, step    0 / 100; loss 2.7181\n",
      "Evaluate (epoch 14) -- logp(x) = -2.604 +/- 0.012\n",
      "epoch  15 / 200, step    0 / 100; loss 2.5814\n",
      "Evaluate (epoch 15) -- logp(x) = -2.602 +/- 0.012\n",
      "epoch  16 / 200, step    0 / 100; loss 2.6064\n",
      "Evaluate (epoch 16) -- logp(x) = -2.599 +/- 0.012\n",
      "epoch  17 / 200, step    0 / 100; loss 2.7280\n",
      "Evaluate (epoch 17) -- logp(x) = -2.596 +/- 0.012\n",
      "epoch  18 / 200, step    0 / 100; loss 2.5141\n",
      "Evaluate (epoch 18) -- logp(x) = -2.594 +/- 0.012\n",
      "epoch  19 / 200, step    0 / 100; loss 2.5325\n",
      "Evaluate (epoch 19) -- logp(x) = -2.590 +/- 0.012\n",
      "epoch  20 / 200, step    0 / 100; loss 2.5417\n",
      "Evaluate (epoch 20) -- logp(x) = -2.588 +/- 0.012\n",
      "epoch  21 / 200, step    0 / 100; loss 2.6827\n",
      "Evaluate (epoch 21) -- logp(x) = -2.584 +/- 0.012\n",
      "epoch  22 / 200, step    0 / 100; loss 2.6590\n",
      "Evaluate (epoch 22) -- logp(x) = -2.582 +/- 0.012\n",
      "epoch  23 / 200, step    0 / 100; loss 2.5786\n",
      "Evaluate (epoch 23) -- logp(x) = -2.578 +/- 0.012\n",
      "epoch  24 / 200, step    0 / 100; loss 2.5517\n",
      "Evaluate (epoch 24) -- logp(x) = -2.575 +/- 0.012\n",
      "epoch  25 / 200, step    0 / 100; loss 2.5443\n",
      "Evaluate (epoch 25) -- logp(x) = -2.571 +/- 0.012\n",
      "epoch  26 / 200, step    0 / 100; loss 2.5751\n",
      "Evaluate (epoch 26) -- logp(x) = -2.567 +/- 0.013\n",
      "epoch  27 / 200, step    0 / 100; loss 2.5440\n",
      "Evaluate (epoch 27) -- logp(x) = -2.563 +/- 0.012\n",
      "epoch  28 / 200, step    0 / 100; loss 2.6164\n",
      "Evaluate (epoch 28) -- logp(x) = -2.560 +/- 0.013\n",
      "epoch  29 / 200, step    0 / 100; loss 2.4993\n",
      "Evaluate (epoch 29) -- logp(x) = -2.556 +/- 0.013\n",
      "epoch  30 / 200, step    0 / 100; loss 2.6659\n",
      "Evaluate (epoch 30) -- logp(x) = -2.551 +/- 0.013\n",
      "epoch  31 / 200, step    0 / 100; loss 2.5836\n",
      "Evaluate (epoch 31) -- logp(x) = -2.547 +/- 0.013\n",
      "epoch  32 / 200, step    0 / 100; loss 2.6321\n",
      "Evaluate (epoch 32) -- logp(x) = -2.543 +/- 0.013\n",
      "epoch  33 / 200, step    0 / 100; loss 2.5655\n",
      "Evaluate (epoch 33) -- logp(x) = -2.539 +/- 0.013\n",
      "epoch  34 / 200, step    0 / 100; loss 2.5691\n",
      "Evaluate (epoch 34) -- logp(x) = -2.534 +/- 0.013\n",
      "epoch  35 / 200, step    0 / 100; loss 2.5556\n",
      "Evaluate (epoch 35) -- logp(x) = -2.531 +/- 0.013\n",
      "epoch  36 / 200, step    0 / 100; loss 2.5820\n",
      "Evaluate (epoch 36) -- logp(x) = -2.526 +/- 0.013\n",
      "epoch  37 / 200, step    0 / 100; loss 2.5670\n",
      "Evaluate (epoch 37) -- logp(x) = -2.521 +/- 0.013\n",
      "epoch  38 / 200, step    0 / 100; loss 2.4389\n",
      "Evaluate (epoch 38) -- logp(x) = -2.517 +/- 0.013\n",
      "epoch  39 / 200, step    0 / 100; loss 2.4707\n",
      "Evaluate (epoch 39) -- logp(x) = -2.513 +/- 0.013\n",
      "epoch  40 / 200, step    0 / 100; loss 2.5111\n",
      "Evaluate (epoch 40) -- logp(x) = -2.509 +/- 0.013\n",
      "epoch  41 / 200, step    0 / 100; loss 2.3489\n",
      "Evaluate (epoch 41) -- logp(x) = -2.504 +/- 0.013\n",
      "epoch  42 / 200, step    0 / 100; loss 2.4832\n",
      "Evaluate (epoch 42) -- logp(x) = -2.500 +/- 0.013\n",
      "epoch  43 / 200, step    0 / 100; loss 2.6151\n",
      "Evaluate (epoch 43) -- logp(x) = -2.496 +/- 0.014\n",
      "epoch  44 / 200, step    0 / 100; loss 2.4665\n",
      "Evaluate (epoch 44) -- logp(x) = -2.492 +/- 0.014\n",
      "epoch  45 / 200, step    0 / 100; loss 2.4843\n",
      "Evaluate (epoch 45) -- logp(x) = -2.488 +/- 0.014\n",
      "epoch  46 / 200, step    0 / 100; loss 2.3962\n",
      "Evaluate (epoch 46) -- logp(x) = -2.485 +/- 0.014\n",
      "epoch  47 / 200, step    0 / 100; loss 2.6372\n",
      "Evaluate (epoch 47) -- logp(x) = -2.481 +/- 0.014\n",
      "epoch  48 / 200, step    0 / 100; loss 2.6305\n",
      "Evaluate (epoch 48) -- logp(x) = -2.477 +/- 0.014\n",
      "epoch  49 / 200, step    0 / 100; loss 2.5040\n",
      "Evaluate (epoch 49) -- logp(x) = -2.474 +/- 0.014\n",
      "epoch  50 / 200, step    0 / 100; loss 2.5351\n",
      "Evaluate (epoch 50) -- logp(x) = -2.471 +/- 0.014\n",
      "epoch  51 / 200, step    0 / 100; loss 2.4424\n",
      "Evaluate (epoch 51) -- logp(x) = -2.469 +/- 0.014\n",
      "epoch  52 / 200, step    0 / 100; loss 2.5218\n",
      "Evaluate (epoch 52) -- logp(x) = -2.466 +/- 0.014\n",
      "epoch  53 / 200, step    0 / 100; loss 2.4776\n",
      "Evaluate (epoch 53) -- logp(x) = -2.463 +/- 0.014\n",
      "epoch  54 / 200, step    0 / 100; loss 2.3868\n",
      "Evaluate (epoch 54) -- logp(x) = -2.460 +/- 0.014\n",
      "epoch  55 / 200, step    0 / 100; loss 2.5180\n",
      "Evaluate (epoch 55) -- logp(x) = -2.458 +/- 0.014\n",
      "epoch  56 / 200, step    0 / 100; loss 2.4159\n",
      "Evaluate (epoch 56) -- logp(x) = -2.456 +/- 0.014\n",
      "epoch  57 / 200, step    0 / 100; loss 2.5286\n",
      "Evaluate (epoch 57) -- logp(x) = -2.454 +/- 0.014\n",
      "epoch  58 / 200, step    0 / 100; loss 2.5813\n",
      "Evaluate (epoch 58) -- logp(x) = -2.451 +/- 0.014\n",
      "epoch  59 / 200, step    0 / 100; loss 2.4226\n",
      "Evaluate (epoch 59) -- logp(x) = -2.449 +/- 0.014\n",
      "epoch  60 / 200, step    0 / 100; loss 2.4786\n",
      "Evaluate (epoch 60) -- logp(x) = -2.448 +/- 0.014\n",
      "epoch  61 / 200, step    0 / 100; loss 2.4206\n",
      "Evaluate (epoch 61) -- logp(x) = -2.449 +/- 0.014\n",
      "epoch  62 / 200, step    0 / 100; loss 2.4023\n",
      "Evaluate (epoch 62) -- logp(x) = -2.444 +/- 0.014\n",
      "epoch  63 / 200, step    0 / 100; loss 2.5101\n",
      "Evaluate (epoch 63) -- logp(x) = -2.443 +/- 0.014\n",
      "epoch  64 / 200, step    0 / 100; loss 2.4395\n",
      "Evaluate (epoch 64) -- logp(x) = -2.442 +/- 0.014\n",
      "epoch  65 / 200, step    0 / 100; loss 2.4530\n",
      "Evaluate (epoch 65) -- logp(x) = -2.440 +/- 0.014\n",
      "epoch  66 / 200, step    0 / 100; loss 2.4797\n",
      "Evaluate (epoch 66) -- logp(x) = -2.439 +/- 0.015\n",
      "epoch  67 / 200, step    0 / 100; loss 2.4484\n",
      "Evaluate (epoch 67) -- logp(x) = -2.437 +/- 0.014\n",
      "epoch  68 / 200, step    0 / 100; loss 2.4891\n",
      "Evaluate (epoch 68) -- logp(x) = -2.437 +/- 0.015\n",
      "epoch  69 / 200, step    0 / 100; loss 2.4430\n",
      "Evaluate (epoch 69) -- logp(x) = -2.435 +/- 0.014\n",
      "epoch  70 / 200, step    0 / 100; loss 2.4926\n",
      "Evaluate (epoch 70) -- logp(x) = -2.434 +/- 0.015\n",
      "epoch  71 / 200, step    0 / 100; loss 2.4543\n",
      "Evaluate (epoch 71) -- logp(x) = -2.432 +/- 0.015\n",
      "epoch  72 / 200, step    0 / 100; loss 2.4758\n",
      "Evaluate (epoch 72) -- logp(x) = -2.431 +/- 0.015\n",
      "epoch  73 / 200, step    0 / 100; loss 2.2940\n",
      "Evaluate (epoch 73) -- logp(x) = -2.430 +/- 0.015\n",
      "epoch  74 / 200, step    0 / 100; loss 2.3478\n",
      "Evaluate (epoch 74) -- logp(x) = -2.429 +/- 0.015\n",
      "epoch  75 / 200, step    0 / 100; loss 2.4478\n",
      "Evaluate (epoch 75) -- logp(x) = -2.427 +/- 0.015\n",
      "epoch  76 / 200, step    0 / 100; loss 2.4569\n",
      "Evaluate (epoch 76) -- logp(x) = -2.427 +/- 0.015\n",
      "epoch  77 / 200, step    0 / 100; loss 2.4192\n",
      "Evaluate (epoch 77) -- logp(x) = -2.425 +/- 0.015\n",
      "epoch  78 / 200, step    0 / 100; loss 2.4726\n",
      "Evaluate (epoch 78) -- logp(x) = -2.425 +/- 0.015\n",
      "epoch  79 / 200, step    0 / 100; loss 2.3029\n",
      "Evaluate (epoch 79) -- logp(x) = -2.424 +/- 0.015\n",
      "epoch  80 / 200, step    0 / 100; loss 2.3819\n",
      "Evaluate (epoch 80) -- logp(x) = -2.423 +/- 0.015\n",
      "epoch  81 / 200, step    0 / 100; loss 2.3925\n",
      "Evaluate (epoch 81) -- logp(x) = -2.422 +/- 0.015\n",
      "epoch  82 / 200, step    0 / 100; loss 2.5087\n",
      "Evaluate (epoch 82) -- logp(x) = -2.421 +/- 0.015\n",
      "epoch  83 / 200, step    0 / 100; loss 2.3269\n",
      "Evaluate (epoch 83) -- logp(x) = -2.420 +/- 0.015\n",
      "epoch  84 / 200, step    0 / 100; loss 2.3687\n",
      "Evaluate (epoch 84) -- logp(x) = -2.419 +/- 0.015\n",
      "epoch  85 / 200, step    0 / 100; loss 2.4079\n",
      "Evaluate (epoch 85) -- logp(x) = -2.417 +/- 0.015\n",
      "epoch  86 / 200, step    0 / 100; loss 2.4444\n",
      "Evaluate (epoch 86) -- logp(x) = -2.418 +/- 0.015\n",
      "epoch  87 / 200, step    0 / 100; loss 2.3830\n",
      "Evaluate (epoch 87) -- logp(x) = -2.416 +/- 0.015\n",
      "epoch  88 / 200, step    0 / 100; loss 2.3890\n",
      "Evaluate (epoch 88) -- logp(x) = -2.415 +/- 0.015\n",
      "epoch  89 / 200, step    0 / 100; loss 2.2936\n",
      "Evaluate (epoch 89) -- logp(x) = -2.416 +/- 0.015\n",
      "epoch  90 / 200, step    0 / 100; loss 2.4060\n",
      "Evaluate (epoch 90) -- logp(x) = -2.413 +/- 0.015\n",
      "epoch  91 / 200, step    0 / 100; loss 2.4309\n",
      "Evaluate (epoch 91) -- logp(x) = -2.412 +/- 0.015\n",
      "epoch  92 / 200, step    0 / 100; loss 2.3685\n",
      "Evaluate (epoch 92) -- logp(x) = -2.411 +/- 0.015\n",
      "epoch  93 / 200, step    0 / 100; loss 2.2808\n",
      "Evaluate (epoch 93) -- logp(x) = -2.412 +/- 0.015\n",
      "epoch  94 / 200, step    0 / 100; loss 2.5127\n",
      "Evaluate (epoch 94) -- logp(x) = -2.409 +/- 0.015\n",
      "epoch  95 / 200, step    0 / 100; loss 2.4325\n",
      "Evaluate (epoch 95) -- logp(x) = -2.408 +/- 0.015\n",
      "epoch  96 / 200, step    0 / 100; loss 2.3433\n",
      "Evaluate (epoch 96) -- logp(x) = -2.409 +/- 0.015\n",
      "epoch  97 / 200, step    0 / 100; loss 2.3984\n",
      "Evaluate (epoch 97) -- logp(x) = -2.408 +/- 0.015\n",
      "epoch  98 / 200, step    0 / 100; loss 2.3298\n",
      "Evaluate (epoch 98) -- logp(x) = -2.406 +/- 0.015\n",
      "epoch  99 / 200, step    0 / 100; loss 2.4366\n",
      "Evaluate (epoch 99) -- logp(x) = -2.405 +/- 0.015\n",
      "epoch 100 / 200, step    0 / 100; loss 2.3922\n",
      "Evaluate (epoch 100) -- logp(x) = -2.406 +/- 0.015\n",
      "epoch 101 / 200, step    0 / 100; loss 2.4121\n",
      "Evaluate (epoch 101) -- logp(x) = -2.405 +/- 0.015\n",
      "epoch 102 / 200, step    0 / 100; loss 2.2514\n",
      "Evaluate (epoch 102) -- logp(x) = -2.403 +/- 0.015\n",
      "epoch 103 / 200, step    0 / 100; loss 2.4106\n",
      "Evaluate (epoch 103) -- logp(x) = -2.403 +/- 0.015\n",
      "epoch 104 / 200, step    0 / 100; loss 2.4056\n",
      "Evaluate (epoch 104) -- logp(x) = -2.403 +/- 0.015\n",
      "epoch 105 / 200, step    0 / 100; loss 2.4963\n",
      "Evaluate (epoch 105) -- logp(x) = -2.402 +/- 0.015\n",
      "epoch 106 / 200, step    0 / 100; loss 2.4066\n",
      "Evaluate (epoch 106) -- logp(x) = -2.402 +/- 0.015\n",
      "epoch 107 / 200, step    0 / 100; loss 2.3181\n",
      "Evaluate (epoch 107) -- logp(x) = -2.400 +/- 0.015\n",
      "epoch 108 / 200, step    0 / 100; loss 2.3689\n",
      "Evaluate (epoch 108) -- logp(x) = -2.400 +/- 0.015\n",
      "epoch 109 / 200, step    0 / 100; loss 2.3252\n",
      "Evaluate (epoch 109) -- logp(x) = -2.400 +/- 0.015\n",
      "epoch 110 / 200, step    0 / 100; loss 2.3366\n",
      "Evaluate (epoch 110) -- logp(x) = -2.399 +/- 0.015\n",
      "epoch 111 / 200, step    0 / 100; loss 2.3630\n",
      "Evaluate (epoch 111) -- logp(x) = -2.398 +/- 0.015\n",
      "epoch 112 / 200, step    0 / 100; loss 2.3793\n",
      "Evaluate (epoch 112) -- logp(x) = -2.398 +/- 0.015\n",
      "epoch 113 / 200, step    0 / 100; loss 2.4932\n",
      "Evaluate (epoch 113) -- logp(x) = -2.397 +/- 0.015\n",
      "epoch 114 / 200, step    0 / 100; loss 2.5151\n",
      "Evaluate (epoch 114) -- logp(x) = -2.396 +/- 0.015\n",
      "epoch 115 / 200, step    0 / 100; loss 2.3488\n",
      "Evaluate (epoch 115) -- logp(x) = -2.396 +/- 0.015\n",
      "epoch 116 / 200, step    0 / 100; loss 2.5050\n",
      "Evaluate (epoch 116) -- logp(x) = -2.398 +/- 0.016\n",
      "epoch 117 / 200, step    0 / 100; loss 2.4133\n",
      "Evaluate (epoch 117) -- logp(x) = -2.395 +/- 0.015\n",
      "epoch 118 / 200, step    0 / 100; loss 2.3278\n",
      "Evaluate (epoch 118) -- logp(x) = -2.394 +/- 0.015\n",
      "epoch 119 / 200, step    0 / 100; loss 2.4632\n",
      "Evaluate (epoch 119) -- logp(x) = -2.395 +/- 0.015\n",
      "epoch 120 / 200, step    0 / 100; loss 2.5195\n",
      "Evaluate (epoch 120) -- logp(x) = -2.393 +/- 0.015\n",
      "epoch 121 / 200, step    0 / 100; loss 2.3837\n",
      "Evaluate (epoch 121) -- logp(x) = -2.393 +/- 0.015\n",
      "epoch 122 / 200, step    0 / 100; loss 2.2029\n",
      "Evaluate (epoch 122) -- logp(x) = -2.392 +/- 0.015\n",
      "epoch 123 / 200, step    0 / 100; loss 2.2664\n",
      "Evaluate (epoch 123) -- logp(x) = -2.391 +/- 0.015\n",
      "epoch 124 / 200, step    0 / 100; loss 2.4522\n",
      "Evaluate (epoch 124) -- logp(x) = -2.392 +/- 0.016\n",
      "epoch 125 / 200, step    0 / 100; loss 2.3678\n",
      "Evaluate (epoch 125) -- logp(x) = -2.391 +/- 0.015\n",
      "epoch 126 / 200, step    0 / 100; loss 2.3923\n",
      "Evaluate (epoch 126) -- logp(x) = -2.390 +/- 0.015\n",
      "epoch 127 / 200, step    0 / 100; loss 2.4085\n",
      "Evaluate (epoch 127) -- logp(x) = -2.390 +/- 0.015\n",
      "epoch 128 / 200, step    0 / 100; loss 2.3029\n",
      "Evaluate (epoch 128) -- logp(x) = -2.389 +/- 0.015\n",
      "epoch 129 / 200, step    0 / 100; loss 2.4545\n",
      "Evaluate (epoch 129) -- logp(x) = -2.391 +/- 0.016\n",
      "epoch 130 / 200, step    0 / 100; loss 2.3071\n",
      "Evaluate (epoch 130) -- logp(x) = -2.389 +/- 0.016\n",
      "epoch 131 / 200, step    0 / 100; loss 2.3855\n",
      "Evaluate (epoch 131) -- logp(x) = -2.388 +/- 0.016\n",
      "epoch 132 / 200, step    0 / 100; loss 2.4039\n",
      "Evaluate (epoch 132) -- logp(x) = -2.388 +/- 0.016\n",
      "epoch 133 / 200, step    0 / 100; loss 2.3585\n",
      "Evaluate (epoch 133) -- logp(x) = -2.388 +/- 0.016\n",
      "epoch 134 / 200, step    0 / 100; loss 2.2849\n",
      "Evaluate (epoch 134) -- logp(x) = -2.388 +/- 0.016\n",
      "epoch 135 / 200, step    0 / 100; loss 2.4830\n",
      "Evaluate (epoch 135) -- logp(x) = -2.386 +/- 0.016\n",
      "epoch 136 / 200, step    0 / 100; loss 2.3789\n",
      "Evaluate (epoch 136) -- logp(x) = -2.392 +/- 0.016\n",
      "epoch 137 / 200, step    0 / 100; loss 2.5346\n",
      "Evaluate (epoch 137) -- logp(x) = -2.386 +/- 0.016\n",
      "epoch 138 / 200, step    0 / 100; loss 2.4421\n",
      "Evaluate (epoch 138) -- logp(x) = -2.386 +/- 0.016\n",
      "epoch 139 / 200, step    0 / 100; loss 2.3671\n",
      "Evaluate (epoch 139) -- logp(x) = -2.385 +/- 0.016\n",
      "epoch 140 / 200, step    0 / 100; loss 2.4621\n",
      "Evaluate (epoch 140) -- logp(x) = -2.385 +/- 0.016\n",
      "epoch 141 / 200, step    0 / 100; loss 2.3652\n",
      "Evaluate (epoch 141) -- logp(x) = -2.384 +/- 0.016\n",
      "epoch 142 / 200, step    0 / 100; loss 2.4505\n",
      "Evaluate (epoch 142) -- logp(x) = -2.384 +/- 0.016\n",
      "epoch 143 / 200, step    0 / 100; loss 2.3893\n",
      "Evaluate (epoch 143) -- logp(x) = -2.384 +/- 0.016\n",
      "epoch 144 / 200, step    0 / 100; loss 2.4263\n",
      "Evaluate (epoch 144) -- logp(x) = -2.383 +/- 0.016\n",
      "epoch 145 / 200, step    0 / 100; loss 2.4732\n",
      "Evaluate (epoch 145) -- logp(x) = -2.382 +/- 0.016\n",
      "epoch 146 / 200, step    0 / 100; loss 2.3913\n",
      "Evaluate (epoch 146) -- logp(x) = -2.385 +/- 0.016\n",
      "epoch 147 / 200, step    0 / 100; loss 2.2363\n",
      "Evaluate (epoch 147) -- logp(x) = -2.382 +/- 0.016\n",
      "epoch 148 / 200, step    0 / 100; loss 2.3308\n",
      "Evaluate (epoch 148) -- logp(x) = -2.381 +/- 0.016\n",
      "epoch 149 / 200, step    0 / 100; loss 2.3296\n",
      "Evaluate (epoch 149) -- logp(x) = -2.383 +/- 0.016\n",
      "epoch 150 / 200, step    0 / 100; loss 2.3849\n",
      "Evaluate (epoch 150) -- logp(x) = -2.381 +/- 0.016\n",
      "epoch 151 / 200, step    0 / 100; loss 2.4396\n",
      "Evaluate (epoch 151) -- logp(x) = -2.382 +/- 0.016\n",
      "epoch 152 / 200, step    0 / 100; loss 2.2614\n",
      "Evaluate (epoch 152) -- logp(x) = -2.380 +/- 0.016\n",
      "epoch 153 / 200, step    0 / 100; loss 2.3954\n",
      "Evaluate (epoch 153) -- logp(x) = -2.382 +/- 0.016\n",
      "epoch 154 / 200, step    0 / 100; loss 2.4550\n",
      "Evaluate (epoch 154) -- logp(x) = -2.380 +/- 0.016\n",
      "epoch 155 / 200, step    0 / 100; loss 2.3720\n",
      "Evaluate (epoch 155) -- logp(x) = -2.379 +/- 0.016\n",
      "epoch 156 / 200, step    0 / 100; loss 2.4040\n",
      "Evaluate (epoch 156) -- logp(x) = -2.379 +/- 0.016\n",
      "epoch 157 / 200, step    0 / 100; loss 2.4041\n",
      "Evaluate (epoch 157) -- logp(x) = -2.380 +/- 0.016\n",
      "epoch 158 / 200, step    0 / 100; loss 2.4996\n",
      "Evaluate (epoch 158) -- logp(x) = -2.379 +/- 0.016\n",
      "epoch 159 / 200, step    0 / 100; loss 2.2782\n",
      "Evaluate (epoch 159) -- logp(x) = -2.378 +/- 0.016\n",
      "epoch 160 / 200, step    0 / 100; loss 2.3974\n",
      "Evaluate (epoch 160) -- logp(x) = -2.378 +/- 0.016\n",
      "epoch 161 / 200, step    0 / 100; loss 2.2472\n",
      "Evaluate (epoch 161) -- logp(x) = -2.378 +/- 0.016\n",
      "epoch 162 / 200, step    0 / 100; loss 2.3883\n",
      "Evaluate (epoch 162) -- logp(x) = -2.377 +/- 0.016\n",
      "epoch 163 / 200, step    0 / 100; loss 2.5190\n",
      "Evaluate (epoch 163) -- logp(x) = -2.379 +/- 0.016\n",
      "epoch 164 / 200, step    0 / 100; loss 2.2944\n",
      "Evaluate (epoch 164) -- logp(x) = -2.377 +/- 0.016\n",
      "epoch 165 / 200, step    0 / 100; loss 2.3364\n",
      "Evaluate (epoch 165) -- logp(x) = -2.377 +/- 0.016\n",
      "epoch 166 / 200, step    0 / 100; loss 2.4259\n",
      "Evaluate (epoch 166) -- logp(x) = -2.376 +/- 0.016\n",
      "epoch 167 / 200, step    0 / 100; loss 2.3219\n",
      "Evaluate (epoch 167) -- logp(x) = -2.376 +/- 0.016\n",
      "epoch 168 / 200, step    0 / 100; loss 2.3439\n",
      "Evaluate (epoch 168) -- logp(x) = -2.377 +/- 0.016\n",
      "epoch 169 / 200, step    0 / 100; loss 2.3097\n",
      "Evaluate (epoch 169) -- logp(x) = -2.376 +/- 0.016\n",
      "epoch 170 / 200, step    0 / 100; loss 2.1664\n",
      "Evaluate (epoch 170) -- logp(x) = -2.376 +/- 0.016\n",
      "epoch 171 / 200, step    0 / 100; loss 2.3294\n",
      "Evaluate (epoch 171) -- logp(x) = -2.376 +/- 0.016\n",
      "epoch 172 / 200, step    0 / 100; loss 2.4373\n",
      "Evaluate (epoch 172) -- logp(x) = -2.376 +/- 0.016\n",
      "epoch 173 / 200, step    0 / 100; loss 2.3571\n",
      "Evaluate (epoch 173) -- logp(x) = -2.375 +/- 0.016\n",
      "epoch 174 / 200, step    0 / 100; loss 2.4443\n",
      "Evaluate (epoch 174) -- logp(x) = -2.375 +/- 0.016\n",
      "epoch 175 / 200, step    0 / 100; loss 2.4496\n",
      "Evaluate (epoch 175) -- logp(x) = -2.374 +/- 0.016\n",
      "epoch 176 / 200, step    0 / 100; loss 2.2928\n",
      "Evaluate (epoch 176) -- logp(x) = -2.375 +/- 0.016\n",
      "epoch 177 / 200, step    0 / 100; loss 2.3747\n",
      "Evaluate (epoch 177) -- logp(x) = -2.373 +/- 0.016\n",
      "epoch 178 / 200, step    0 / 100; loss 2.3658\n",
      "Evaluate (epoch 178) -- logp(x) = -2.373 +/- 0.016\n",
      "epoch 179 / 200, step    0 / 100; loss 2.3582\n",
      "Evaluate (epoch 179) -- logp(x) = -2.373 +/- 0.016\n",
      "epoch 180 / 200, step    0 / 100; loss 2.2807\n",
      "Evaluate (epoch 180) -- logp(x) = -2.376 +/- 0.016\n",
      "epoch 181 / 200, step    0 / 100; loss 2.3094\n",
      "Evaluate (epoch 181) -- logp(x) = -2.373 +/- 0.016\n",
      "epoch 182 / 200, step    0 / 100; loss 2.2227\n",
      "Evaluate (epoch 182) -- logp(x) = -2.373 +/- 0.016\n",
      "epoch 183 / 200, step    0 / 100; loss 2.3350\n",
      "Evaluate (epoch 183) -- logp(x) = -2.372 +/- 0.016\n",
      "epoch 184 / 200, step    0 / 100; loss 2.3369\n",
      "Evaluate (epoch 184) -- logp(x) = -2.372 +/- 0.016\n",
      "epoch 185 / 200, step    0 / 100; loss 2.3883\n",
      "Evaluate (epoch 185) -- logp(x) = -2.372 +/- 0.016\n",
      "epoch 186 / 200, step    0 / 100; loss 2.4032\n",
      "Evaluate (epoch 186) -- logp(x) = -2.372 +/- 0.016\n",
      "epoch 187 / 200, step    0 / 100; loss 2.5082\n",
      "Evaluate (epoch 187) -- logp(x) = -2.371 +/- 0.016\n",
      "epoch 188 / 200, step    0 / 100; loss 2.4262\n",
      "Evaluate (epoch 188) -- logp(x) = -2.371 +/- 0.016\n",
      "epoch 189 / 200, step    0 / 100; loss 2.4411\n",
      "Evaluate (epoch 189) -- logp(x) = -2.371 +/- 0.016\n",
      "epoch 190 / 200, step    0 / 100; loss 2.2517\n",
      "Evaluate (epoch 190) -- logp(x) = -2.371 +/- 0.016\n",
      "epoch 191 / 200, step    0 / 100; loss 2.4410\n",
      "Evaluate (epoch 191) -- logp(x) = -2.371 +/- 0.016\n",
      "epoch 192 / 200, step    0 / 100; loss 2.4496\n",
      "Evaluate (epoch 192) -- logp(x) = -2.370 +/- 0.016\n",
      "epoch 193 / 200, step    0 / 100; loss 2.5415\n",
      "Evaluate (epoch 193) -- logp(x) = -2.369 +/- 0.016\n",
      "epoch 194 / 200, step    0 / 100; loss 2.3002\n",
      "Evaluate (epoch 194) -- logp(x) = -2.369 +/- 0.016\n",
      "epoch 195 / 200, step    0 / 100; loss 2.4991\n",
      "Evaluate (epoch 195) -- logp(x) = -2.370 +/- 0.016\n",
      "epoch 196 / 200, step    0 / 100; loss 2.3211\n",
      "Evaluate (epoch 196) -- logp(x) = -2.369 +/- 0.016\n",
      "epoch 197 / 200, step    0 / 100; loss 2.3800\n",
      "Evaluate (epoch 197) -- logp(x) = -2.370 +/- 0.016\n",
      "epoch 198 / 200, step    0 / 100; loss 2.4629\n",
      "Evaluate (epoch 198) -- logp(x) = -2.369 +/- 0.016\n",
      "epoch 199 / 200, step    0 / 100; loss 2.3619\n",
      "Evaluate (epoch 199) -- logp(x) = -2.369 +/- 0.016\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'TORUS',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': True,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 3,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 3,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'made',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 50,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': False,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/made',\n",
      " 'restore_file': './results/made/best_model_checkpoint.pt',\n",
      " 'results_file': './results/made\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 199,\n",
      " 'train': False}\n",
      "MADE(\n",
      "  (net_input): MaskedLinear(in_features=3, out_features=100, bias=True)\n",
      "  (net): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaskedLinear(in_features=100, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'INVOLUTE',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': False,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 2,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 2,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'made',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 200,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': False,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/made',\n",
      " 'restore_file': None,\n",
      " 'results_file': './results/made\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 0,\n",
      " 'train': True}\n",
      "MADE(\n",
      "  (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "  (net): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch   0 / 200, step    0 / 100; loss 2.0033\n",
      "Evaluate (epoch 0) -- logp(x) = -1.589 +/- 0.006\n",
      "epoch   1 / 200, step    0 / 100; loss 1.5199\n",
      "Evaluate (epoch 1) -- logp(x) = -1.482 +/- 0.012\n",
      "epoch   2 / 200, step    0 / 100; loss 1.4995\n",
      "Evaluate (epoch 2) -- logp(x) = -1.469 +/- 0.013\n",
      "epoch   3 / 200, step    0 / 100; loss 1.5009\n",
      "Evaluate (epoch 3) -- logp(x) = -1.458 +/- 0.012\n",
      "epoch   4 / 200, step    0 / 100; loss 1.4306\n",
      "Evaluate (epoch 4) -- logp(x) = -1.449 +/- 0.012\n",
      "epoch   5 / 200, step    0 / 100; loss 1.4666\n",
      "Evaluate (epoch 5) -- logp(x) = -1.439 +/- 0.012\n",
      "epoch   6 / 200, step    0 / 100; loss 1.4135\n",
      "Evaluate (epoch 6) -- logp(x) = -1.430 +/- 0.012\n",
      "epoch   7 / 200, step    0 / 100; loss 1.4833\n",
      "Evaluate (epoch 7) -- logp(x) = -1.420 +/- 0.012\n",
      "epoch   8 / 200, step    0 / 100; loss 1.4029\n",
      "Evaluate (epoch 8) -- logp(x) = -1.411 +/- 0.012\n",
      "epoch   9 / 200, step    0 / 100; loss 1.3223\n",
      "Evaluate (epoch 9) -- logp(x) = -1.402 +/- 0.012\n",
      "epoch  10 / 200, step    0 / 100; loss 1.3993\n",
      "Evaluate (epoch 10) -- logp(x) = -1.393 +/- 0.012\n",
      "epoch  11 / 200, step    0 / 100; loss 1.4095\n",
      "Evaluate (epoch 11) -- logp(x) = -1.384 +/- 0.013\n",
      "epoch  12 / 200, step    0 / 100; loss 1.4081\n",
      "Evaluate (epoch 12) -- logp(x) = -1.376 +/- 0.012\n",
      "epoch  13 / 200, step    0 / 100; loss 1.3566\n",
      "Evaluate (epoch 13) -- logp(x) = -1.367 +/- 0.013\n",
      "epoch  14 / 200, step    0 / 100; loss 1.4384\n",
      "Evaluate (epoch 14) -- logp(x) = -1.358 +/- 0.013\n",
      "epoch  15 / 200, step    0 / 100; loss 1.3515\n",
      "Evaluate (epoch 15) -- logp(x) = -1.349 +/- 0.012\n",
      "epoch  16 / 200, step    0 / 100; loss 1.3245\n",
      "Evaluate (epoch 16) -- logp(x) = -1.340 +/- 0.013\n",
      "epoch  17 / 200, step    0 / 100; loss 1.3479\n",
      "Evaluate (epoch 17) -- logp(x) = -1.332 +/- 0.012\n",
      "epoch  18 / 200, step    0 / 100; loss 1.3122\n",
      "Evaluate (epoch 18) -- logp(x) = -1.322 +/- 0.012\n",
      "epoch  19 / 200, step    0 / 100; loss 1.3965\n",
      "Evaluate (epoch 19) -- logp(x) = -1.314 +/- 0.012\n",
      "epoch  20 / 200, step    0 / 100; loss 1.3398\n",
      "Evaluate (epoch 20) -- logp(x) = -1.305 +/- 0.012\n",
      "epoch  21 / 200, step    0 / 100; loss 1.3757\n",
      "Evaluate (epoch 21) -- logp(x) = -1.296 +/- 0.013\n",
      "epoch  22 / 200, step    0 / 100; loss 1.2918\n",
      "Evaluate (epoch 22) -- logp(x) = -1.288 +/- 0.012\n",
      "epoch  23 / 200, step    0 / 100; loss 1.2736\n",
      "Evaluate (epoch 23) -- logp(x) = -1.280 +/- 0.012\n",
      "epoch  24 / 200, step    0 / 100; loss 1.3607\n",
      "Evaluate (epoch 24) -- logp(x) = -1.271 +/- 0.013\n",
      "epoch  25 / 200, step    0 / 100; loss 1.3259\n",
      "Evaluate (epoch 25) -- logp(x) = -1.262 +/- 0.013\n",
      "epoch  26 / 200, step    0 / 100; loss 1.3198\n",
      "Evaluate (epoch 26) -- logp(x) = -1.256 +/- 0.013\n",
      "epoch  27 / 200, step    0 / 100; loss 1.2131\n",
      "Evaluate (epoch 27) -- logp(x) = -1.248 +/- 0.013\n",
      "epoch  28 / 200, step    0 / 100; loss 1.2670\n",
      "Evaluate (epoch 28) -- logp(x) = -1.238 +/- 0.012\n",
      "epoch  29 / 200, step    0 / 100; loss 1.2666\n",
      "Evaluate (epoch 29) -- logp(x) = -1.231 +/- 0.013\n",
      "epoch  30 / 200, step    0 / 100; loss 1.1590\n",
      "Evaluate (epoch 30) -- logp(x) = -1.223 +/- 0.012\n",
      "epoch  31 / 200, step    0 / 100; loss 1.1979\n",
      "Evaluate (epoch 31) -- logp(x) = -1.215 +/- 0.013\n",
      "epoch  32 / 200, step    0 / 100; loss 1.1617\n",
      "Evaluate (epoch 32) -- logp(x) = -1.207 +/- 0.013\n",
      "epoch  33 / 200, step    0 / 100; loss 1.1710\n",
      "Evaluate (epoch 33) -- logp(x) = -1.200 +/- 0.013\n",
      "epoch  34 / 200, step    0 / 100; loss 1.2736\n",
      "Evaluate (epoch 34) -- logp(x) = -1.192 +/- 0.013\n",
      "epoch  35 / 200, step    0 / 100; loss 1.2080\n",
      "Evaluate (epoch 35) -- logp(x) = -1.185 +/- 0.013\n",
      "epoch  36 / 200, step    0 / 100; loss 1.2689\n",
      "Evaluate (epoch 36) -- logp(x) = -1.179 +/- 0.013\n",
      "epoch  37 / 200, step    0 / 100; loss 1.2160\n",
      "Evaluate (epoch 37) -- logp(x) = -1.171 +/- 0.013\n",
      "epoch  38 / 200, step    0 / 100; loss 1.2272\n",
      "Evaluate (epoch 38) -- logp(x) = -1.164 +/- 0.013\n",
      "epoch  39 / 200, step    0 / 100; loss 1.0177\n",
      "Evaluate (epoch 39) -- logp(x) = -1.158 +/- 0.013\n",
      "epoch  40 / 200, step    0 / 100; loss 1.0007\n",
      "Evaluate (epoch 40) -- logp(x) = -1.150 +/- 0.013\n",
      "epoch  41 / 200, step    0 / 100; loss 1.1382\n",
      "Evaluate (epoch 41) -- logp(x) = -1.145 +/- 0.013\n",
      "epoch  42 / 200, step    0 / 100; loss 1.2370\n",
      "Evaluate (epoch 42) -- logp(x) = -1.137 +/- 0.013\n",
      "epoch  43 / 200, step    0 / 100; loss 1.0872\n",
      "Evaluate (epoch 43) -- logp(x) = -1.130 +/- 0.013\n",
      "epoch  44 / 200, step    0 / 100; loss 1.1821\n",
      "Evaluate (epoch 44) -- logp(x) = -1.125 +/- 0.013\n",
      "epoch  45 / 200, step    0 / 100; loss 1.1355\n",
      "Evaluate (epoch 45) -- logp(x) = -1.118 +/- 0.013\n",
      "epoch  46 / 200, step    0 / 100; loss 1.0652\n",
      "Evaluate (epoch 46) -- logp(x) = -1.113 +/- 0.013\n",
      "epoch  47 / 200, step    0 / 100; loss 1.2095\n",
      "Evaluate (epoch 47) -- logp(x) = -1.107 +/- 0.013\n",
      "epoch  48 / 200, step    0 / 100; loss 1.0769\n",
      "Evaluate (epoch 48) -- logp(x) = -1.100 +/- 0.013\n",
      "epoch  49 / 200, step    0 / 100; loss 1.2123\n",
      "Evaluate (epoch 49) -- logp(x) = -1.094 +/- 0.013\n",
      "epoch  50 / 200, step    0 / 100; loss 1.1320\n",
      "Evaluate (epoch 50) -- logp(x) = -1.092 +/- 0.013\n",
      "epoch  51 / 200, step    0 / 100; loss 1.1358\n",
      "Evaluate (epoch 51) -- logp(x) = -1.083 +/- 0.014\n",
      "epoch  52 / 200, step    0 / 100; loss 1.1454\n",
      "Evaluate (epoch 52) -- logp(x) = -1.078 +/- 0.014\n",
      "epoch  53 / 200, step    0 / 100; loss 1.0732\n",
      "Evaluate (epoch 53) -- logp(x) = -1.073 +/- 0.014\n",
      "epoch  54 / 200, step    0 / 100; loss 1.0956\n",
      "Evaluate (epoch 54) -- logp(x) = -1.067 +/- 0.014\n",
      "epoch  55 / 200, step    0 / 100; loss 1.0495\n",
      "Evaluate (epoch 55) -- logp(x) = -1.061 +/- 0.013\n",
      "epoch  56 / 200, step    0 / 100; loss 1.0436\n",
      "Evaluate (epoch 56) -- logp(x) = -1.056 +/- 0.013\n",
      "epoch  57 / 200, step    0 / 100; loss 1.0508\n",
      "Evaluate (epoch 57) -- logp(x) = -1.052 +/- 0.014\n",
      "epoch  58 / 200, step    0 / 100; loss 0.9739\n",
      "Evaluate (epoch 58) -- logp(x) = -1.047 +/- 0.014\n",
      "epoch  59 / 200, step    0 / 100; loss 1.1518\n",
      "Evaluate (epoch 59) -- logp(x) = -1.042 +/- 0.014\n",
      "epoch  60 / 200, step    0 / 100; loss 0.9860\n",
      "Evaluate (epoch 60) -- logp(x) = -1.037 +/- 0.014\n",
      "epoch  61 / 200, step    0 / 100; loss 0.8398\n",
      "Evaluate (epoch 61) -- logp(x) = -1.031 +/- 0.014\n",
      "epoch  62 / 200, step    0 / 100; loss 1.0015\n",
      "Evaluate (epoch 62) -- logp(x) = -1.028 +/- 0.015\n",
      "epoch  63 / 200, step    0 / 100; loss 1.0077\n",
      "Evaluate (epoch 63) -- logp(x) = -1.023 +/- 0.014\n",
      "epoch  64 / 200, step    0 / 100; loss 0.9270\n",
      "Evaluate (epoch 64) -- logp(x) = -1.017 +/- 0.014\n",
      "epoch  65 / 200, step    0 / 100; loss 0.9596\n",
      "Evaluate (epoch 65) -- logp(x) = -1.014 +/- 0.014\n",
      "epoch  66 / 200, step    0 / 100; loss 1.0178\n",
      "Evaluate (epoch 66) -- logp(x) = -1.010 +/- 0.014\n",
      "epoch  67 / 200, step    0 / 100; loss 1.1055\n",
      "Evaluate (epoch 67) -- logp(x) = -1.005 +/- 0.014\n",
      "epoch  68 / 200, step    0 / 100; loss 0.9678\n",
      "Evaluate (epoch 68) -- logp(x) = -1.001 +/- 0.014\n",
      "epoch  69 / 200, step    0 / 100; loss 0.9491\n",
      "Evaluate (epoch 69) -- logp(x) = -0.997 +/- 0.014\n",
      "epoch  70 / 200, step    0 / 100; loss 1.0161\n",
      "Evaluate (epoch 70) -- logp(x) = -0.994 +/- 0.014\n",
      "epoch  71 / 200, step    0 / 100; loss 1.0791\n",
      "Evaluate (epoch 71) -- logp(x) = -0.990 +/- 0.015\n",
      "epoch  72 / 200, step    0 / 100; loss 1.0192\n",
      "Evaluate (epoch 72) -- logp(x) = -0.987 +/- 0.014\n",
      "epoch  73 / 200, step    0 / 100; loss 1.0313\n",
      "Evaluate (epoch 73) -- logp(x) = -0.983 +/- 0.014\n",
      "epoch  74 / 200, step    0 / 100; loss 0.9610\n",
      "Evaluate (epoch 74) -- logp(x) = -0.980 +/- 0.014\n",
      "epoch  75 / 200, step    0 / 100; loss 0.8831\n",
      "Evaluate (epoch 75) -- logp(x) = -0.984 +/- 0.015\n",
      "epoch  76 / 200, step    0 / 100; loss 0.9799\n",
      "Evaluate (epoch 76) -- logp(x) = -0.973 +/- 0.014\n",
      "epoch  77 / 200, step    0 / 100; loss 1.0355\n",
      "Evaluate (epoch 77) -- logp(x) = -0.971 +/- 0.015\n",
      "epoch  78 / 200, step    0 / 100; loss 1.0157\n",
      "Evaluate (epoch 78) -- logp(x) = -0.971 +/- 0.014\n",
      "epoch  79 / 200, step    0 / 100; loss 1.0101\n",
      "Evaluate (epoch 79) -- logp(x) = -0.967 +/- 0.014\n",
      "epoch  80 / 200, step    0 / 100; loss 1.0097\n",
      "Evaluate (epoch 80) -- logp(x) = -0.964 +/- 0.014\n",
      "epoch  81 / 200, step    0 / 100; loss 0.9609\n",
      "Evaluate (epoch 81) -- logp(x) = -0.962 +/- 0.015\n",
      "epoch  82 / 200, step    0 / 100; loss 0.8794\n",
      "Evaluate (epoch 82) -- logp(x) = -0.959 +/- 0.015\n",
      "epoch  83 / 200, step    0 / 100; loss 0.9767\n",
      "Evaluate (epoch 83) -- logp(x) = -0.959 +/- 0.015\n",
      "epoch  84 / 200, step    0 / 100; loss 1.0680\n",
      "Evaluate (epoch 84) -- logp(x) = -0.954 +/- 0.015\n",
      "epoch  85 / 200, step    0 / 100; loss 0.8869\n",
      "Evaluate (epoch 85) -- logp(x) = -0.953 +/- 0.015\n",
      "epoch  86 / 200, step    0 / 100; loss 0.9131\n",
      "Evaluate (epoch 86) -- logp(x) = -0.951 +/- 0.015\n",
      "epoch  87 / 200, step    0 / 100; loss 0.8123\n",
      "Evaluate (epoch 87) -- logp(x) = -0.951 +/- 0.015\n",
      "epoch  88 / 200, step    0 / 100; loss 0.8871\n",
      "Evaluate (epoch 88) -- logp(x) = -0.948 +/- 0.015\n",
      "epoch  89 / 200, step    0 / 100; loss 0.9703\n",
      "Evaluate (epoch 89) -- logp(x) = -0.947 +/- 0.015\n",
      "epoch  90 / 200, step    0 / 100; loss 0.9840\n",
      "Evaluate (epoch 90) -- logp(x) = -0.948 +/- 0.015\n",
      "epoch  91 / 200, step    0 / 100; loss 0.7974\n",
      "Evaluate (epoch 91) -- logp(x) = -0.945 +/- 0.015\n",
      "epoch  92 / 200, step    0 / 100; loss 0.9742\n",
      "Evaluate (epoch 92) -- logp(x) = -0.942 +/- 0.015\n",
      "epoch  93 / 200, step    0 / 100; loss 1.1398\n",
      "Evaluate (epoch 93) -- logp(x) = -0.943 +/- 0.016\n",
      "epoch  94 / 200, step    0 / 100; loss 0.9191\n",
      "Evaluate (epoch 94) -- logp(x) = -0.944 +/- 0.015\n",
      "epoch  95 / 200, step    0 / 100; loss 0.8889\n",
      "Evaluate (epoch 95) -- logp(x) = -0.942 +/- 0.015\n",
      "epoch  96 / 200, step    0 / 100; loss 0.9750\n",
      "Evaluate (epoch 96) -- logp(x) = -0.942 +/- 0.016\n",
      "epoch  97 / 200, step    0 / 100; loss 0.9037\n",
      "Evaluate (epoch 97) -- logp(x) = -0.937 +/- 0.016\n",
      "epoch  98 / 200, step    0 / 100; loss 1.0396\n",
      "Evaluate (epoch 98) -- logp(x) = -0.940 +/- 0.016\n",
      "epoch  99 / 200, step    0 / 100; loss 1.0028\n",
      "Evaluate (epoch 99) -- logp(x) = -0.936 +/- 0.015\n",
      "epoch 100 / 200, step    0 / 100; loss 0.9721\n",
      "Evaluate (epoch 100) -- logp(x) = -0.935 +/- 0.016\n",
      "epoch 101 / 200, step    0 / 100; loss 0.9935\n",
      "Evaluate (epoch 101) -- logp(x) = -0.934 +/- 0.016\n",
      "epoch 102 / 200, step    0 / 100; loss 0.9342\n",
      "Evaluate (epoch 102) -- logp(x) = -0.934 +/- 0.016\n",
      "epoch 103 / 200, step    0 / 100; loss 0.8332\n",
      "Evaluate (epoch 103) -- logp(x) = -0.934 +/- 0.016\n",
      "epoch 104 / 200, step    0 / 100; loss 0.8285\n",
      "Evaluate (epoch 104) -- logp(x) = -0.934 +/- 0.016\n",
      "epoch 105 / 200, step    0 / 100; loss 1.0855\n",
      "Evaluate (epoch 105) -- logp(x) = -0.934 +/- 0.016\n",
      "epoch 106 / 200, step    0 / 100; loss 0.9463\n",
      "Evaluate (epoch 106) -- logp(x) = -0.932 +/- 0.016\n",
      "epoch 107 / 200, step    0 / 100; loss 0.9746\n",
      "Evaluate (epoch 107) -- logp(x) = -0.940 +/- 0.017\n",
      "epoch 108 / 200, step    0 / 100; loss 0.9383\n",
      "Evaluate (epoch 108) -- logp(x) = -0.932 +/- 0.015\n",
      "epoch 109 / 200, step    0 / 100; loss 1.0053\n",
      "Evaluate (epoch 109) -- logp(x) = -0.932 +/- 0.016\n",
      "epoch 110 / 200, step    0 / 100; loss 0.9176\n",
      "Evaluate (epoch 110) -- logp(x) = -0.930 +/- 0.016\n",
      "epoch 111 / 200, step    0 / 100; loss 0.8693\n",
      "Evaluate (epoch 111) -- logp(x) = -0.930 +/- 0.016\n",
      "epoch 112 / 200, step    0 / 100; loss 0.8467\n",
      "Evaluate (epoch 112) -- logp(x) = -0.932 +/- 0.016\n",
      "epoch 113 / 200, step    0 / 100; loss 0.9966\n",
      "Evaluate (epoch 113) -- logp(x) = -0.929 +/- 0.016\n",
      "epoch 114 / 200, step    0 / 100; loss 0.8210\n",
      "Evaluate (epoch 114) -- logp(x) = -0.930 +/- 0.016\n",
      "epoch 115 / 200, step    0 / 100; loss 0.8824\n",
      "Evaluate (epoch 115) -- logp(x) = -0.929 +/- 0.016\n",
      "epoch 116 / 200, step    0 / 100; loss 1.0262\n",
      "Evaluate (epoch 116) -- logp(x) = -0.928 +/- 0.016\n",
      "epoch 117 / 200, step    0 / 100; loss 0.8119\n",
      "Evaluate (epoch 117) -- logp(x) = -0.928 +/- 0.016\n",
      "epoch 118 / 200, step    0 / 100; loss 0.7688\n",
      "Evaluate (epoch 118) -- logp(x) = -0.931 +/- 0.016\n",
      "epoch 119 / 200, step    0 / 100; loss 0.9214\n",
      "Evaluate (epoch 119) -- logp(x) = -0.931 +/- 0.016\n",
      "epoch 120 / 200, step    0 / 100; loss 0.9650\n",
      "Evaluate (epoch 120) -- logp(x) = -0.927 +/- 0.016\n",
      "epoch 121 / 200, step    0 / 100; loss 0.9866\n",
      "Evaluate (epoch 121) -- logp(x) = -0.928 +/- 0.016\n",
      "epoch 122 / 200, step    0 / 100; loss 0.9800\n",
      "Evaluate (epoch 122) -- logp(x) = -0.927 +/- 0.016\n",
      "epoch 123 / 200, step    0 / 100; loss 1.0841\n",
      "Evaluate (epoch 123) -- logp(x) = -0.928 +/- 0.016\n",
      "epoch 124 / 200, step    0 / 100; loss 0.9509\n",
      "Evaluate (epoch 124) -- logp(x) = -0.928 +/- 0.016\n",
      "epoch 125 / 200, step    0 / 100; loss 0.9938\n",
      "Evaluate (epoch 125) -- logp(x) = -0.935 +/- 0.016\n",
      "epoch 126 / 200, step    0 / 100; loss 0.9121\n",
      "Evaluate (epoch 126) -- logp(x) = -0.933 +/- 0.017\n",
      "epoch 127 / 200, step    0 / 100; loss 0.9319\n",
      "Evaluate (epoch 127) -- logp(x) = -0.930 +/- 0.016\n",
      "epoch 128 / 200, step    0 / 100; loss 0.9406\n",
      "Evaluate (epoch 128) -- logp(x) = -0.926 +/- 0.016\n",
      "epoch 129 / 200, step    0 / 100; loss 0.8551\n",
      "Evaluate (epoch 129) -- logp(x) = -0.925 +/- 0.016\n",
      "epoch 130 / 200, step    0 / 100; loss 0.8968\n",
      "Evaluate (epoch 130) -- logp(x) = -0.925 +/- 0.016\n",
      "epoch 131 / 200, step    0 / 100; loss 0.9728\n",
      "Evaluate (epoch 131) -- logp(x) = -0.926 +/- 0.016\n",
      "epoch 132 / 200, step    0 / 100; loss 0.9895\n",
      "Evaluate (epoch 132) -- logp(x) = -0.925 +/- 0.016\n",
      "epoch 133 / 200, step    0 / 100; loss 1.1473\n",
      "Evaluate (epoch 133) -- logp(x) = -0.925 +/- 0.016\n",
      "epoch 134 / 200, step    0 / 100; loss 0.8721\n",
      "Evaluate (epoch 134) -- logp(x) = -0.932 +/- 0.016\n",
      "epoch 135 / 200, step    0 / 100; loss 0.8876\n",
      "Evaluate (epoch 135) -- logp(x) = -0.927 +/- 0.016\n",
      "epoch 136 / 200, step    0 / 100; loss 0.8638\n",
      "Evaluate (epoch 136) -- logp(x) = -0.925 +/- 0.016\n",
      "epoch 137 / 200, step    0 / 100; loss 0.9890\n",
      "Evaluate (epoch 137) -- logp(x) = -0.924 +/- 0.016\n",
      "epoch 138 / 200, step    0 / 100; loss 0.7909\n",
      "Evaluate (epoch 138) -- logp(x) = -0.924 +/- 0.016\n",
      "epoch 139 / 200, step    0 / 100; loss 0.9113\n",
      "Evaluate (epoch 139) -- logp(x) = -0.924 +/- 0.016\n",
      "epoch 140 / 200, step    0 / 100; loss 1.0101\n",
      "Evaluate (epoch 140) -- logp(x) = -0.923 +/- 0.016\n",
      "epoch 141 / 200, step    0 / 100; loss 0.9912\n",
      "Evaluate (epoch 141) -- logp(x) = -0.926 +/- 0.016\n",
      "epoch 142 / 200, step    0 / 100; loss 0.8117\n",
      "Evaluate (epoch 142) -- logp(x) = -0.923 +/- 0.017\n",
      "epoch 143 / 200, step    0 / 100; loss 0.7527\n",
      "Evaluate (epoch 143) -- logp(x) = -0.924 +/- 0.016\n",
      "epoch 144 / 200, step    0 / 100; loss 0.9595\n",
      "Evaluate (epoch 144) -- logp(x) = -0.926 +/- 0.016\n",
      "epoch 145 / 200, step    0 / 100; loss 0.8174\n",
      "Evaluate (epoch 145) -- logp(x) = -0.922 +/- 0.016\n",
      "epoch 146 / 200, step    0 / 100; loss 0.8721\n",
      "Evaluate (epoch 146) -- logp(x) = -0.922 +/- 0.017\n",
      "epoch 147 / 200, step    0 / 100; loss 0.9632\n",
      "Evaluate (epoch 147) -- logp(x) = -0.922 +/- 0.016\n",
      "epoch 148 / 200, step    0 / 100; loss 0.8666\n",
      "Evaluate (epoch 148) -- logp(x) = -0.923 +/- 0.016\n",
      "epoch 149 / 200, step    0 / 100; loss 0.8615\n",
      "Evaluate (epoch 149) -- logp(x) = -0.921 +/- 0.016\n",
      "epoch 150 / 200, step    0 / 100; loss 1.0421\n",
      "Evaluate (epoch 150) -- logp(x) = -0.921 +/- 0.016\n",
      "epoch 151 / 200, step    0 / 100; loss 0.9352\n",
      "Evaluate (epoch 151) -- logp(x) = -0.923 +/- 0.016\n",
      "epoch 152 / 200, step    0 / 100; loss 1.0573\n",
      "Evaluate (epoch 152) -- logp(x) = -0.921 +/- 0.017\n",
      "epoch 153 / 200, step    0 / 100; loss 1.0069\n",
      "Evaluate (epoch 153) -- logp(x) = -0.923 +/- 0.017\n",
      "epoch 154 / 200, step    0 / 100; loss 0.8634\n",
      "Evaluate (epoch 154) -- logp(x) = -0.921 +/- 0.016\n",
      "epoch 155 / 200, step    0 / 100; loss 0.9608\n",
      "Evaluate (epoch 155) -- logp(x) = -0.920 +/- 0.017\n",
      "epoch 156 / 200, step    0 / 100; loss 0.9329\n",
      "Evaluate (epoch 156) -- logp(x) = -0.920 +/- 0.016\n",
      "epoch 157 / 200, step    0 / 100; loss 0.9181\n",
      "Evaluate (epoch 157) -- logp(x) = -0.920 +/- 0.016\n",
      "epoch 158 / 200, step    0 / 100; loss 0.9721\n",
      "Evaluate (epoch 158) -- logp(x) = -0.920 +/- 0.016\n",
      "epoch 159 / 200, step    0 / 100; loss 1.0106\n",
      "Evaluate (epoch 159) -- logp(x) = -0.925 +/- 0.017\n",
      "epoch 160 / 200, step    0 / 100; loss 0.8991\n",
      "Evaluate (epoch 160) -- logp(x) = -0.920 +/- 0.017\n",
      "epoch 161 / 200, step    0 / 100; loss 0.9494\n",
      "Evaluate (epoch 161) -- logp(x) = -0.920 +/- 0.016\n",
      "epoch 162 / 200, step    0 / 100; loss 0.9265\n",
      "Evaluate (epoch 162) -- logp(x) = -0.920 +/- 0.017\n",
      "epoch 163 / 200, step    0 / 100; loss 0.9305\n",
      "Evaluate (epoch 163) -- logp(x) = -0.918 +/- 0.016\n",
      "epoch 164 / 200, step    0 / 100; loss 0.9454\n",
      "Evaluate (epoch 164) -- logp(x) = -0.918 +/- 0.016\n",
      "epoch 165 / 200, step    0 / 100; loss 0.9452\n",
      "Evaluate (epoch 165) -- logp(x) = -0.917 +/- 0.016\n",
      "epoch 166 / 200, step    0 / 100; loss 0.8392\n",
      "Evaluate (epoch 166) -- logp(x) = -0.917 +/- 0.016\n",
      "epoch 167 / 200, step    0 / 100; loss 1.0620\n",
      "Evaluate (epoch 167) -- logp(x) = -0.917 +/- 0.016\n",
      "epoch 168 / 200, step    0 / 100; loss 0.9551\n",
      "Evaluate (epoch 168) -- logp(x) = -0.917 +/- 0.016\n",
      "epoch 169 / 200, step    0 / 100; loss 0.9047\n",
      "Evaluate (epoch 169) -- logp(x) = -0.918 +/- 0.016\n",
      "epoch 170 / 200, step    0 / 100; loss 0.9015\n",
      "Evaluate (epoch 170) -- logp(x) = -0.916 +/- 0.016\n",
      "epoch 171 / 200, step    0 / 100; loss 0.9634\n",
      "Evaluate (epoch 171) -- logp(x) = -0.918 +/- 0.016\n",
      "epoch 172 / 200, step    0 / 100; loss 0.8378\n",
      "Evaluate (epoch 172) -- logp(x) = -0.920 +/- 0.017\n",
      "epoch 173 / 200, step    0 / 100; loss 0.8500\n",
      "Evaluate (epoch 173) -- logp(x) = -0.916 +/- 0.016\n",
      "epoch 174 / 200, step    0 / 100; loss 0.9724\n",
      "Evaluate (epoch 174) -- logp(x) = -0.924 +/- 0.017\n",
      "epoch 175 / 200, step    0 / 100; loss 0.8940\n",
      "Evaluate (epoch 175) -- logp(x) = -0.915 +/- 0.017\n",
      "epoch 176 / 200, step    0 / 100; loss 0.9869\n",
      "Evaluate (epoch 176) -- logp(x) = -0.917 +/- 0.017\n",
      "epoch 177 / 200, step    0 / 100; loss 0.9045\n",
      "Evaluate (epoch 177) -- logp(x) = -0.916 +/- 0.017\n",
      "epoch 178 / 200, step    0 / 100; loss 0.9693\n",
      "Evaluate (epoch 178) -- logp(x) = -0.915 +/- 0.017\n",
      "epoch 179 / 200, step    0 / 100; loss 0.8542\n",
      "Evaluate (epoch 179) -- logp(x) = -0.915 +/- 0.016\n",
      "epoch 180 / 200, step    0 / 100; loss 0.8702\n",
      "Evaluate (epoch 180) -- logp(x) = -0.921 +/- 0.016\n",
      "epoch 181 / 200, step    0 / 100; loss 0.8050\n",
      "Evaluate (epoch 181) -- logp(x) = -0.916 +/- 0.016\n",
      "epoch 182 / 200, step    0 / 100; loss 0.8616\n",
      "Evaluate (epoch 182) -- logp(x) = -0.914 +/- 0.017\n",
      "epoch 183 / 200, step    0 / 100; loss 0.8995\n",
      "Evaluate (epoch 183) -- logp(x) = -0.915 +/- 0.016\n",
      "epoch 184 / 200, step    0 / 100; loss 0.9427\n",
      "Evaluate (epoch 184) -- logp(x) = -0.915 +/- 0.016\n",
      "epoch 185 / 200, step    0 / 100; loss 0.8668\n",
      "Evaluate (epoch 185) -- logp(x) = -0.926 +/- 0.017\n",
      "epoch 186 / 200, step    0 / 100; loss 0.8174\n",
      "Evaluate (epoch 186) -- logp(x) = -0.915 +/- 0.016\n",
      "epoch 187 / 200, step    0 / 100; loss 0.9002\n",
      "Evaluate (epoch 187) -- logp(x) = -0.914 +/- 0.016\n",
      "epoch 188 / 200, step    0 / 100; loss 1.0159\n",
      "Evaluate (epoch 188) -- logp(x) = -0.913 +/- 0.017\n",
      "epoch 189 / 200, step    0 / 100; loss 0.8801\n",
      "Evaluate (epoch 189) -- logp(x) = -0.913 +/- 0.016\n",
      "epoch 190 / 200, step    0 / 100; loss 0.9942\n",
      "Evaluate (epoch 190) -- logp(x) = -0.916 +/- 0.016\n",
      "epoch 191 / 200, step    0 / 100; loss 1.0477\n",
      "Evaluate (epoch 191) -- logp(x) = -0.913 +/- 0.017\n",
      "epoch 192 / 200, step    0 / 100; loss 0.7569\n",
      "Evaluate (epoch 192) -- logp(x) = -0.913 +/- 0.016\n",
      "epoch 193 / 200, step    0 / 100; loss 1.0028\n",
      "Evaluate (epoch 193) -- logp(x) = -0.912 +/- 0.016\n",
      "epoch 194 / 200, step    0 / 100; loss 0.8777\n",
      "Evaluate (epoch 194) -- logp(x) = -0.913 +/- 0.016\n",
      "epoch 195 / 200, step    0 / 100; loss 0.8217\n",
      "Evaluate (epoch 195) -- logp(x) = -0.916 +/- 0.017\n",
      "epoch 196 / 200, step    0 / 100; loss 0.9966\n",
      "Evaluate (epoch 196) -- logp(x) = -0.912 +/- 0.016\n",
      "epoch 197 / 200, step    0 / 100; loss 0.9840\n",
      "Evaluate (epoch 197) -- logp(x) = -0.914 +/- 0.017\n",
      "epoch 198 / 200, step    0 / 100; loss 0.9555\n",
      "Evaluate (epoch 198) -- logp(x) = -0.918 +/- 0.017\n",
      "epoch 199 / 200, step    0 / 100; loss 0.9191\n",
      "Evaluate (epoch 199) -- logp(x) = -0.911 +/- 0.016\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'INVOLUTE',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': True,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 2,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 2,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'made',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 50,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': False,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/made',\n",
      " 'restore_file': './results/made/best_model_checkpoint.pt',\n",
      " 'results_file': './results/made\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 200,\n",
      " 'train': False}\n",
      "MADE(\n",
      "  (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "  (net): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "!python normalizing_flows/maf.py --train --model made --dataset CIRCLE --n_epochs 200\n",
    "!python normalizing_flows/maf.py --generate --model made --dataset CIRCLE --restore_file ./results/made/best_model_checkpoint.pt\n",
    "!python normalizing_flows/maf.py --train --model made --dataset TORUS --n_epochs 200\n",
    "!python normalizing_flows/maf.py --generate --model made --dataset TORUS --restore_file ./results/made/best_model_checkpoint.pt\n",
    "!python normalizing_flows/maf.py --train --model made --dataset INVOLUTE --n_epochs 200\n",
    "!python normalizing_flows/maf.py --generate --model made --dataset INVOLUTE --restore_file ./results/made/best_model_checkpoint.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'CIRCLE',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': False,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 2,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 2,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'maf',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 200,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': True,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/maf',\n",
      " 'restore_file': None,\n",
      " 'results_file': './results/maf\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 0,\n",
      " 'train': True}\n",
      "MAF(\n",
      "  (net): FlowSequential(\n",
      "    (0): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch   0 / 200, step    0 / 100; loss 2.3375\n",
      "Evaluate (epoch 0) -- logp(x) = -1.957 +/- 0.004\n",
      "epoch   1 / 200, step    0 / 100; loss 1.9298\n",
      "Evaluate (epoch 1) -- logp(x) = -1.691 +/- 0.006\n",
      "epoch   2 / 200, step    0 / 100; loss 1.7086\n",
      "Evaluate (epoch 2) -- logp(x) = -0.969 +/- 0.011\n",
      "epoch   3 / 200, step    0 / 100; loss 1.0466\n",
      "Evaluate (epoch 3) -- logp(x) = -0.764 +/- 0.015\n",
      "epoch   4 / 200, step    0 / 100; loss 0.8116\n",
      "Evaluate (epoch 4) -- logp(x) = -0.778 +/- 0.016\n",
      "epoch   5 / 200, step    0 / 100; loss 0.7684\n",
      "Evaluate (epoch 5) -- logp(x) = -0.852 +/- 0.016\n",
      "epoch   6 / 200, step    0 / 100; loss 0.8010\n",
      "Evaluate (epoch 6) -- logp(x) = -0.949 +/- 0.017\n",
      "epoch   7 / 200, step    0 / 100; loss 1.0144\n",
      "Evaluate (epoch 7) -- logp(x) = -0.652 +/- 0.017\n",
      "epoch   8 / 200, step    0 / 100; loss 0.6613\n",
      "Evaluate (epoch 8) -- logp(x) = -0.474 +/- 0.019\n",
      "epoch   9 / 200, step    0 / 100; loss 0.3808\n",
      "Evaluate (epoch 9) -- logp(x) = -0.533 +/- 0.018\n",
      "epoch  10 / 200, step    0 / 100; loss 0.5859\n",
      "Evaluate (epoch 10) -- logp(x) = -0.401 +/- 0.018\n",
      "epoch  11 / 200, step    0 / 100; loss 0.4117\n",
      "Evaluate (epoch 11) -- logp(x) = -0.411 +/- 0.020\n",
      "epoch  12 / 200, step    0 / 100; loss 0.5298\n",
      "Evaluate (epoch 12) -- logp(x) = -1.061 +/- 0.020\n",
      "epoch  13 / 200, step    0 / 100; loss 1.0498\n",
      "Evaluate (epoch 13) -- logp(x) = -0.692 +/- 0.017\n",
      "epoch  14 / 200, step    0 / 100; loss 0.6855\n",
      "Evaluate (epoch 14) -- logp(x) = -0.601 +/- 0.019\n",
      "epoch  15 / 200, step    0 / 100; loss 0.5516\n",
      "Evaluate (epoch 15) -- logp(x) = -0.529 +/- 0.019\n",
      "epoch  16 / 200, step    0 / 100; loss 0.3975\n",
      "Evaluate (epoch 16) -- logp(x) = -0.497 +/- 0.037\n",
      "epoch  17 / 200, step    0 / 100; loss 0.4510\n",
      "Evaluate (epoch 17) -- logp(x) = -0.420 +/- 0.021\n",
      "epoch  18 / 200, step    0 / 100; loss 0.4487\n",
      "Evaluate (epoch 18) -- logp(x) = -0.407 +/- 0.018\n",
      "epoch  19 / 200, step    0 / 100; loss 0.4216\n",
      "Evaluate (epoch 19) -- logp(x) = -0.356 +/- 0.018\n",
      "epoch  20 / 200, step    0 / 100; loss 0.3444\n",
      "Evaluate (epoch 20) -- logp(x) = -0.331 +/- 0.018\n",
      "epoch  21 / 200, step    0 / 100; loss 0.4560\n",
      "Evaluate (epoch 21) -- logp(x) = -0.263 +/- 0.019\n",
      "epoch  22 / 200, step    0 / 100; loss 0.2485\n",
      "Evaluate (epoch 22) -- logp(x) = -0.225 +/- 0.019\n",
      "epoch  23 / 200, step    0 / 100; loss 0.2220\n",
      "Evaluate (epoch 23) -- logp(x) = -0.282 +/- 0.019\n",
      "epoch  24 / 200, step    0 / 100; loss 0.2719\n",
      "Evaluate (epoch 24) -- logp(x) = -0.149 +/- 0.026\n",
      "epoch  25 / 200, step    0 / 100; loss 0.1419\n",
      "Evaluate (epoch 25) -- logp(x) = -0.338 +/- 0.021\n",
      "epoch  26 / 200, step    0 / 100; loss 0.2139\n",
      "Evaluate (epoch 26) -- logp(x) = -0.302 +/- 0.016\n",
      "epoch  27 / 200, step    0 / 100; loss 0.3223\n",
      "Evaluate (epoch 27) -- logp(x) = -2.308 +/- 0.018\n",
      "epoch  28 / 200, step    0 / 100; loss 2.3692\n",
      "Evaluate (epoch 28) -- logp(x) = -1.576 +/- 0.019\n",
      "epoch  29 / 200, step    0 / 100; loss 1.4218\n",
      "Evaluate (epoch 29) -- logp(x) = -0.980 +/- 0.019\n",
      "epoch  30 / 200, step    0 / 100; loss 0.9266\n",
      "Evaluate (epoch 30) -- logp(x) = -0.394 +/- 0.021\n",
      "epoch  31 / 200, step    0 / 100; loss 0.4361\n",
      "Evaluate (epoch 31) -- logp(x) = -0.156 +/- 0.018\n",
      "epoch  32 / 200, step    0 / 100; loss 0.2573\n",
      "Evaluate (epoch 32) -- logp(x) = -0.060 +/- 0.018\n",
      "epoch  33 / 200, step    0 / 100; loss 0.2125\n",
      "Evaluate (epoch 33) -- logp(x) = -0.016 +/- 0.022\n",
      "epoch  34 / 200, step    0 / 100; loss 0.0910\n",
      "Evaluate (epoch 34) -- logp(x) = -0.872 +/- 0.018\n",
      "epoch  35 / 200, step    0 / 100; loss 0.8044\n",
      "Evaluate (epoch 35) -- logp(x) = -0.208 +/- 0.017\n",
      "epoch  36 / 200, step    0 / 100; loss 0.2702\n",
      "Evaluate (epoch 36) -- logp(x) = -0.054 +/- 0.016\n",
      "epoch  37 / 200, step    0 / 100; loss 0.1233\n",
      "Evaluate (epoch 37) -- logp(x) = 0.006 +/- 0.017\n",
      "epoch  38 / 200, step    0 / 100; loss 0.0077\n",
      "Evaluate (epoch 38) -- logp(x) = 0.047 +/- 0.016\n",
      "epoch  39 / 200, step    0 / 100; loss -0.1162\n",
      "Evaluate (epoch 39) -- logp(x) = -0.041 +/- 0.027\n",
      "epoch  40 / 200, step    0 / 100; loss 0.0999\n",
      "Evaluate (epoch 40) -- logp(x) = 0.093 +/- 0.016\n",
      "epoch  41 / 200, step    0 / 100; loss -0.0840\n",
      "Evaluate (epoch 41) -- logp(x) = 0.074 +/- 0.017\n",
      "epoch  42 / 200, step    0 / 100; loss -0.0646\n",
      "Evaluate (epoch 42) -- logp(x) = 0.139 +/- 0.017\n",
      "epoch  43 / 200, step    0 / 100; loss -0.2058\n",
      "Evaluate (epoch 43) -- logp(x) = 0.134 +/- 0.018\n",
      "epoch  44 / 200, step    0 / 100; loss -0.2360\n",
      "Evaluate (epoch 44) -- logp(x) = 0.174 +/- 0.017\n",
      "epoch  45 / 200, step    0 / 100; loss -0.1628\n",
      "Evaluate (epoch 45) -- logp(x) = 0.195 +/- 0.017\n",
      "epoch  46 / 200, step    0 / 100; loss -0.2678\n",
      "Evaluate (epoch 46) -- logp(x) = 0.204 +/- 0.019\n",
      "epoch  47 / 200, step    0 / 100; loss -0.1690\n",
      "Evaluate (epoch 47) -- logp(x) = 0.219 +/- 0.019\n",
      "epoch  48 / 200, step    0 / 100; loss -0.1748\n",
      "Evaluate (epoch 48) -- logp(x) = 0.233 +/- 0.018\n",
      "epoch  49 / 200, step    0 / 100; loss -0.1708\n",
      "Evaluate (epoch 49) -- logp(x) = 0.243 +/- 0.018\n",
      "epoch  50 / 200, step    0 / 100; loss -0.3546\n",
      "Evaluate (epoch 50) -- logp(x) = 0.238 +/- 0.018\n",
      "epoch  51 / 200, step    0 / 100; loss -0.2367\n",
      "Evaluate (epoch 51) -- logp(x) = 0.272 +/- 0.020\n",
      "epoch  52 / 200, step    0 / 100; loss -0.2353\n",
      "Evaluate (epoch 52) -- logp(x) = 0.274 +/- 0.023\n",
      "epoch  53 / 200, step    0 / 100; loss -0.2071\n",
      "Evaluate (epoch 53) -- logp(x) = 0.271 +/- 0.019\n",
      "epoch  54 / 200, step    0 / 100; loss -0.2230\n",
      "Evaluate (epoch 54) -- logp(x) = 0.297 +/- 0.018\n",
      "epoch  55 / 200, step    0 / 100; loss -0.3507\n",
      "Evaluate (epoch 55) -- logp(x) = 0.308 +/- 0.019\n",
      "epoch  56 / 200, step    0 / 100; loss -0.3079\n",
      "Evaluate (epoch 56) -- logp(x) = 0.301 +/- 0.019\n",
      "epoch  57 / 200, step    0 / 100; loss -0.3343\n",
      "Evaluate (epoch 57) -- logp(x) = 0.323 +/- 0.022\n",
      "epoch  58 / 200, step    0 / 100; loss -0.3749\n",
      "Evaluate (epoch 58) -- logp(x) = 0.342 +/- 0.019\n",
      "epoch  59 / 200, step    0 / 100; loss -0.2130\n",
      "Evaluate (epoch 59) -- logp(x) = 0.348 +/- 0.020\n",
      "epoch  60 / 200, step    0 / 100; loss -0.3033\n",
      "Evaluate (epoch 60) -- logp(x) = 0.336 +/- 0.020\n",
      "epoch  61 / 200, step    0 / 100; loss -0.2997\n",
      "Evaluate (epoch 61) -- logp(x) = 0.368 +/- 0.019\n",
      "epoch  62 / 200, step    0 / 100; loss -0.4189\n",
      "Evaluate (epoch 62) -- logp(x) = 0.357 +/- 0.019\n",
      "epoch  63 / 200, step    0 / 100; loss -0.3067\n",
      "Evaluate (epoch 63) -- logp(x) = 0.328 +/- 0.029\n",
      "epoch  64 / 200, step    0 / 100; loss -0.2788\n",
      "Evaluate (epoch 64) -- logp(x) = 0.365 +/- 0.019\n",
      "epoch  65 / 200, step    0 / 100; loss -0.2993\n",
      "Evaluate (epoch 65) -- logp(x) = 0.384 +/- 0.019\n",
      "epoch  66 / 200, step    0 / 100; loss -0.5634\n",
      "Evaluate (epoch 66) -- logp(x) = 0.393 +/- 0.019\n",
      "epoch  67 / 200, step    0 / 100; loss -0.3180\n",
      "Evaluate (epoch 67) -- logp(x) = 0.392 +/- 0.020\n",
      "epoch  68 / 200, step    0 / 100; loss -0.4116\n",
      "Evaluate (epoch 68) -- logp(x) = 0.409 +/- 0.021\n",
      "epoch  69 / 200, step    0 / 100; loss -0.3581\n",
      "Evaluate (epoch 69) -- logp(x) = 0.331 +/- 0.019\n",
      "epoch  70 / 200, step    0 / 100; loss -0.2993\n",
      "Evaluate (epoch 70) -- logp(x) = 0.389 +/- 0.020\n",
      "epoch  71 / 200, step    0 / 100; loss -0.2734\n",
      "Evaluate (epoch 71) -- logp(x) = 0.426 +/- 0.021\n",
      "epoch  72 / 200, step    0 / 100; loss -0.5775\n",
      "Evaluate (epoch 72) -- logp(x) = 0.425 +/- 0.021\n",
      "epoch  73 / 200, step    0 / 100; loss -0.5022\n",
      "Evaluate (epoch 73) -- logp(x) = 0.442 +/- 0.019\n",
      "epoch  74 / 200, step    0 / 100; loss -0.4236\n",
      "Evaluate (epoch 74) -- logp(x) = 0.408 +/- 0.018\n",
      "epoch  75 / 200, step    0 / 100; loss -0.4911\n",
      "Evaluate (epoch 75) -- logp(x) = 0.441 +/- 0.019\n",
      "epoch  76 / 200, step    0 / 100; loss -0.5066\n",
      "Evaluate (epoch 76) -- logp(x) = 0.427 +/- 0.023\n",
      "epoch  77 / 200, step    0 / 100; loss -0.4606\n",
      "Evaluate (epoch 77) -- logp(x) = 0.457 +/- 0.019\n",
      "epoch  78 / 200, step    0 / 100; loss -0.4304\n",
      "Evaluate (epoch 78) -- logp(x) = 0.440 +/- 0.020\n",
      "epoch  79 / 200, step    0 / 100; loss -0.5169\n",
      "Evaluate (epoch 79) -- logp(x) = 0.374 +/- 0.024\n",
      "epoch  80 / 200, step    0 / 100; loss -0.3147\n",
      "Evaluate (epoch 80) -- logp(x) = 0.481 +/- 0.022\n",
      "epoch  81 / 200, step    0 / 100; loss -0.5783\n",
      "Evaluate (epoch 81) -- logp(x) = 0.478 +/- 0.020\n",
      "epoch  82 / 200, step    0 / 100; loss -0.4443\n",
      "Evaluate (epoch 82) -- logp(x) = 0.484 +/- 0.020\n",
      "epoch  83 / 200, step    0 / 100; loss -0.4689\n",
      "Evaluate (epoch 83) -- logp(x) = 0.471 +/- 0.025\n",
      "epoch  84 / 200, step    0 / 100; loss -0.3712\n",
      "Evaluate (epoch 84) -- logp(x) = 0.466 +/- 0.026\n",
      "epoch  85 / 200, step    0 / 100; loss -0.5170\n",
      "Evaluate (epoch 85) -- logp(x) = 0.468 +/- 0.034\n",
      "epoch  86 / 200, step    0 / 100; loss -0.6409\n",
      "Evaluate (epoch 86) -- logp(x) = 0.497 +/- 0.024\n",
      "epoch  87 / 200, step    0 / 100; loss -0.3845\n",
      "Evaluate (epoch 87) -- logp(x) = 0.507 +/- 0.022\n",
      "epoch  88 / 200, step    0 / 100; loss -0.4674\n",
      "Evaluate (epoch 88) -- logp(x) = 0.504 +/- 0.020\n",
      "epoch  89 / 200, step    0 / 100; loss -0.5283\n",
      "Evaluate (epoch 89) -- logp(x) = 0.498 +/- 0.020\n",
      "epoch  90 / 200, step    0 / 100; loss -0.3813\n",
      "Evaluate (epoch 90) -- logp(x) = 0.475 +/- 0.019\n",
      "epoch  91 / 200, step    0 / 100; loss -0.5132\n",
      "Evaluate (epoch 91) -- logp(x) = 0.493 +/- 0.020\n",
      "epoch  92 / 200, step    0 / 100; loss -0.5768\n",
      "Evaluate (epoch 92) -- logp(x) = 0.518 +/- 0.021\n",
      "epoch  93 / 200, step    0 / 100; loss -0.4901\n",
      "Evaluate (epoch 93) -- logp(x) = 0.521 +/- 0.020\n",
      "epoch  94 / 200, step    0 / 100; loss -0.4557\n",
      "Evaluate (epoch 94) -- logp(x) = 0.521 +/- 0.022\n",
      "epoch  95 / 200, step    0 / 100; loss -0.6208\n",
      "Evaluate (epoch 95) -- logp(x) = 0.528 +/- 0.021\n",
      "epoch  96 / 200, step    0 / 100; loss -0.4921\n",
      "Evaluate (epoch 96) -- logp(x) = 0.525 +/- 0.020\n",
      "epoch  97 / 200, step    0 / 100; loss -0.6223\n",
      "Evaluate (epoch 97) -- logp(x) = 0.536 +/- 0.021\n",
      "epoch  98 / 200, step    0 / 100; loss -0.4757\n",
      "Evaluate (epoch 98) -- logp(x) = 0.540 +/- 0.026\n",
      "epoch  99 / 200, step    0 / 100; loss -0.4713\n",
      "Evaluate (epoch 99) -- logp(x) = 0.551 +/- 0.021\n",
      "epoch 100 / 200, step    0 / 100; loss -0.4788\n",
      "Evaluate (epoch 100) -- logp(x) = 0.555 +/- 0.020\n",
      "epoch 101 / 200, step    0 / 100; loss -0.6439\n",
      "Evaluate (epoch 101) -- logp(x) = 0.528 +/- 0.023\n",
      "epoch 102 / 200, step    0 / 100; loss -0.2337\n",
      "Evaluate (epoch 102) -- logp(x) = 0.558 +/- 0.028\n",
      "epoch 103 / 200, step    0 / 100; loss -0.4185\n",
      "Evaluate (epoch 103) -- logp(x) = 0.568 +/- 0.020\n",
      "epoch 104 / 200, step    0 / 100; loss -0.5646\n",
      "Evaluate (epoch 104) -- logp(x) = 0.498 +/- 0.020\n",
      "epoch 105 / 200, step    0 / 100; loss -0.5594\n",
      "Evaluate (epoch 105) -- logp(x) = 0.575 +/- 0.020\n",
      "epoch 106 / 200, step    0 / 100; loss -0.3973\n",
      "Evaluate (epoch 106) -- logp(x) = 0.424 +/- 0.021\n",
      "epoch 107 / 200, step    0 / 100; loss -0.4000\n",
      "Evaluate (epoch 107) -- logp(x) = 0.544 +/- 0.020\n",
      "epoch 108 / 200, step    0 / 100; loss -0.6194\n",
      "Evaluate (epoch 108) -- logp(x) = 0.535 +/- 0.021\n",
      "epoch 109 / 200, step    0 / 100; loss -0.6105\n",
      "Evaluate (epoch 109) -- logp(x) = 0.496 +/- 0.020\n",
      "epoch 110 / 200, step    0 / 100; loss -0.5326\n",
      "Evaluate (epoch 110) -- logp(x) = 0.575 +/- 0.022\n",
      "epoch 111 / 200, step    0 / 100; loss -0.5689\n",
      "Evaluate (epoch 111) -- logp(x) = 0.559 +/- 0.021\n",
      "epoch 112 / 200, step    0 / 100; loss -0.5521\n",
      "Evaluate (epoch 112) -- logp(x) = 0.598 +/- 0.022\n",
      "epoch 113 / 200, step    0 / 100; loss -0.4840\n",
      "Evaluate (epoch 113) -- logp(x) = 0.600 +/- 0.021\n",
      "epoch 114 / 200, step    0 / 100; loss -0.5416\n",
      "Evaluate (epoch 114) -- logp(x) = 0.586 +/- 0.034\n",
      "epoch 115 / 200, step    0 / 100; loss -0.6875\n",
      "Evaluate (epoch 115) -- logp(x) = 0.602 +/- 0.024\n",
      "epoch 116 / 200, step    0 / 100; loss -0.6899\n",
      "Evaluate (epoch 116) -- logp(x) = 0.569 +/- 0.021\n",
      "epoch 117 / 200, step    0 / 100; loss -0.3637\n",
      "Evaluate (epoch 117) -- logp(x) = 0.591 +/- 0.025\n",
      "epoch 118 / 200, step    0 / 100; loss -0.6279\n",
      "Evaluate (epoch 118) -- logp(x) = 0.594 +/- 0.022\n",
      "epoch 119 / 200, step    0 / 100; loss -0.5804\n",
      "Evaluate (epoch 119) -- logp(x) = 0.590 +/- 0.021\n",
      "epoch 120 / 200, step    0 / 100; loss -0.3502\n",
      "Evaluate (epoch 120) -- logp(x) = 0.580 +/- 0.022\n",
      "epoch 121 / 200, step    0 / 100; loss -0.5812\n",
      "Evaluate (epoch 121) -- logp(x) = 0.607 +/- 0.025\n",
      "epoch 122 / 200, step    0 / 100; loss -0.6033\n",
      "Evaluate (epoch 122) -- logp(x) = 0.598 +/- 0.022\n",
      "epoch 123 / 200, step    0 / 100; loss -0.5253\n",
      "Evaluate (epoch 123) -- logp(x) = 0.626 +/- 0.021\n",
      "epoch 124 / 200, step    0 / 100; loss -0.5954\n",
      "Evaluate (epoch 124) -- logp(x) = 0.608 +/- 0.024\n",
      "epoch 125 / 200, step    0 / 100; loss -0.7043\n",
      "Evaluate (epoch 125) -- logp(x) = 0.601 +/- 0.021\n",
      "epoch 126 / 200, step    0 / 100; loss -0.6871\n",
      "Evaluate (epoch 126) -- logp(x) = 0.626 +/- 0.021\n",
      "epoch 127 / 200, step    0 / 100; loss -0.4860\n",
      "Evaluate (epoch 127) -- logp(x) = 0.628 +/- 0.021\n",
      "epoch 128 / 200, step    0 / 100; loss -0.3860\n",
      "Evaluate (epoch 128) -- logp(x) = 0.496 +/- 0.031\n",
      "epoch 129 / 200, step    0 / 100; loss -0.5076\n",
      "Evaluate (epoch 129) -- logp(x) = 0.606 +/- 0.026\n",
      "epoch 130 / 200, step    0 / 100; loss -0.6881\n",
      "Evaluate (epoch 130) -- logp(x) = 0.642 +/- 0.021\n",
      "epoch 131 / 200, step    0 / 100; loss -0.4856\n",
      "Evaluate (epoch 131) -- logp(x) = 0.649 +/- 0.024\n",
      "epoch 132 / 200, step    0 / 100; loss -0.6218\n",
      "Evaluate (epoch 132) -- logp(x) = 0.635 +/- 0.021\n",
      "epoch 133 / 200, step    0 / 100; loss -0.7122\n",
      "Evaluate (epoch 133) -- logp(x) = 0.615 +/- 0.023\n",
      "epoch 134 / 200, step    0 / 100; loss -0.7411\n",
      "Evaluate (epoch 134) -- logp(x) = -0.005 +/- 0.034\n",
      "epoch 135 / 200, step    0 / 100; loss -0.0619\n",
      "Evaluate (epoch 135) -- logp(x) = 0.135 +/- 0.022\n",
      "epoch 136 / 200, step    0 / 100; loss -0.1986\n",
      "Evaluate (epoch 136) -- logp(x) = 0.432 +/- 0.019\n",
      "epoch 137 / 200, step    0 / 100; loss -0.4303\n",
      "Evaluate (epoch 137) -- logp(x) = 0.597 +/- 0.023\n",
      "epoch 138 / 200, step    0 / 100; loss -0.6701\n",
      "Evaluate (epoch 138) -- logp(x) = 0.584 +/- 0.023\n",
      "epoch 139 / 200, step    0 / 100; loss -0.6666\n",
      "Evaluate (epoch 139) -- logp(x) = 0.543 +/- 0.023\n",
      "epoch 140 / 200, step    0 / 100; loss -0.3810\n",
      "Evaluate (epoch 140) -- logp(x) = 0.627 +/- 0.022\n",
      "epoch 141 / 200, step    0 / 100; loss -0.7470\n",
      "Evaluate (epoch 141) -- logp(x) = 0.641 +/- 0.024\n",
      "epoch 142 / 200, step    0 / 100; loss -0.4783\n",
      "Evaluate (epoch 142) -- logp(x) = 0.583 +/- 0.023\n",
      "epoch 143 / 200, step    0 / 100; loss -0.4576\n",
      "Evaluate (epoch 143) -- logp(x) = 0.631 +/- 0.022\n",
      "epoch 144 / 200, step    0 / 100; loss -0.6642\n",
      "Evaluate (epoch 144) -- logp(x) = 0.643 +/- 0.025\n",
      "epoch 145 / 200, step    0 / 100; loss -0.7359\n",
      "Evaluate (epoch 145) -- logp(x) = 0.666 +/- 0.023\n",
      "epoch 146 / 200, step    0 / 100; loss -0.5686\n",
      "Evaluate (epoch 146) -- logp(x) = 0.557 +/- 0.081\n",
      "epoch 147 / 200, step    0 / 100; loss -0.7314\n",
      "Evaluate (epoch 147) -- logp(x) = 0.657 +/- 0.022\n",
      "epoch 148 / 200, step    0 / 100; loss -0.5217\n",
      "Evaluate (epoch 148) -- logp(x) = 0.645 +/- 0.023\n",
      "epoch 149 / 200, step    0 / 100; loss -0.5420\n",
      "Evaluate (epoch 149) -- logp(x) = 0.686 +/- 0.022\n",
      "epoch 150 / 200, step    0 / 100; loss -0.5744\n",
      "Evaluate (epoch 150) -- logp(x) = 0.675 +/- 0.023\n",
      "epoch 151 / 200, step    0 / 100; loss -0.7683\n",
      "Evaluate (epoch 151) -- logp(x) = 0.521 +/- 0.023\n",
      "epoch 152 / 200, step    0 / 100; loss -0.4727\n",
      "Evaluate (epoch 152) -- logp(x) = 0.508 +/- 0.030\n",
      "epoch 153 / 200, step    0 / 100; loss -0.3278\n",
      "Evaluate (epoch 153) -- logp(x) = 0.673 +/- 0.022\n",
      "epoch 154 / 200, step    0 / 100; loss -0.6276\n",
      "Evaluate (epoch 154) -- logp(x) = 0.686 +/- 0.023\n",
      "epoch 155 / 200, step    0 / 100; loss -0.7855\n",
      "Evaluate (epoch 155) -- logp(x) = 0.681 +/- 0.026\n",
      "epoch 156 / 200, step    0 / 100; loss -0.6849\n",
      "Evaluate (epoch 156) -- logp(x) = 0.687 +/- 0.025\n",
      "epoch 157 / 200, step    0 / 100; loss -0.6203\n",
      "Evaluate (epoch 157) -- logp(x) = 0.673 +/- 0.033\n",
      "epoch 158 / 200, step    0 / 100; loss -0.6085\n",
      "Evaluate (epoch 158) -- logp(x) = 0.656 +/- 0.025\n",
      "epoch 159 / 200, step    0 / 100; loss -0.7000\n",
      "Evaluate (epoch 159) -- logp(x) = 0.638 +/- 0.023\n",
      "epoch 160 / 200, step    0 / 100; loss -0.6568\n",
      "Evaluate (epoch 160) -- logp(x) = 0.657 +/- 0.027\n",
      "epoch 161 / 200, step    0 / 100; loss -0.6598\n",
      "Evaluate (epoch 161) -- logp(x) = 0.679 +/- 0.021\n",
      "epoch 162 / 200, step    0 / 100; loss -0.6437\n",
      "Evaluate (epoch 162) -- logp(x) = 0.652 +/- 0.021\n",
      "epoch 163 / 200, step    0 / 100; loss -0.7024\n",
      "Evaluate (epoch 163) -- logp(x) = 0.698 +/- 0.022\n",
      "epoch 164 / 200, step    0 / 100; loss -0.7717\n",
      "Evaluate (epoch 164) -- logp(x) = 0.730 +/- 0.022\n",
      "epoch 165 / 200, step    0 / 100; loss -0.6758\n",
      "Evaluate (epoch 165) -- logp(x) = 0.668 +/- 0.026\n",
      "epoch 166 / 200, step    0 / 100; loss -0.6975\n",
      "Evaluate (epoch 166) -- logp(x) = 0.501 +/- 0.022\n",
      "epoch 167 / 200, step    0 / 100; loss -0.6292\n",
      "Evaluate (epoch 167) -- logp(x) = 0.642 +/- 0.025\n",
      "epoch 168 / 200, step    0 / 100; loss -0.4016\n",
      "Evaluate (epoch 168) -- logp(x) = 0.677 +/- 0.024\n",
      "epoch 169 / 200, step    0 / 100; loss -0.7472\n",
      "Evaluate (epoch 169) -- logp(x) = 0.631 +/- 0.023\n",
      "epoch 170 / 200, step    0 / 100; loss -0.7161\n",
      "Evaluate (epoch 170) -- logp(x) = 0.686 +/- 0.024\n",
      "epoch 171 / 200, step    0 / 100; loss -0.7902\n",
      "Evaluate (epoch 171) -- logp(x) = 0.703 +/- 0.027\n",
      "epoch 172 / 200, step    0 / 100; loss -0.6930\n",
      "Evaluate (epoch 172) -- logp(x) = 0.697 +/- 0.023\n",
      "epoch 173 / 200, step    0 / 100; loss -0.7608\n",
      "Evaluate (epoch 173) -- logp(x) = 0.718 +/- 0.024\n",
      "epoch 174 / 200, step    0 / 100; loss -0.7718\n",
      "Evaluate (epoch 174) -- logp(x) = 0.735 +/- 0.022\n",
      "epoch 175 / 200, step    0 / 100; loss -0.4051\n",
      "Evaluate (epoch 175) -- logp(x) = 0.754 +/- 0.022\n",
      "epoch 176 / 200, step    0 / 100; loss -0.8019\n",
      "Evaluate (epoch 176) -- logp(x) = 0.736 +/- 0.026\n",
      "epoch 177 / 200, step    0 / 100; loss -0.7844\n",
      "Evaluate (epoch 177) -- logp(x) = 0.714 +/- 0.026\n",
      "epoch 178 / 200, step    0 / 100; loss -0.4466\n",
      "Evaluate (epoch 178) -- logp(x) = 0.716 +/- 0.024\n",
      "epoch 179 / 200, step    0 / 100; loss -0.6678\n",
      "Evaluate (epoch 179) -- logp(x) = 0.674 +/- 0.024\n",
      "epoch 180 / 200, step    0 / 100; loss -0.7857\n",
      "Evaluate (epoch 180) -- logp(x) = 0.707 +/- 0.026\n",
      "epoch 181 / 200, step    0 / 100; loss -0.6628\n",
      "Evaluate (epoch 181) -- logp(x) = 0.698 +/- 0.028\n",
      "epoch 182 / 200, step    0 / 100; loss -0.6599\n",
      "Evaluate (epoch 182) -- logp(x) = 0.748 +/- 0.027\n",
      "epoch 183 / 200, step    0 / 100; loss -0.8771\n",
      "Evaluate (epoch 183) -- logp(x) = 0.298 +/- 0.029\n",
      "epoch 184 / 200, step    0 / 100; loss -0.1871\n",
      "Evaluate (epoch 184) -- logp(x) = 0.556 +/- 0.024\n",
      "epoch 185 / 200, step    0 / 100; loss -0.4343\n",
      "Evaluate (epoch 185) -- logp(x) = 0.697 +/- 0.028\n",
      "epoch 186 / 200, step    0 / 100; loss -0.4449\n",
      "Evaluate (epoch 186) -- logp(x) = 0.693 +/- 0.026\n",
      "epoch 187 / 200, step    0 / 100; loss -0.6347\n",
      "Evaluate (epoch 187) -- logp(x) = 0.654 +/- 0.023\n",
      "epoch 188 / 200, step    0 / 100; loss -0.5600\n",
      "Evaluate (epoch 188) -- logp(x) = 0.703 +/- 0.024\n",
      "epoch 189 / 200, step    0 / 100; loss -0.6469\n",
      "Evaluate (epoch 189) -- logp(x) = 0.745 +/- 0.029\n",
      "epoch 190 / 200, step    0 / 100; loss -0.8468\n",
      "Evaluate (epoch 190) -- logp(x) = 0.707 +/- 0.026\n",
      "epoch 191 / 200, step    0 / 100; loss -0.3787\n",
      "Evaluate (epoch 191) -- logp(x) = 0.765 +/- 0.025\n",
      "epoch 192 / 200, step    0 / 100; loss -0.7325\n",
      "Evaluate (epoch 192) -- logp(x) = 0.722 +/- 0.023\n",
      "epoch 193 / 200, step    0 / 100; loss -0.8557\n",
      "Evaluate (epoch 193) -- logp(x) = 0.646 +/- 0.024\n",
      "epoch 194 / 200, step    0 / 100; loss -0.7521\n",
      "Evaluate (epoch 194) -- logp(x) = 0.765 +/- 0.024\n",
      "epoch 195 / 200, step    0 / 100; loss -0.8612\n",
      "Evaluate (epoch 195) -- logp(x) = 0.752 +/- 0.026\n",
      "epoch 196 / 200, step    0 / 100; loss -0.8571\n",
      "Evaluate (epoch 196) -- logp(x) = 0.764 +/- 0.026\n",
      "epoch 197 / 200, step    0 / 100; loss -0.8700\n",
      "Evaluate (epoch 197) -- logp(x) = 0.703 +/- 0.030\n",
      "epoch 198 / 200, step    0 / 100; loss -0.2995\n",
      "Evaluate (epoch 198) -- logp(x) = 0.792 +/- 0.024\n",
      "epoch 199 / 200, step    0 / 100; loss -0.5914\n",
      "Evaluate (epoch 199) -- logp(x) = 0.781 +/- 0.024\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'CIRCLE',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': True,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 2,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 2,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'maf',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 50,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': True,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/maf',\n",
      " 'restore_file': './results/maf/best_model_checkpoint.pt',\n",
      " 'results_file': './results/maf\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 199,\n",
      " 'train': False}\n",
      "MAF(\n",
      "  (net): FlowSequential(\n",
      "    (0): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'TORUS',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': False,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 3,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 3,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'maf',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 200,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': True,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/maf',\n",
      " 'restore_file': None,\n",
      " 'results_file': './results/maf\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 0,\n",
      " 'train': True}\n",
      "MAF(\n",
      "  (net): FlowSequential(\n",
      "    (0): MADE(\n",
      "      (net_input): MaskedLinear(in_features=3, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=6, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): MADE(\n",
      "      (net_input): MaskedLinear(in_features=3, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=6, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): MADE(\n",
      "      (net_input): MaskedLinear(in_features=3, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=6, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): MADE(\n",
      "      (net_input): MaskedLinear(in_features=3, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=6, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): MADE(\n",
      "      (net_input): MaskedLinear(in_features=3, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=6, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch   0 / 200, step    0 / 100; loss 3.3418\n",
      "Evaluate (epoch 0) -- logp(x) = -2.626 +/- 0.013\n",
      "epoch   1 / 200, step    0 / 100; loss 2.6448\n",
      "Evaluate (epoch 1) -- logp(x) = -2.603 +/- 0.012\n",
      "epoch   2 / 200, step    0 / 100; loss 2.6207\n",
      "Evaluate (epoch 2) -- logp(x) = -2.569 +/- 0.012\n",
      "epoch   3 / 200, step    0 / 100; loss 2.5034\n",
      "Evaluate (epoch 3) -- logp(x) = -2.503 +/- 0.012\n",
      "epoch   4 / 200, step    0 / 100; loss 2.5096\n",
      "Evaluate (epoch 4) -- logp(x) = -2.435 +/- 0.011\n",
      "epoch   5 / 200, step    0 / 100; loss 2.4401\n",
      "Evaluate (epoch 5) -- logp(x) = -2.355 +/- 0.011\n",
      "epoch   6 / 200, step    0 / 100; loss 2.3626\n",
      "Evaluate (epoch 6) -- logp(x) = -2.245 +/- 0.011\n",
      "epoch   7 / 200, step    0 / 100; loss 2.1895\n",
      "Evaluate (epoch 7) -- logp(x) = -2.121 +/- 0.012\n",
      "epoch   8 / 200, step    0 / 100; loss 2.1409\n",
      "Evaluate (epoch 8) -- logp(x) = -2.038 +/- 0.013\n",
      "epoch   9 / 200, step    0 / 100; loss 1.9042\n",
      "Evaluate (epoch 9) -- logp(x) = -1.993 +/- 0.013\n",
      "epoch  10 / 200, step    0 / 100; loss 1.9504\n",
      "Evaluate (epoch 10) -- logp(x) = -1.950 +/- 0.013\n",
      "epoch  11 / 200, step    0 / 100; loss 1.9713\n",
      "Evaluate (epoch 11) -- logp(x) = -1.904 +/- 0.014\n",
      "epoch  12 / 200, step    0 / 100; loss 1.9830\n",
      "Evaluate (epoch 12) -- logp(x) = -1.889 +/- 0.015\n",
      "epoch  13 / 200, step    0 / 100; loss 1.9186\n",
      "Evaluate (epoch 13) -- logp(x) = -2.012 +/- 0.014\n",
      "epoch  14 / 200, step    0 / 100; loss 2.0797\n",
      "Evaluate (epoch 14) -- logp(x) = -1.835 +/- 0.015\n",
      "epoch  15 / 200, step    0 / 100; loss 1.8326\n",
      "Evaluate (epoch 15) -- logp(x) = -1.850 +/- 0.015\n",
      "epoch  16 / 200, step    0 / 100; loss 1.7799\n",
      "Evaluate (epoch 16) -- logp(x) = -1.872 +/- 0.014\n",
      "epoch  17 / 200, step    0 / 100; loss 1.9366\n",
      "Evaluate (epoch 17) -- logp(x) = -1.792 +/- 0.017\n",
      "epoch  18 / 200, step    0 / 100; loss 1.6729\n",
      "Evaluate (epoch 18) -- logp(x) = -1.814 +/- 0.015\n",
      "epoch  19 / 200, step    0 / 100; loss 2.0320\n",
      "Evaluate (epoch 19) -- logp(x) = -1.786 +/- 0.016\n",
      "epoch  20 / 200, step    0 / 100; loss 1.8255\n",
      "Evaluate (epoch 20) -- logp(x) = -1.760 +/- 0.015\n",
      "epoch  21 / 200, step    0 / 100; loss 1.8539\n",
      "Evaluate (epoch 21) -- logp(x) = -1.766 +/- 0.016\n",
      "epoch  22 / 200, step    0 / 100; loss 1.7779\n",
      "Evaluate (epoch 22) -- logp(x) = -1.994 +/- 0.014\n",
      "epoch  23 / 200, step    0 / 100; loss 2.1132\n",
      "Evaluate (epoch 23) -- logp(x) = -1.733 +/- 0.016\n",
      "epoch  24 / 200, step    0 / 100; loss 1.7269\n",
      "Evaluate (epoch 24) -- logp(x) = -1.741 +/- 0.017\n",
      "epoch  25 / 200, step    0 / 100; loss 1.7260\n",
      "Evaluate (epoch 25) -- logp(x) = -1.706 +/- 0.016\n",
      "epoch  26 / 200, step    0 / 100; loss 1.7539\n",
      "Evaluate (epoch 26) -- logp(x) = -1.731 +/- 0.016\n",
      "epoch  27 / 200, step    0 / 100; loss 1.7112\n",
      "Evaluate (epoch 27) -- logp(x) = -1.685 +/- 0.016\n",
      "epoch  28 / 200, step    0 / 100; loss 1.7152\n",
      "Evaluate (epoch 28) -- logp(x) = -1.704 +/- 0.016\n",
      "epoch  29 / 200, step    0 / 100; loss 1.7835\n",
      "Evaluate (epoch 29) -- logp(x) = -1.672 +/- 0.017\n",
      "epoch  30 / 200, step    0 / 100; loss 1.5697\n",
      "Evaluate (epoch 30) -- logp(x) = -1.651 +/- 0.017\n",
      "epoch  31 / 200, step    0 / 100; loss 1.5738\n",
      "Evaluate (epoch 31) -- logp(x) = -1.657 +/- 0.016\n",
      "epoch  32 / 200, step    0 / 100; loss 1.7219\n",
      "Evaluate (epoch 32) -- logp(x) = -1.645 +/- 0.017\n",
      "epoch  33 / 200, step    0 / 100; loss 1.6890\n",
      "Evaluate (epoch 33) -- logp(x) = -1.632 +/- 0.017\n",
      "epoch  34 / 200, step    0 / 100; loss 1.8176\n",
      "Evaluate (epoch 34) -- logp(x) = -1.643 +/- 0.016\n",
      "epoch  35 / 200, step    0 / 100; loss 1.5983\n",
      "Evaluate (epoch 35) -- logp(x) = -1.651 +/- 0.017\n",
      "epoch  36 / 200, step    0 / 100; loss 1.7226\n",
      "Evaluate (epoch 36) -- logp(x) = -1.609 +/- 0.016\n",
      "epoch  37 / 200, step    0 / 100; loss 1.6461\n",
      "Evaluate (epoch 37) -- logp(x) = -1.605 +/- 0.017\n",
      "epoch  38 / 200, step    0 / 100; loss 1.5444\n",
      "Evaluate (epoch 38) -- logp(x) = -1.580 +/- 0.016\n",
      "epoch  39 / 200, step    0 / 100; loss 1.5857\n",
      "Evaluate (epoch 39) -- logp(x) = -1.614 +/- 0.016\n",
      "epoch  40 / 200, step    0 / 100; loss 1.5489\n",
      "Evaluate (epoch 40) -- logp(x) = -1.653 +/- 0.016\n",
      "epoch  41 / 200, step    0 / 100; loss 1.6780\n",
      "Evaluate (epoch 41) -- logp(x) = -1.596 +/- 0.020\n",
      "epoch  42 / 200, step    0 / 100; loss 1.5110\n",
      "Evaluate (epoch 42) -- logp(x) = -1.560 +/- 0.024\n",
      "epoch  43 / 200, step    0 / 100; loss 1.4709\n",
      "Evaluate (epoch 43) -- logp(x) = -1.577 +/- 0.017\n",
      "epoch  44 / 200, step    0 / 100; loss 1.4503\n",
      "Evaluate (epoch 44) -- logp(x) = -1.800 +/- 0.014\n",
      "epoch  45 / 200, step    0 / 100; loss 1.9084\n",
      "Evaluate (epoch 45) -- logp(x) = -1.564 +/- 0.017\n",
      "epoch  46 / 200, step    0 / 100; loss 1.4675\n",
      "Evaluate (epoch 46) -- logp(x) = -1.573 +/- 0.017\n",
      "epoch  47 / 200, step    0 / 100; loss 1.5100\n",
      "Evaluate (epoch 47) -- logp(x) = -1.527 +/- 0.016\n",
      "epoch  48 / 200, step    0 / 100; loss 1.5076\n",
      "Evaluate (epoch 48) -- logp(x) = -1.499 +/- 0.017\n",
      "epoch  49 / 200, step    0 / 100; loss 1.4687\n",
      "Evaluate (epoch 49) -- logp(x) = -1.549 +/- 0.021\n",
      "epoch  50 / 200, step    0 / 100; loss 1.5919\n",
      "Evaluate (epoch 50) -- logp(x) = -1.649 +/- 0.016\n",
      "epoch  51 / 200, step    0 / 100; loss 1.7188\n",
      "Evaluate (epoch 51) -- logp(x) = -1.562 +/- 0.016\n",
      "epoch  52 / 200, step    0 / 100; loss 1.7417\n",
      "Evaluate (epoch 52) -- logp(x) = -1.512 +/- 0.022\n",
      "epoch  53 / 200, step    0 / 100; loss 1.4473\n",
      "Evaluate (epoch 53) -- logp(x) = -1.530 +/- 0.016\n",
      "epoch  54 / 200, step    0 / 100; loss 1.6032\n",
      "Evaluate (epoch 54) -- logp(x) = -1.531 +/- 0.016\n",
      "epoch  55 / 200, step    0 / 100; loss 1.3464\n",
      "Evaluate (epoch 55) -- logp(x) = -1.463 +/- 0.017\n",
      "epoch  56 / 200, step    0 / 100; loss 1.4759\n",
      "Evaluate (epoch 56) -- logp(x) = -1.494 +/- 0.017\n",
      "epoch  57 / 200, step    0 / 100; loss 1.6806\n",
      "Evaluate (epoch 57) -- logp(x) = -1.471 +/- 0.017\n",
      "epoch  58 / 200, step    0 / 100; loss 1.6155\n",
      "Evaluate (epoch 58) -- logp(x) = -1.464 +/- 0.017\n",
      "epoch  59 / 200, step    0 / 100; loss 1.4483\n",
      "Evaluate (epoch 59) -- logp(x) = -1.455 +/- 0.017\n",
      "epoch  60 / 200, step    0 / 100; loss 1.4827\n",
      "Evaluate (epoch 60) -- logp(x) = -1.635 +/- 0.015\n",
      "epoch  61 / 200, step    0 / 100; loss 1.5711\n",
      "Evaluate (epoch 61) -- logp(x) = -1.466 +/- 0.021\n",
      "epoch  62 / 200, step    0 / 100; loss 1.4739\n",
      "Evaluate (epoch 62) -- logp(x) = -1.618 +/- 0.023\n",
      "epoch  63 / 200, step    0 / 100; loss 1.4466\n",
      "Evaluate (epoch 63) -- logp(x) = -1.428 +/- 0.017\n",
      "epoch  64 / 200, step    0 / 100; loss 1.5415\n",
      "Evaluate (epoch 64) -- logp(x) = -1.422 +/- 0.017\n",
      "epoch  65 / 200, step    0 / 100; loss 1.3086\n",
      "Evaluate (epoch 65) -- logp(x) = -1.427 +/- 0.022\n",
      "epoch  66 / 200, step    0 / 100; loss 1.3233\n",
      "Evaluate (epoch 66) -- logp(x) = -1.407 +/- 0.017\n",
      "epoch  67 / 200, step    0 / 100; loss 1.4184\n",
      "Evaluate (epoch 67) -- logp(x) = -1.412 +/- 0.017\n",
      "epoch  68 / 200, step    0 / 100; loss 1.5322\n",
      "Evaluate (epoch 68) -- logp(x) = -1.432 +/- 0.017\n",
      "epoch  69 / 200, step    0 / 100; loss 1.4786\n",
      "Evaluate (epoch 69) -- logp(x) = -1.391 +/- 0.018\n",
      "epoch  70 / 200, step    0 / 100; loss 1.4951\n",
      "Evaluate (epoch 70) -- logp(x) = -1.387 +/- 0.019\n",
      "epoch  71 / 200, step    0 / 100; loss 1.3095\n",
      "Evaluate (epoch 71) -- logp(x) = -1.773 +/- 0.048\n",
      "epoch  72 / 200, step    0 / 100; loss 1.6755\n",
      "Evaluate (epoch 72) -- logp(x) = -2.579 +/- 0.014\n",
      "epoch  73 / 200, step    0 / 100; loss 2.6249\n",
      "Evaluate (epoch 73) -- logp(x) = -2.360 +/- 0.013\n",
      "epoch  74 / 200, step    0 / 100; loss 2.3032\n",
      "Evaluate (epoch 74) -- logp(x) = -2.127 +/- 0.013\n",
      "epoch  75 / 200, step    0 / 100; loss 2.1230\n",
      "Evaluate (epoch 75) -- logp(x) = -2.018 +/- 0.014\n",
      "epoch  76 / 200, step    0 / 100; loss 1.9278\n",
      "Evaluate (epoch 76) -- logp(x) = -1.938 +/- 0.014\n",
      "epoch  77 / 200, step    0 / 100; loss 1.9502\n",
      "Evaluate (epoch 77) -- logp(x) = -1.875 +/- 0.015\n",
      "epoch  78 / 200, step    0 / 100; loss 1.8953\n",
      "Evaluate (epoch 78) -- logp(x) = -1.831 +/- 0.013\n",
      "epoch  79 / 200, step    0 / 100; loss 1.8785\n",
      "Evaluate (epoch 79) -- logp(x) = -1.751 +/- 0.014\n",
      "epoch  80 / 200, step    0 / 100; loss 1.7758\n",
      "Evaluate (epoch 80) -- logp(x) = -1.674 +/- 0.014\n",
      "epoch  81 / 200, step    0 / 100; loss 1.6481\n",
      "Evaluate (epoch 81) -- logp(x) = -1.679 +/- 0.014\n",
      "epoch  82 / 200, step    0 / 100; loss 1.7478\n",
      "Evaluate (epoch 82) -- logp(x) = -1.586 +/- 0.014\n",
      "epoch  83 / 200, step    0 / 100; loss 1.5959\n",
      "Evaluate (epoch 83) -- logp(x) = -1.614 +/- 0.014\n",
      "epoch  84 / 200, step    0 / 100; loss 1.6498\n",
      "Evaluate (epoch 84) -- logp(x) = -1.557 +/- 0.015\n",
      "epoch  85 / 200, step    0 / 100; loss 1.4665\n",
      "Evaluate (epoch 85) -- logp(x) = -1.532 +/- 0.015\n",
      "epoch  86 / 200, step    0 / 100; loss 1.5810\n",
      "Evaluate (epoch 86) -- logp(x) = -1.520 +/- 0.015\n",
      "epoch  87 / 200, step    0 / 100; loss 1.5294\n",
      "Evaluate (epoch 87) -- logp(x) = -1.523 +/- 0.016\n",
      "epoch  88 / 200, step    0 / 100; loss 1.5011\n",
      "Evaluate (epoch 88) -- logp(x) = -1.497 +/- 0.016\n",
      "epoch  89 / 200, step    0 / 100; loss 1.4177\n",
      "Evaluate (epoch 89) -- logp(x) = -1.471 +/- 0.016\n",
      "epoch  90 / 200, step    0 / 100; loss 1.4613\n",
      "Evaluate (epoch 90) -- logp(x) = -1.468 +/- 0.016\n",
      "epoch  91 / 200, step    0 / 100; loss 1.4442\n",
      "Evaluate (epoch 91) -- logp(x) = -1.450 +/- 0.016\n",
      "epoch  92 / 200, step    0 / 100; loss 1.3975\n",
      "Evaluate (epoch 92) -- logp(x) = -1.530 +/- 0.015\n",
      "epoch  93 / 200, step    0 / 100; loss 1.5237\n",
      "Evaluate (epoch 93) -- logp(x) = -1.445 +/- 0.016\n",
      "epoch  94 / 200, step    0 / 100; loss 1.4999\n",
      "Evaluate (epoch 94) -- logp(x) = -1.501 +/- 0.017\n",
      "epoch  95 / 200, step    0 / 100; loss 1.5358\n",
      "Evaluate (epoch 95) -- logp(x) = -1.448 +/- 0.017\n",
      "epoch  96 / 200, step    0 / 100; loss 1.3998\n",
      "Evaluate (epoch 96) -- logp(x) = -1.505 +/- 0.016\n",
      "epoch  97 / 200, step    0 / 100; loss 1.3815\n",
      "Evaluate (epoch 97) -- logp(x) = -1.449 +/- 0.017\n",
      "epoch  98 / 200, step    0 / 100; loss 1.4490\n",
      "Evaluate (epoch 98) -- logp(x) = -1.435 +/- 0.017\n",
      "epoch  99 / 200, step    0 / 100; loss 1.4250\n",
      "Evaluate (epoch 99) -- logp(x) = -1.447 +/- 0.017\n",
      "epoch 100 / 200, step    0 / 100; loss 1.4983\n",
      "Evaluate (epoch 100) -- logp(x) = -1.394 +/- 0.017\n",
      "epoch 101 / 200, step    0 / 100; loss 1.5647\n",
      "Evaluate (epoch 101) -- logp(x) = -1.380 +/- 0.018\n",
      "epoch 102 / 200, step    0 / 100; loss 1.3895\n",
      "Evaluate (epoch 102) -- logp(x) = -1.388 +/- 0.018\n",
      "epoch 103 / 200, step    0 / 100; loss 1.4230\n",
      "Evaluate (epoch 103) -- logp(x) = -1.378 +/- 0.017\n",
      "epoch 104 / 200, step    0 / 100; loss 1.2715\n",
      "Evaluate (epoch 104) -- logp(x) = -1.377 +/- 0.018\n",
      "epoch 105 / 200, step    0 / 100; loss 1.5244\n",
      "Evaluate (epoch 105) -- logp(x) = -1.360 +/- 0.018\n",
      "epoch 106 / 200, step    0 / 100; loss 1.3762\n",
      "Evaluate (epoch 106) -- logp(x) = -1.355 +/- 0.018\n",
      "epoch 107 / 200, step    0 / 100; loss 1.3126\n",
      "Evaluate (epoch 107) -- logp(x) = -1.488 +/- 0.017\n",
      "epoch 108 / 200, step    0 / 100; loss 1.4078\n",
      "Evaluate (epoch 108) -- logp(x) = -1.423 +/- 0.017\n",
      "epoch 109 / 200, step    0 / 100; loss 1.3196\n",
      "Evaluate (epoch 109) -- logp(x) = -1.360 +/- 0.018\n",
      "epoch 110 / 200, step    0 / 100; loss 1.1614\n",
      "Evaluate (epoch 110) -- logp(x) = -1.361 +/- 0.018\n",
      "epoch 111 / 200, step    0 / 100; loss 1.3738\n",
      "Evaluate (epoch 111) -- logp(x) = -1.352 +/- 0.018\n",
      "epoch 112 / 200, step    0 / 100; loss 1.5705\n",
      "Evaluate (epoch 112) -- logp(x) = -1.387 +/- 0.029\n",
      "epoch 113 / 200, step    0 / 100; loss 1.5975\n",
      "Evaluate (epoch 113) -- logp(x) = -1.722 +/- 0.019\n",
      "epoch 114 / 200, step    0 / 100; loss 1.6351\n",
      "Evaluate (epoch 114) -- logp(x) = -1.538 +/- 0.017\n",
      "epoch 115 / 200, step    0 / 100; loss 1.5908\n",
      "Evaluate (epoch 115) -- logp(x) = -1.458 +/- 0.018\n",
      "epoch 116 / 200, step    0 / 100; loss 1.4017\n",
      "Evaluate (epoch 116) -- logp(x) = -1.417 +/- 0.018\n",
      "epoch 117 / 200, step    0 / 100; loss 1.3372\n",
      "Evaluate (epoch 117) -- logp(x) = -1.394 +/- 0.017\n",
      "epoch 118 / 200, step    0 / 100; loss 1.4526\n",
      "Evaluate (epoch 118) -- logp(x) = -1.446 +/- 0.018\n",
      "epoch 119 / 200, step    0 / 100; loss 1.5175\n",
      "Evaluate (epoch 119) -- logp(x) = -1.366 +/- 0.018\n",
      "epoch 120 / 200, step    0 / 100; loss 1.2824\n",
      "Evaluate (epoch 120) -- logp(x) = -1.414 +/- 0.034\n",
      "epoch 121 / 200, step    0 / 100; loss 1.3520\n",
      "Evaluate (epoch 121) -- logp(x) = -1.351 +/- 0.018\n",
      "epoch 122 / 200, step    0 / 100; loss 1.3821\n",
      "Evaluate (epoch 122) -- logp(x) = -1.344 +/- 0.019\n",
      "epoch 123 / 200, step    0 / 100; loss 1.4672\n",
      "Evaluate (epoch 123) -- logp(x) = -1.375 +/- 0.018\n",
      "epoch 124 / 200, step    0 / 100; loss 1.3567\n",
      "Evaluate (epoch 124) -- logp(x) = -1.383 +/- 0.018\n",
      "epoch 125 / 200, step    0 / 100; loss 1.3685\n",
      "Evaluate (epoch 125) -- logp(x) = -1.324 +/- 0.018\n",
      "epoch 126 / 200, step    0 / 100; loss 1.2537\n",
      "Evaluate (epoch 126) -- logp(x) = -1.330 +/- 0.019\n",
      "epoch 127 / 200, step    0 / 100; loss 1.3424\n",
      "Evaluate (epoch 127) -- logp(x) = -1.388 +/- 0.019\n",
      "epoch 128 / 200, step    0 / 100; loss 1.2466\n",
      "Evaluate (epoch 128) -- logp(x) = -1.384 +/- 0.018\n",
      "epoch 129 / 200, step    0 / 100; loss 1.4320\n",
      "Evaluate (epoch 129) -- logp(x) = -1.359 +/- 0.018\n",
      "epoch 130 / 200, step    0 / 100; loss 1.4684\n",
      "Evaluate (epoch 130) -- logp(x) = -1.346 +/- 0.020\n",
      "epoch 131 / 200, step    0 / 100; loss 1.4975\n",
      "Evaluate (epoch 131) -- logp(x) = -1.335 +/- 0.020\n",
      "epoch 132 / 200, step    0 / 100; loss 1.5001\n",
      "Evaluate (epoch 132) -- logp(x) = -1.302 +/- 0.018\n",
      "epoch 133 / 200, step    0 / 100; loss 1.2790\n",
      "Evaluate (epoch 133) -- logp(x) = -1.301 +/- 0.020\n",
      "epoch 134 / 200, step    0 / 100; loss 1.2696\n",
      "Evaluate (epoch 134) -- logp(x) = -1.289 +/- 0.020\n",
      "epoch 135 / 200, step    0 / 100; loss 1.2978\n",
      "Evaluate (epoch 135) -- logp(x) = -1.351 +/- 0.019\n",
      "epoch 136 / 200, step    0 / 100; loss 1.2931\n",
      "Evaluate (epoch 136) -- logp(x) = -1.308 +/- 0.020\n",
      "epoch 137 / 200, step    0 / 100; loss 1.2627\n",
      "Evaluate (epoch 137) -- logp(x) = -1.367 +/- 0.019\n",
      "epoch 138 / 200, step    0 / 100; loss 1.3320\n",
      "Evaluate (epoch 138) -- logp(x) = -1.295 +/- 0.018\n",
      "epoch 139 / 200, step    0 / 100; loss 1.3028\n",
      "Evaluate (epoch 139) -- logp(x) = -1.307 +/- 0.020\n",
      "epoch 140 / 200, step    0 / 100; loss 1.3564\n",
      "Evaluate (epoch 140) -- logp(x) = -1.285 +/- 0.019\n",
      "epoch 141 / 200, step    0 / 100; loss 1.3519\n",
      "Evaluate (epoch 141) -- logp(x) = -1.313 +/- 0.019\n",
      "epoch 142 / 200, step    0 / 100; loss 1.2890\n",
      "Evaluate (epoch 142) -- logp(x) = -1.328 +/- 0.019\n",
      "epoch 143 / 200, step    0 / 100; loss 1.3257\n",
      "Evaluate (epoch 143) -- logp(x) = -1.369 +/- 0.018\n",
      "epoch 144 / 200, step    0 / 100; loss 1.2294\n",
      "Evaluate (epoch 144) -- logp(x) = -1.279 +/- 0.018\n",
      "epoch 145 / 200, step    0 / 100; loss 1.1427\n",
      "Evaluate (epoch 145) -- logp(x) = -1.297 +/- 0.019\n",
      "epoch 146 / 200, step    0 / 100; loss 1.3299\n",
      "Evaluate (epoch 146) -- logp(x) = -1.405 +/- 0.018\n",
      "epoch 147 / 200, step    0 / 100; loss 1.3910\n",
      "Evaluate (epoch 147) -- logp(x) = -1.392 +/- 0.018\n",
      "epoch 148 / 200, step    0 / 100; loss 1.4568\n",
      "Evaluate (epoch 148) -- logp(x) = -1.284 +/- 0.020\n",
      "epoch 149 / 200, step    0 / 100; loss 1.4325\n",
      "Evaluate (epoch 149) -- logp(x) = -1.332 +/- 0.020\n",
      "epoch 150 / 200, step    0 / 100; loss 1.3418\n",
      "Evaluate (epoch 150) -- logp(x) = -1.291 +/- 0.019\n",
      "epoch 151 / 200, step    0 / 100; loss 1.2335\n",
      "Evaluate (epoch 151) -- logp(x) = -1.259 +/- 0.018\n",
      "epoch 152 / 200, step    0 / 100; loss 1.2379\n",
      "Evaluate (epoch 152) -- logp(x) = -1.339 +/- 0.019\n",
      "epoch 153 / 200, step    0 / 100; loss 1.4871\n",
      "Evaluate (epoch 153) -- logp(x) = -1.295 +/- 0.018\n",
      "epoch 154 / 200, step    0 / 100; loss 1.3917\n",
      "Evaluate (epoch 154) -- logp(x) = -1.277 +/- 0.018\n",
      "epoch 155 / 200, step    0 / 100; loss 1.1797\n",
      "Evaluate (epoch 155) -- logp(x) = -1.254 +/- 0.020\n",
      "epoch 156 / 200, step    0 / 100; loss 1.4027\n",
      "Evaluate (epoch 156) -- logp(x) = -1.822 +/- 0.015\n",
      "epoch 157 / 200, step    0 / 100; loss 1.8767\n",
      "Evaluate (epoch 157) -- logp(x) = -1.533 +/- 0.019\n",
      "epoch 158 / 200, step    0 / 100; loss 1.5409\n",
      "Evaluate (epoch 158) -- logp(x) = -1.550 +/- 0.017\n",
      "epoch 159 / 200, step    0 / 100; loss 1.5570\n",
      "Evaluate (epoch 159) -- logp(x) = -1.464 +/- 0.018\n",
      "epoch 160 / 200, step    0 / 100; loss 1.5737\n",
      "Evaluate (epoch 160) -- logp(x) = -1.416 +/- 0.020\n",
      "epoch 161 / 200, step    0 / 100; loss 1.4443\n",
      "Evaluate (epoch 161) -- logp(x) = -1.369 +/- 0.019\n",
      "epoch 162 / 200, step    0 / 100; loss 1.3194\n",
      "Evaluate (epoch 162) -- logp(x) = -1.481 +/- 0.072\n",
      "epoch 163 / 200, step    0 / 100; loss 1.3846\n",
      "Evaluate (epoch 163) -- logp(x) = -1.793 +/- 0.017\n",
      "epoch 164 / 200, step    0 / 100; loss 1.6505\n",
      "Evaluate (epoch 164) -- logp(x) = -1.606 +/- 0.019\n",
      "epoch 165 / 200, step    0 / 100; loss 1.7072\n",
      "Evaluate (epoch 165) -- logp(x) = -1.476 +/- 0.016\n",
      "epoch 166 / 200, step    0 / 100; loss 1.6676\n",
      "Evaluate (epoch 166) -- logp(x) = -1.368 +/- 0.016\n",
      "epoch 167 / 200, step    0 / 100; loss 1.3893\n",
      "Evaluate (epoch 167) -- logp(x) = -1.344 +/- 0.016\n",
      "epoch 168 / 200, step    0 / 100; loss 1.3200\n",
      "Evaluate (epoch 168) -- logp(x) = -1.300 +/- 0.017\n",
      "epoch 169 / 200, step    0 / 100; loss 1.2789\n",
      "Evaluate (epoch 169) -- logp(x) = -1.303 +/- 0.017\n",
      "epoch 170 / 200, step    0 / 100; loss 1.1780\n",
      "Evaluate (epoch 170) -- logp(x) = -1.316 +/- 0.018\n",
      "epoch 171 / 200, step    0 / 100; loss 1.2860\n",
      "Evaluate (epoch 171) -- logp(x) = -1.249 +/- 0.018\n",
      "epoch 172 / 200, step    0 / 100; loss 1.3117\n",
      "Evaluate (epoch 172) -- logp(x) = -1.419 +/- 0.018\n",
      "epoch 173 / 200, step    0 / 100; loss 1.6167\n",
      "Evaluate (epoch 173) -- logp(x) = -1.339 +/- 0.019\n",
      "epoch 174 / 200, step    0 / 100; loss 1.2366\n",
      "Evaluate (epoch 174) -- logp(x) = -1.427 +/- 0.018\n",
      "epoch 175 / 200, step    0 / 100; loss 1.3615\n",
      "Evaluate (epoch 175) -- logp(x) = -1.290 +/- 0.018\n",
      "epoch 176 / 200, step    0 / 100; loss 1.2904\n",
      "Evaluate (epoch 176) -- logp(x) = -1.317 +/- 0.019\n",
      "epoch 177 / 200, step    0 / 100; loss 1.3598\n",
      "Evaluate (epoch 177) -- logp(x) = -1.563 +/- 0.015\n",
      "epoch 178 / 200, step    0 / 100; loss 1.4537\n",
      "Evaluate (epoch 178) -- logp(x) = -1.250 +/- 0.022\n",
      "epoch 179 / 200, step    0 / 100; loss 1.2554\n",
      "Evaluate (epoch 179) -- logp(x) = -1.362 +/- 0.018\n",
      "epoch 180 / 200, step    0 / 100; loss 1.2322\n",
      "Evaluate (epoch 180) -- logp(x) = -1.299 +/- 0.018\n",
      "epoch 181 / 200, step    0 / 100; loss 1.2609\n",
      "Evaluate (epoch 181) -- logp(x) = -1.648 +/- 0.015\n",
      "epoch 182 / 200, step    0 / 100; loss 1.7629\n",
      "Evaluate (epoch 182) -- logp(x) = -1.248 +/- 0.018\n",
      "epoch 183 / 200, step    0 / 100; loss 1.3224\n",
      "Evaluate (epoch 183) -- logp(x) = -1.264 +/- 0.019\n",
      "epoch 184 / 200, step    0 / 100; loss 1.2927\n",
      "Evaluate (epoch 184) -- logp(x) = -1.246 +/- 0.019\n",
      "epoch 185 / 200, step    0 / 100; loss 1.1833\n",
      "Evaluate (epoch 185) -- logp(x) = -1.420 +/- 0.017\n",
      "epoch 186 / 200, step    0 / 100; loss 1.5936\n",
      "Evaluate (epoch 186) -- logp(x) = -1.289 +/- 0.016\n",
      "epoch 187 / 200, step    0 / 100; loss 1.3103\n",
      "Evaluate (epoch 187) -- logp(x) = -1.359 +/- 0.017\n",
      "epoch 188 / 200, step    0 / 100; loss 1.2818\n",
      "Evaluate (epoch 188) -- logp(x) = -1.286 +/- 0.018\n",
      "epoch 189 / 200, step    0 / 100; loss 1.2486\n",
      "Evaluate (epoch 189) -- logp(x) = -1.236 +/- 0.071\n",
      "epoch 190 / 200, step    0 / 100; loss 1.2596\n",
      "Evaluate (epoch 190) -- logp(x) = -1.456 +/- 0.016\n",
      "epoch 191 / 200, step    0 / 100; loss 1.5302\n",
      "Evaluate (epoch 191) -- logp(x) = -1.260 +/- 0.019\n",
      "epoch 192 / 200, step    0 / 100; loss 1.2166\n",
      "Evaluate (epoch 192) -- logp(x) = -1.343 +/- 0.016\n",
      "epoch 193 / 200, step    0 / 100; loss 1.3255\n",
      "Evaluate (epoch 193) -- logp(x) = -1.257 +/- 0.018\n",
      "epoch 194 / 200, step    0 / 100; loss 1.2114\n",
      "Evaluate (epoch 194) -- logp(x) = -1.349 +/- 0.018\n",
      "epoch 195 / 200, step    0 / 100; loss 1.3306\n",
      "Evaluate (epoch 195) -- logp(x) = -1.223 +/- 0.018\n",
      "epoch 196 / 200, step    0 / 100; loss 1.0851\n",
      "Evaluate (epoch 196) -- logp(x) = -1.592 +/- 0.017\n",
      "epoch 197 / 200, step    0 / 100; loss 1.6695\n",
      "Evaluate (epoch 197) -- logp(x) = -1.445 +/- 0.017\n",
      "epoch 198 / 200, step    0 / 100; loss 1.3917\n",
      "Evaluate (epoch 198) -- logp(x) = -1.260 +/- 0.016\n",
      "epoch 199 / 200, step    0 / 100; loss 1.4119\n",
      "Evaluate (epoch 199) -- logp(x) = -1.211 +/- 0.018\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'TORUS',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': True,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 3,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 3,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'maf',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 50,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': True,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/maf',\n",
      " 'restore_file': './results/maf/best_model_checkpoint.pt',\n",
      " 'results_file': './results/maf\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 200,\n",
      " 'train': False}\n",
      "MAF(\n",
      "  (net): FlowSequential(\n",
      "    (0): MADE(\n",
      "      (net_input): MaskedLinear(in_features=3, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=6, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): MADE(\n",
      "      (net_input): MaskedLinear(in_features=3, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=6, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): MADE(\n",
      "      (net_input): MaskedLinear(in_features=3, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=6, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): MADE(\n",
      "      (net_input): MaskedLinear(in_features=3, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=6, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): MADE(\n",
      "      (net_input): MaskedLinear(in_features=3, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=6, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'INVOLUTE',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': False,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 2,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 2,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'maf',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 200,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': True,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/maf',\n",
      " 'restore_file': None,\n",
      " 'results_file': './results/maf\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 0,\n",
      " 'train': True}\n",
      "MAF(\n",
      "  (net): FlowSequential(\n",
      "    (0): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch   0 / 200, step    0 / 100; loss 2.0109\n",
      "Evaluate (epoch 0) -- logp(x) = -1.020 +/- 0.016\n",
      "epoch   1 / 200, step    0 / 100; loss 1.0857\n",
      "Evaluate (epoch 1) -- logp(x) = -1.000 +/- 0.016\n",
      "epoch   2 / 200, step    0 / 100; loss 1.1009\n",
      "Evaluate (epoch 2) -- logp(x) = -0.986 +/- 0.016\n",
      "epoch   3 / 200, step    0 / 100; loss 0.8168\n",
      "Evaluate (epoch 3) -- logp(x) = -0.970 +/- 0.017\n",
      "epoch   4 / 200, step    0 / 100; loss 1.0377\n",
      "Evaluate (epoch 4) -- logp(x) = -0.955 +/- 0.016\n",
      "epoch   5 / 200, step    0 / 100; loss 1.0162\n",
      "Evaluate (epoch 5) -- logp(x) = -0.938 +/- 0.016\n",
      "epoch   6 / 200, step    0 / 100; loss 0.9340\n",
      "Evaluate (epoch 6) -- logp(x) = -0.918 +/- 0.015\n",
      "epoch   7 / 200, step    0 / 100; loss 1.0946\n",
      "Evaluate (epoch 7) -- logp(x) = -0.888 +/- 0.015\n",
      "epoch   8 / 200, step    0 / 100; loss 0.8413\n",
      "Evaluate (epoch 8) -- logp(x) = -0.850 +/- 0.014\n",
      "epoch   9 / 200, step    0 / 100; loss 0.7849\n",
      "Evaluate (epoch 9) -- logp(x) = -0.821 +/- 0.014\n",
      "epoch  10 / 200, step    0 / 100; loss 0.7189\n",
      "Evaluate (epoch 10) -- logp(x) = -0.811 +/- 0.014\n",
      "epoch  11 / 200, step    0 / 100; loss 0.8252\n",
      "Evaluate (epoch 11) -- logp(x) = -0.798 +/- 0.014\n",
      "epoch  12 / 200, step    0 / 100; loss 0.8609\n",
      "Evaluate (epoch 12) -- logp(x) = -0.777 +/- 0.014\n",
      "epoch  13 / 200, step    0 / 100; loss 0.9837\n",
      "Evaluate (epoch 13) -- logp(x) = -0.774 +/- 0.015\n",
      "epoch  14 / 200, step    0 / 100; loss 0.7624\n",
      "Evaluate (epoch 14) -- logp(x) = -0.786 +/- 0.014\n",
      "epoch  15 / 200, step    0 / 100; loss 0.6777\n",
      "Evaluate (epoch 15) -- logp(x) = -0.769 +/- 0.014\n",
      "epoch  16 / 200, step    0 / 100; loss 0.6892\n",
      "Evaluate (epoch 16) -- logp(x) = -0.749 +/- 0.014\n",
      "epoch  17 / 200, step    0 / 100; loss 0.7793\n",
      "Evaluate (epoch 17) -- logp(x) = -0.758 +/- 0.015\n",
      "epoch  18 / 200, step    0 / 100; loss 0.7921\n",
      "Evaluate (epoch 18) -- logp(x) = -0.796 +/- 0.015\n",
      "epoch  19 / 200, step    0 / 100; loss 0.8346\n",
      "Evaluate (epoch 19) -- logp(x) = -0.740 +/- 0.015\n",
      "epoch  20 / 200, step    0 / 100; loss 0.6556\n",
      "Evaluate (epoch 20) -- logp(x) = -0.799 +/- 0.016\n",
      "epoch  21 / 200, step    0 / 100; loss 0.8483\n",
      "Evaluate (epoch 21) -- logp(x) = -0.755 +/- 0.016\n",
      "epoch  22 / 200, step    0 / 100; loss 0.8437\n",
      "Evaluate (epoch 22) -- logp(x) = -0.729 +/- 0.015\n",
      "epoch  23 / 200, step    0 / 100; loss 0.6710\n",
      "Evaluate (epoch 23) -- logp(x) = -0.745 +/- 0.014\n",
      "epoch  24 / 200, step    0 / 100; loss 0.7442\n",
      "Evaluate (epoch 24) -- logp(x) = -0.720 +/- 0.015\n",
      "epoch  25 / 200, step    0 / 100; loss 0.7128\n",
      "Evaluate (epoch 25) -- logp(x) = -0.769 +/- 0.015\n",
      "epoch  26 / 200, step    0 / 100; loss 0.7755\n",
      "Evaluate (epoch 26) -- logp(x) = -0.802 +/- 0.025\n",
      "epoch  27 / 200, step    0 / 100; loss 1.0038\n",
      "Evaluate (epoch 27) -- logp(x) = -0.726 +/- 0.016\n",
      "epoch  28 / 200, step    0 / 100; loss 0.7321\n",
      "Evaluate (epoch 28) -- logp(x) = -0.780 +/- 0.129\n",
      "epoch  29 / 200, step    0 / 100; loss 0.6971\n",
      "Evaluate (epoch 29) -- logp(x) = -0.706 +/- 0.015\n",
      "epoch  30 / 200, step    0 / 100; loss 0.6184\n",
      "Evaluate (epoch 30) -- logp(x) = -0.704 +/- 0.015\n",
      "epoch  31 / 200, step    0 / 100; loss 0.6896\n",
      "Evaluate (epoch 31) -- logp(x) = -0.735 +/- 0.016\n",
      "epoch  32 / 200, step    0 / 100; loss 0.7618\n",
      "Evaluate (epoch 32) -- logp(x) = -0.711 +/- 0.016\n",
      "epoch  33 / 200, step    0 / 100; loss 0.6020\n",
      "Evaluate (epoch 33) -- logp(x) = -0.777 +/- 0.017\n",
      "epoch  34 / 200, step    0 / 100; loss 0.8234\n",
      "Evaluate (epoch 34) -- logp(x) = -0.747 +/- 0.016\n",
      "epoch  35 / 200, step    0 / 100; loss 0.8191\n",
      "Evaluate (epoch 35) -- logp(x) = -0.740 +/- 0.016\n",
      "epoch  36 / 200, step    0 / 100; loss 0.6882\n",
      "Evaluate (epoch 36) -- logp(x) = -0.716 +/- 0.016\n",
      "epoch  37 / 200, step    0 / 100; loss 0.6312\n",
      "Evaluate (epoch 37) -- logp(x) = -0.718 +/- 0.016\n",
      "epoch  38 / 200, step    0 / 100; loss 0.7460\n",
      "Evaluate (epoch 38) -- logp(x) = -0.708 +/- 0.016\n",
      "epoch  39 / 200, step    0 / 100; loss 0.6356\n",
      "Evaluate (epoch 39) -- logp(x) = -0.694 +/- 0.016\n",
      "epoch  40 / 200, step    0 / 100; loss 0.7427\n",
      "Evaluate (epoch 40) -- logp(x) = -0.697 +/- 0.016\n",
      "epoch  41 / 200, step    0 / 100; loss 0.7732\n",
      "Evaluate (epoch 41) -- logp(x) = -0.688 +/- 0.017\n",
      "epoch  42 / 200, step    0 / 100; loss 0.7530\n",
      "Evaluate (epoch 42) -- logp(x) = -0.687 +/- 0.017\n",
      "epoch  43 / 200, step    0 / 100; loss 0.8257\n",
      "Evaluate (epoch 43) -- logp(x) = -0.683 +/- 0.017\n",
      "epoch  44 / 200, step    0 / 100; loss 0.6987\n",
      "Evaluate (epoch 44) -- logp(x) = -0.684 +/- 0.017\n",
      "epoch  45 / 200, step    0 / 100; loss 0.6648\n",
      "Evaluate (epoch 45) -- logp(x) = -0.682 +/- 0.019\n",
      "epoch  46 / 200, step    0 / 100; loss 0.8247\n",
      "Evaluate (epoch 46) -- logp(x) = -0.681 +/- 0.017\n",
      "epoch  47 / 200, step    0 / 100; loss 0.7301\n",
      "Evaluate (epoch 47) -- logp(x) = -0.678 +/- 0.017\n",
      "epoch  48 / 200, step    0 / 100; loss 0.5820\n",
      "Evaluate (epoch 48) -- logp(x) = -0.719 +/- 0.016\n",
      "epoch  49 / 200, step    0 / 100; loss 0.7312\n",
      "Evaluate (epoch 49) -- logp(x) = -0.690 +/- 0.018\n",
      "epoch  50 / 200, step    0 / 100; loss 0.7092\n",
      "Evaluate (epoch 50) -- logp(x) = -0.670 +/- 0.017\n",
      "epoch  51 / 200, step    0 / 100; loss 0.7106\n",
      "Evaluate (epoch 51) -- logp(x) = -0.672 +/- 0.019\n",
      "epoch  52 / 200, step    0 / 100; loss 0.6397\n",
      "Evaluate (epoch 52) -- logp(x) = -0.653 +/- 0.018\n",
      "epoch  53 / 200, step    0 / 100; loss 0.7008\n",
      "Evaluate (epoch 53) -- logp(x) = -0.667 +/- 0.017\n",
      "epoch  54 / 200, step    0 / 100; loss 0.7948\n",
      "Evaluate (epoch 54) -- logp(x) = -0.696 +/- 0.017\n",
      "epoch  55 / 200, step    0 / 100; loss 0.4842\n",
      "Evaluate (epoch 55) -- logp(x) = -0.660 +/- 0.018\n",
      "epoch  56 / 200, step    0 / 100; loss 0.6966\n",
      "Evaluate (epoch 56) -- logp(x) = -0.704 +/- 0.018\n",
      "epoch  57 / 200, step    0 / 100; loss 0.7936\n",
      "Evaluate (epoch 57) -- logp(x) = -0.669 +/- 0.021\n",
      "epoch  58 / 200, step    0 / 100; loss 0.6316\n",
      "Evaluate (epoch 58) -- logp(x) = -0.659 +/- 0.019\n",
      "epoch  59 / 200, step    0 / 100; loss 0.6808\n",
      "Evaluate (epoch 59) -- logp(x) = -1.302 +/- 0.061\n",
      "epoch  60 / 200, step    0 / 100; loss 1.1514\n",
      "Evaluate (epoch 60) -- logp(x) = -0.944 +/- 0.019\n",
      "epoch  61 / 200, step    0 / 100; loss 0.7700\n",
      "Evaluate (epoch 61) -- logp(x) = -0.850 +/- 0.019\n",
      "epoch  62 / 200, step    0 / 100; loss 0.9654\n",
      "Evaluate (epoch 62) -- logp(x) = -0.732 +/- 0.019\n",
      "epoch  63 / 200, step    0 / 100; loss 0.6872\n",
      "Evaluate (epoch 63) -- logp(x) = -0.702 +/- 0.019\n",
      "epoch  64 / 200, step    0 / 100; loss 0.7607\n",
      "Evaluate (epoch 64) -- logp(x) = -0.671 +/- 0.019\n",
      "epoch  65 / 200, step    0 / 100; loss 0.5823\n",
      "Evaluate (epoch 65) -- logp(x) = -0.670 +/- 0.018\n",
      "epoch  66 / 200, step    0 / 100; loss 0.7319\n",
      "Evaluate (epoch 66) -- logp(x) = -0.688 +/- 0.018\n",
      "epoch  67 / 200, step    0 / 100; loss 0.6442\n",
      "Evaluate (epoch 67) -- logp(x) = -0.715 +/- 0.019\n",
      "epoch  68 / 200, step    0 / 100; loss 0.7730\n",
      "Evaluate (epoch 68) -- logp(x) = -0.666 +/- 0.019\n",
      "epoch  69 / 200, step    0 / 100; loss 0.6329\n",
      "Evaluate (epoch 69) -- logp(x) = -0.668 +/- 0.019\n",
      "epoch  70 / 200, step    0 / 100; loss 0.4844\n",
      "Evaluate (epoch 70) -- logp(x) = -0.659 +/- 0.018\n",
      "epoch  71 / 200, step    0 / 100; loss 0.7919\n",
      "Evaluate (epoch 71) -- logp(x) = -0.738 +/- 0.018\n",
      "epoch  72 / 200, step    0 / 100; loss 0.6753\n",
      "Evaluate (epoch 72) -- logp(x) = -0.657 +/- 0.018\n",
      "epoch  73 / 200, step    0 / 100; loss 0.6299\n",
      "Evaluate (epoch 73) -- logp(x) = -0.642 +/- 0.018\n",
      "epoch  74 / 200, step    0 / 100; loss 0.6559\n",
      "Evaluate (epoch 74) -- logp(x) = -0.641 +/- 0.019\n",
      "epoch  75 / 200, step    0 / 100; loss 0.6105\n",
      "Evaluate (epoch 75) -- logp(x) = -0.654 +/- 0.019\n",
      "epoch  76 / 200, step    0 / 100; loss 0.5853\n",
      "Evaluate (epoch 76) -- logp(x) = -0.628 +/- 0.019\n",
      "epoch  77 / 200, step    0 / 100; loss 0.6667\n",
      "Evaluate (epoch 77) -- logp(x) = -0.630 +/- 0.019\n",
      "epoch  78 / 200, step    0 / 100; loss 0.7153\n",
      "Evaluate (epoch 78) -- logp(x) = -0.694 +/- 0.020\n",
      "epoch  79 / 200, step    0 / 100; loss 0.8216\n",
      "Evaluate (epoch 79) -- logp(x) = -0.638 +/- 0.019\n",
      "epoch  80 / 200, step    0 / 100; loss 0.7753\n",
      "Evaluate (epoch 80) -- logp(x) = -0.638 +/- 0.019\n",
      "epoch  81 / 200, step    0 / 100; loss 0.5922\n",
      "Evaluate (epoch 81) -- logp(x) = -0.668 +/- 0.019\n",
      "epoch  82 / 200, step    0 / 100; loss 0.7428\n",
      "Evaluate (epoch 82) -- logp(x) = -0.634 +/- 0.019\n",
      "epoch  83 / 200, step    0 / 100; loss 0.7923\n",
      "Evaluate (epoch 83) -- logp(x) = -0.631 +/- 0.019\n",
      "epoch  84 / 200, step    0 / 100; loss 0.5964\n",
      "Evaluate (epoch 84) -- logp(x) = -0.625 +/- 0.019\n",
      "epoch  85 / 200, step    0 / 100; loss 0.5949\n",
      "Evaluate (epoch 85) -- logp(x) = -0.603 +/- 0.019\n",
      "epoch  86 / 200, step    0 / 100; loss 0.6283\n",
      "Evaluate (epoch 86) -- logp(x) = -0.695 +/- 0.023\n",
      "epoch  87 / 200, step    0 / 100; loss 0.8713\n",
      "Evaluate (epoch 87) -- logp(x) = -0.855 +/- 0.018\n",
      "epoch  88 / 200, step    0 / 100; loss 0.8977\n",
      "Evaluate (epoch 88) -- logp(x) = -0.766 +/- 0.020\n",
      "epoch  89 / 200, step    0 / 100; loss 0.8386\n",
      "Evaluate (epoch 89) -- logp(x) = -0.703 +/- 0.019\n",
      "epoch  90 / 200, step    0 / 100; loss 0.6742\n",
      "Evaluate (epoch 90) -- logp(x) = -33180.270 +/- 50675.902\n",
      "epoch  91 / 200, step    0 / 100; loss 2368324.5000\n",
      "Evaluate (epoch 91) -- logp(x) = -0.944 +/- 0.018\n",
      "epoch  92 / 200, step    0 / 100; loss 0.9647\n",
      "Evaluate (epoch 92) -- logp(x) = -0.869 +/- 0.018\n",
      "epoch  93 / 200, step    0 / 100; loss 0.9292\n",
      "Evaluate (epoch 93) -- logp(x) = -0.824 +/- 0.018\n",
      "epoch  94 / 200, step    0 / 100; loss 0.8518\n",
      "Evaluate (epoch 94) -- logp(x) = -0.800 +/- 0.019\n",
      "epoch  95 / 200, step    0 / 100; loss 0.9385\n",
      "Evaluate (epoch 95) -- logp(x) = -0.780 +/- 0.019\n",
      "epoch  96 / 200, step    0 / 100; loss 0.8845\n",
      "Evaluate (epoch 96) -- logp(x) = -0.767 +/- 0.019\n",
      "epoch  97 / 200, step    0 / 100; loss 0.7867\n",
      "Evaluate (epoch 97) -- logp(x) = -0.760 +/- 0.020\n",
      "epoch  98 / 200, step    0 / 100; loss 0.7558\n",
      "Evaluate (epoch 98) -- logp(x) = -0.742 +/- 0.019\n",
      "epoch  99 / 200, step    0 / 100; loss 0.7497\n",
      "Evaluate (epoch 99) -- logp(x) = -0.737 +/- 0.020\n",
      "epoch 100 / 200, step    0 / 100; loss 0.8464\n",
      "Evaluate (epoch 100) -- logp(x) = -0.726 +/- 0.020\n",
      "epoch 101 / 200, step    0 / 100; loss 0.6905\n",
      "Evaluate (epoch 101) -- logp(x) = -0.721 +/- 0.020\n",
      "epoch 102 / 200, step    0 / 100; loss 0.6890\n",
      "Evaluate (epoch 102) -- logp(x) = -0.705 +/- 0.020\n",
      "epoch 103 / 200, step    0 / 100; loss 0.6260\n",
      "Evaluate (epoch 103) -- logp(x) = -0.694 +/- 0.020\n",
      "epoch 104 / 200, step    0 / 100; loss 0.7729\n",
      "Evaluate (epoch 104) -- logp(x) = -0.686 +/- 0.020\n",
      "epoch 105 / 200, step    0 / 100; loss 0.6928\n",
      "Evaluate (epoch 105) -- logp(x) = -0.680 +/- 0.020\n",
      "epoch 106 / 200, step    0 / 100; loss 0.7027\n",
      "Evaluate (epoch 106) -- logp(x) = -0.696 +/- 0.019\n",
      "epoch 107 / 200, step    0 / 100; loss 0.6856\n",
      "Evaluate (epoch 107) -- logp(x) = -0.672 +/- 0.020\n",
      "epoch 108 / 200, step    0 / 100; loss 0.4683\n",
      "Evaluate (epoch 108) -- logp(x) = -0.668 +/- 0.019\n",
      "epoch 109 / 200, step    0 / 100; loss 0.7235\n",
      "Evaluate (epoch 109) -- logp(x) = -0.675 +/- 0.021\n",
      "epoch 110 / 200, step    0 / 100; loss 0.7267\n",
      "Evaluate (epoch 110) -- logp(x) = -0.671 +/- 0.020\n",
      "epoch 111 / 200, step    0 / 100; loss 0.6860\n",
      "Evaluate (epoch 111) -- logp(x) = -0.668 +/- 0.020\n",
      "epoch 112 / 200, step    0 / 100; loss 0.5552\n",
      "Evaluate (epoch 112) -- logp(x) = -0.688 +/- 0.020\n",
      "epoch 113 / 200, step    0 / 100; loss 0.7523\n",
      "Evaluate (epoch 113) -- logp(x) = -0.661 +/- 0.020\n",
      "epoch 114 / 200, step    0 / 100; loss 0.8192\n",
      "Evaluate (epoch 114) -- logp(x) = -0.660 +/- 0.020\n",
      "epoch 115 / 200, step    0 / 100; loss 0.7532\n",
      "Evaluate (epoch 115) -- logp(x) = -0.661 +/- 0.021\n",
      "epoch 116 / 200, step    0 / 100; loss 0.7359\n",
      "Evaluate (epoch 116) -- logp(x) = -0.652 +/- 0.020\n",
      "epoch 117 / 200, step    0 / 100; loss 0.7492\n",
      "Evaluate (epoch 117) -- logp(x) = -0.651 +/- 0.020\n",
      "epoch 118 / 200, step    0 / 100; loss 0.6511\n",
      "Evaluate (epoch 118) -- logp(x) = -0.644 +/- 0.020\n",
      "epoch 119 / 200, step    0 / 100; loss 0.5616\n",
      "Evaluate (epoch 119) -- logp(x) = -0.680 +/- 0.023\n",
      "epoch 120 / 200, step    0 / 100; loss 0.6802\n",
      "Evaluate (epoch 120) -- logp(x) = -0.647 +/- 0.020\n",
      "epoch 121 / 200, step    0 / 100; loss 0.7050\n",
      "Evaluate (epoch 121) -- logp(x) = -0.640 +/- 0.021\n",
      "epoch 122 / 200, step    0 / 100; loss 0.7632\n",
      "Evaluate (epoch 122) -- logp(x) = -0.646 +/- 0.020\n",
      "epoch 123 / 200, step    0 / 100; loss 0.6481\n",
      "Evaluate (epoch 123) -- logp(x) = -0.650 +/- 0.020\n",
      "epoch 124 / 200, step    0 / 100; loss 0.6816\n",
      "Evaluate (epoch 124) -- logp(x) = -0.644 +/- 0.021\n",
      "epoch 125 / 200, step    0 / 100; loss 0.5591\n",
      "Evaluate (epoch 125) -- logp(x) = -0.636 +/- 0.020\n",
      "epoch 126 / 200, step    0 / 100; loss 0.7448\n",
      "Evaluate (epoch 126) -- logp(x) = -0.672 +/- 0.020\n",
      "epoch 127 / 200, step    0 / 100; loss 0.6970\n",
      "Evaluate (epoch 127) -- logp(x) = -0.646 +/- 0.020\n",
      "epoch 128 / 200, step    0 / 100; loss 0.4863\n",
      "Evaluate (epoch 128) -- logp(x) = -0.661 +/- 0.020\n",
      "epoch 129 / 200, step    0 / 100; loss 0.6092\n",
      "Evaluate (epoch 129) -- logp(x) = -0.639 +/- 0.020\n",
      "epoch 130 / 200, step    0 / 100; loss 0.5532\n",
      "Evaluate (epoch 130) -- logp(x) = -0.642 +/- 0.020\n",
      "epoch 131 / 200, step    0 / 100; loss 0.4324\n",
      "Evaluate (epoch 131) -- logp(x) = -0.636 +/- 0.020\n",
      "epoch 132 / 200, step    0 / 100; loss 0.5269\n",
      "Evaluate (epoch 132) -- logp(x) = -0.666 +/- 0.021\n",
      "epoch 133 / 200, step    0 / 100; loss 0.4414\n",
      "Evaluate (epoch 133) -- logp(x) = -0.626 +/- 0.020\n",
      "epoch 134 / 200, step    0 / 100; loss 0.5026\n",
      "Evaluate (epoch 134) -- logp(x) = -0.621 +/- 0.020\n",
      "epoch 135 / 200, step    0 / 100; loss 0.6443\n",
      "Evaluate (epoch 135) -- logp(x) = -0.621 +/- 0.020\n",
      "epoch 136 / 200, step    0 / 100; loss 0.4535\n",
      "Evaluate (epoch 136) -- logp(x) = -0.627 +/- 0.019\n",
      "epoch 137 / 200, step    0 / 100; loss 0.6366\n",
      "Evaluate (epoch 137) -- logp(x) = -0.658 +/- 0.021\n",
      "epoch 138 / 200, step    0 / 100; loss 0.5439\n",
      "Evaluate (epoch 138) -- logp(x) = -0.637 +/- 0.021\n",
      "epoch 139 / 200, step    0 / 100; loss 0.6544\n",
      "Evaluate (epoch 139) -- logp(x) = -0.616 +/- 0.021\n",
      "epoch 140 / 200, step    0 / 100; loss 0.6716\n",
      "Evaluate (epoch 140) -- logp(x) = -0.615 +/- 0.020\n",
      "epoch 141 / 200, step    0 / 100; loss 0.6653\n",
      "Evaluate (epoch 141) -- logp(x) = -0.873 +/- 0.022\n",
      "epoch 142 / 200, step    0 / 100; loss 0.9732\n",
      "Evaluate (epoch 142) -- logp(x) = -0.747 +/- 0.022\n",
      "epoch 143 / 200, step    0 / 100; loss 0.8740\n",
      "Evaluate (epoch 143) -- logp(x) = -0.701 +/- 0.021\n",
      "epoch 144 / 200, step    0 / 100; loss 0.5183\n",
      "Evaluate (epoch 144) -- logp(x) = -0.684 +/- 0.021\n",
      "epoch 145 / 200, step    0 / 100; loss 0.7492\n",
      "Evaluate (epoch 145) -- logp(x) = -0.671 +/- 0.021\n",
      "epoch 146 / 200, step    0 / 100; loss 0.6115\n",
      "Evaluate (epoch 146) -- logp(x) = -0.663 +/- 0.021\n",
      "epoch 147 / 200, step    0 / 100; loss 0.8272\n",
      "Evaluate (epoch 147) -- logp(x) = -0.664 +/- 0.021\n",
      "epoch 148 / 200, step    0 / 100; loss 0.5237\n",
      "Evaluate (epoch 148) -- logp(x) = -0.654 +/- 0.021\n",
      "epoch 149 / 200, step    0 / 100; loss 0.7231\n",
      "Evaluate (epoch 149) -- logp(x) = -0.665 +/- 0.021\n",
      "epoch 150 / 200, step    0 / 100; loss 0.7268\n",
      "Evaluate (epoch 150) -- logp(x) = -0.654 +/- 0.021\n",
      "epoch 151 / 200, step    0 / 100; loss 0.6495\n",
      "Evaluate (epoch 151) -- logp(x) = -0.647 +/- 0.020\n",
      "epoch 152 / 200, step    0 / 100; loss 0.6479\n",
      "Evaluate (epoch 152) -- logp(x) = -0.642 +/- 0.021\n",
      "epoch 153 / 200, step    0 / 100; loss 0.8687\n",
      "Evaluate (epoch 153) -- logp(x) = -0.637 +/- 0.021\n",
      "epoch 154 / 200, step    0 / 100; loss 0.7144\n",
      "Evaluate (epoch 154) -- logp(x) = -0.658 +/- 0.021\n",
      "epoch 155 / 200, step    0 / 100; loss 0.6070\n",
      "Evaluate (epoch 155) -- logp(x) = -0.633 +/- 0.020\n",
      "epoch 156 / 200, step    0 / 100; loss 0.6446\n",
      "Evaluate (epoch 156) -- logp(x) = -0.629 +/- 0.020\n",
      "epoch 157 / 200, step    0 / 100; loss 0.6231\n",
      "Evaluate (epoch 157) -- logp(x) = -0.638 +/- 0.020\n",
      "epoch 158 / 200, step    0 / 100; loss 0.4502\n",
      "Evaluate (epoch 158) -- logp(x) = -0.622 +/- 0.021\n",
      "epoch 159 / 200, step    0 / 100; loss 0.6560\n",
      "Evaluate (epoch 159) -- logp(x) = -0.617 +/- 0.020\n",
      "epoch 160 / 200, step    0 / 100; loss 0.5330\n",
      "Evaluate (epoch 160) -- logp(x) = -0.613 +/- 0.021\n",
      "epoch 161 / 200, step    0 / 100; loss 0.6852\n",
      "Evaluate (epoch 161) -- logp(x) = -0.621 +/- 0.020\n",
      "epoch 162 / 200, step    0 / 100; loss 0.7674\n",
      "Evaluate (epoch 162) -- logp(x) = -0.608 +/- 0.020\n",
      "epoch 163 / 200, step    0 / 100; loss 0.5721\n",
      "Evaluate (epoch 163) -- logp(x) = -0.607 +/- 0.020\n",
      "epoch 164 / 200, step    0 / 100; loss 0.6533\n",
      "Evaluate (epoch 164) -- logp(x) = -0.696 +/- 0.020\n",
      "epoch 165 / 200, step    0 / 100; loss 0.5459\n",
      "Evaluate (epoch 165) -- logp(x) = -0.684 +/- 0.021\n",
      "epoch 166 / 200, step    0 / 100; loss 0.7458\n",
      "Evaluate (epoch 166) -- logp(x) = -0.656 +/- 0.020\n",
      "epoch 167 / 200, step    0 / 100; loss 0.5444\n",
      "Evaluate (epoch 167) -- logp(x) = -0.641 +/- 0.020\n",
      "epoch 168 / 200, step    0 / 100; loss 0.5481\n",
      "Evaluate (epoch 168) -- logp(x) = -0.630 +/- 0.020\n",
      "epoch 169 / 200, step    0 / 100; loss 0.7880\n",
      "Evaluate (epoch 169) -- logp(x) = -0.619 +/- 0.020\n",
      "epoch 170 / 200, step    0 / 100; loss 0.7247\n",
      "Evaluate (epoch 170) -- logp(x) = -0.610 +/- 0.020\n",
      "epoch 171 / 200, step    0 / 100; loss 0.6127\n",
      "Evaluate (epoch 171) -- logp(x) = -0.603 +/- 0.020\n",
      "epoch 172 / 200, step    0 / 100; loss 0.7403\n",
      "Evaluate (epoch 172) -- logp(x) = -0.596 +/- 0.020\n",
      "epoch 173 / 200, step    0 / 100; loss 0.5589\n",
      "Evaluate (epoch 173) -- logp(x) = -0.591 +/- 0.020\n",
      "epoch 174 / 200, step    0 / 100; loss 0.4946\n",
      "Evaluate (epoch 174) -- logp(x) = -0.585 +/- 0.020\n",
      "epoch 175 / 200, step    0 / 100; loss 0.5419\n",
      "Evaluate (epoch 175) -- logp(x) = -0.584 +/- 0.020\n",
      "epoch 176 / 200, step    0 / 100; loss 0.7528\n",
      "Evaluate (epoch 176) -- logp(x) = -0.583 +/- 0.020\n",
      "epoch 177 / 200, step    0 / 100; loss 0.6475\n",
      "Evaluate (epoch 177) -- logp(x) = -0.574 +/- 0.020\n",
      "epoch 178 / 200, step    0 / 100; loss 0.4373\n",
      "Evaluate (epoch 178) -- logp(x) = -0.568 +/- 0.020\n",
      "epoch 179 / 200, step    0 / 100; loss 0.7016\n",
      "Evaluate (epoch 179) -- logp(x) = -0.572 +/- 0.020\n",
      "epoch 180 / 200, step    0 / 100; loss 0.4843\n",
      "Evaluate (epoch 180) -- logp(x) = -0.562 +/- 0.020\n",
      "epoch 181 / 200, step    0 / 100; loss 0.5131\n",
      "Evaluate (epoch 181) -- logp(x) = -0.557 +/- 0.020\n",
      "epoch 182 / 200, step    0 / 100; loss 0.6338\n",
      "Evaluate (epoch 182) -- logp(x) = -0.561 +/- 0.020\n",
      "epoch 183 / 200, step    0 / 100; loss 0.6149\n",
      "Evaluate (epoch 183) -- logp(x) = -0.555 +/- 0.021\n",
      "epoch 184 / 200, step    0 / 100; loss 0.5143\n",
      "Evaluate (epoch 184) -- logp(x) = -0.548 +/- 0.020\n",
      "epoch 185 / 200, step    0 / 100; loss 0.4931\n",
      "Evaluate (epoch 185) -- logp(x) = -0.547 +/- 0.020\n",
      "epoch 186 / 200, step    0 / 100; loss 0.5727\n",
      "Evaluate (epoch 186) -- logp(x) = -0.545 +/- 0.020\n",
      "epoch 187 / 200, step    0 / 100; loss 0.5235\n",
      "Evaluate (epoch 187) -- logp(x) = -0.539 +/- 0.021\n",
      "epoch 188 / 200, step    0 / 100; loss 0.5546\n",
      "Evaluate (epoch 188) -- logp(x) = -0.556 +/- 0.024\n",
      "epoch 189 / 200, step    0 / 100; loss 0.6235\n",
      "Evaluate (epoch 189) -- logp(x) = -0.558 +/- 0.020\n",
      "epoch 190 / 200, step    0 / 100; loss 0.5874\n",
      "Evaluate (epoch 190) -- logp(x) = -0.534 +/- 0.020\n",
      "epoch 191 / 200, step    0 / 100; loss 0.5312\n",
      "Evaluate (epoch 191) -- logp(x) = -0.535 +/- 0.020\n",
      "epoch 192 / 200, step    0 / 100; loss 0.4767\n",
      "Evaluate (epoch 192) -- logp(x) = -0.525 +/- 0.020\n",
      "epoch 193 / 200, step    0 / 100; loss 0.5346\n",
      "Evaluate (epoch 193) -- logp(x) = -0.529 +/- 0.021\n",
      "epoch 194 / 200, step    0 / 100; loss 0.4745\n",
      "Evaluate (epoch 194) -- logp(x) = -0.555 +/- 0.020\n",
      "epoch 195 / 200, step    0 / 100; loss 0.5014\n",
      "Evaluate (epoch 195) -- logp(x) = -0.521 +/- 0.021\n",
      "epoch 196 / 200, step    0 / 100; loss 0.5112\n",
      "Evaluate (epoch 196) -- logp(x) = -0.515 +/- 0.021\n",
      "epoch 197 / 200, step    0 / 100; loss 0.3895\n",
      "Evaluate (epoch 197) -- logp(x) = -0.523 +/- 0.021\n",
      "epoch 198 / 200, step    0 / 100; loss 0.5735\n",
      "Evaluate (epoch 198) -- logp(x) = -0.525 +/- 0.021\n",
      "epoch 199 / 200, step    0 / 100; loss 0.4347\n",
      "Evaluate (epoch 199) -- logp(x) = -0.848 +/- 0.038\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'INVOLUTE',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': True,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 2,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 2,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'maf',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 50,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': True,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/maf',\n",
      " 'restore_file': './results/maf/best_model_checkpoint.pt',\n",
      " 'results_file': './results/maf\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 197,\n",
      " 'train': False}\n",
      "MAF(\n",
      "  (net): FlowSequential(\n",
      "    (0): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "!python normalizing_flows/maf.py --train --model maf --dataset CIRCLE --n_epochs 200 --no_batch_norm\n",
    "!python normalizing_flows/maf.py --generate --model maf --dataset CIRCLE --no_batch_norm --restore_file ./results/maf/best_model_checkpoint.pt\n",
    "!python normalizing_flows/maf.py --train --model maf --dataset TORUS --n_epochs 200 --no_batch_norm\n",
    "!python normalizing_flows/maf.py --generate --model maf --dataset TORUS --no_batch_norm --restore_file ./results/maf/best_model_checkpoint.pt\n",
    "!python normalizing_flows/maf.py --train --model maf --dataset INVOLUTE --n_epochs 200 --no_batch_norm\n",
    "!python normalizing_flows/maf.py --generate --model maf --dataset INVOLUTE --no_batch_norm --restore_file ./results/maf/best_model_checkpoint.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'CIRCLE',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': False,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 2,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 2,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'realnvp',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 200,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': True,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/realnvp',\n",
      " 'restore_file': None,\n",
      " 'results_file': './results/realnvp\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 0,\n",
      " 'train': True}\n",
      "RealNVP(\n",
      "  (net): FlowSequential(\n",
      "    (0): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch   0 / 200, step    0 / 100; loss 2.4201\n",
      "Evaluate (epoch 0) -- logp(x) = -2.008 +/- 0.002\n",
      "epoch   1 / 200, step    0 / 100; loss 2.0069\n",
      "Evaluate (epoch 1) -- logp(x) = -1.003 +/- 0.022\n",
      "epoch   2 / 200, step    0 / 100; loss 1.0791\n",
      "Evaluate (epoch 2) -- logp(x) = -0.686 +/- 0.025\n",
      "epoch   3 / 200, step    0 / 100; loss 0.6519\n",
      "Evaluate (epoch 3) -- logp(x) = -0.669 +/- 0.024\n",
      "epoch   4 / 200, step    0 / 100; loss 0.6964\n",
      "Evaluate (epoch 4) -- logp(x) = -0.446 +/- 0.025\n",
      "epoch   5 / 200, step    0 / 100; loss 0.5759\n",
      "Evaluate (epoch 5) -- logp(x) = -0.113 +/- 0.028\n",
      "epoch   6 / 200, step    0 / 100; loss 0.2261\n",
      "Evaluate (epoch 6) -- logp(x) = -0.055 +/- 0.027\n",
      "epoch   7 / 200, step    0 / 100; loss 0.0882\n",
      "Evaluate (epoch 7) -- logp(x) = 0.091 +/- 0.025\n",
      "epoch   8 / 200, step    0 / 100; loss -0.1083\n",
      "Evaluate (epoch 8) -- logp(x) = 0.138 +/- 0.025\n",
      "epoch   9 / 200, step    0 / 100; loss -0.1280\n",
      "Evaluate (epoch 9) -- logp(x) = 0.245 +/- 0.024\n",
      "epoch  10 / 200, step    0 / 100; loss -0.3026\n",
      "Evaluate (epoch 10) -- logp(x) = 0.214 +/- 0.023\n",
      "epoch  11 / 200, step    0 / 100; loss -0.3589\n",
      "Evaluate (epoch 11) -- logp(x) = 0.259 +/- 0.022\n",
      "epoch  12 / 200, step    0 / 100; loss -0.1919\n",
      "Evaluate (epoch 12) -- logp(x) = 0.378 +/- 0.022\n",
      "epoch  13 / 200, step    0 / 100; loss -0.3017\n",
      "Evaluate (epoch 13) -- logp(x) = 0.395 +/- 0.024\n",
      "epoch  14 / 200, step    0 / 100; loss -0.4979\n",
      "Evaluate (epoch 14) -- logp(x) = 0.402 +/- 0.021\n",
      "epoch  15 / 200, step    0 / 100; loss -0.3882\n",
      "Evaluate (epoch 15) -- logp(x) = 0.455 +/- 0.023\n",
      "epoch  16 / 200, step    0 / 100; loss -0.4875\n",
      "Evaluate (epoch 16) -- logp(x) = 0.506 +/- 0.022\n",
      "epoch  17 / 200, step    0 / 100; loss -0.4878\n",
      "Evaluate (epoch 17) -- logp(x) = 0.552 +/- 0.021\n",
      "epoch  18 / 200, step    0 / 100; loss -0.5118\n",
      "Evaluate (epoch 18) -- logp(x) = 0.546 +/- 0.022\n",
      "epoch  19 / 200, step    0 / 100; loss -0.5848\n",
      "Evaluate (epoch 19) -- logp(x) = 0.554 +/- 0.021\n",
      "epoch  20 / 200, step    0 / 100; loss -0.3864\n",
      "Evaluate (epoch 20) -- logp(x) = 0.549 +/- 0.021\n",
      "epoch  21 / 200, step    0 / 100; loss -0.5714\n",
      "Evaluate (epoch 21) -- logp(x) = 0.200 +/- 0.022\n",
      "epoch  22 / 200, step    0 / 100; loss -0.2614\n",
      "Evaluate (epoch 22) -- logp(x) = 0.454 +/- 0.020\n",
      "epoch  23 / 200, step    0 / 100; loss -0.3480\n",
      "Evaluate (epoch 23) -- logp(x) = 0.613 +/- 0.021\n",
      "epoch  24 / 200, step    0 / 100; loss -0.5249\n",
      "Evaluate (epoch 24) -- logp(x) = 0.618 +/- 0.021\n",
      "epoch  25 / 200, step    0 / 100; loss -0.5624\n",
      "Evaluate (epoch 25) -- logp(x) = 0.328 +/- 0.024\n",
      "epoch  26 / 200, step    0 / 100; loss -0.3275\n",
      "Evaluate (epoch 26) -- logp(x) = 0.626 +/- 0.021\n",
      "epoch  27 / 200, step    0 / 100; loss -0.6007\n",
      "Evaluate (epoch 27) -- logp(x) = 0.626 +/- 0.023\n",
      "epoch  28 / 200, step    0 / 100; loss -0.4626\n",
      "Evaluate (epoch 28) -- logp(x) = 0.563 +/- 0.025\n",
      "epoch  29 / 200, step    0 / 100; loss -0.4726\n",
      "Evaluate (epoch 29) -- logp(x) = 0.406 +/- 0.021\n",
      "epoch  30 / 200, step    0 / 100; loss -0.3075\n",
      "Evaluate (epoch 30) -- logp(x) = 0.685 +/- 0.021\n",
      "epoch  31 / 200, step    0 / 100; loss -0.6502\n",
      "Evaluate (epoch 31) -- logp(x) = 0.611 +/- 0.023\n",
      "epoch  32 / 200, step    0 / 100; loss -0.6155\n",
      "Evaluate (epoch 32) -- logp(x) = 0.668 +/- 0.021\n",
      "epoch  33 / 200, step    0 / 100; loss -0.6036\n",
      "Evaluate (epoch 33) -- logp(x) = 0.504 +/- 0.021\n",
      "epoch  34 / 200, step    0 / 100; loss -0.6321\n",
      "Evaluate (epoch 34) -- logp(x) = 0.607 +/- 0.022\n",
      "epoch  35 / 200, step    0 / 100; loss -0.4471\n",
      "Evaluate (epoch 35) -- logp(x) = 0.607 +/- 0.024\n",
      "epoch  36 / 200, step    0 / 100; loss -0.3397\n",
      "Evaluate (epoch 36) -- logp(x) = 0.673 +/- 0.021\n",
      "epoch  37 / 200, step    0 / 100; loss -0.7856\n",
      "Evaluate (epoch 37) -- logp(x) = 0.691 +/- 0.020\n",
      "epoch  38 / 200, step    0 / 100; loss -0.6546\n",
      "Evaluate (epoch 38) -- logp(x) = 0.657 +/- 0.022\n",
      "epoch  39 / 200, step    0 / 100; loss -0.6617\n",
      "Evaluate (epoch 39) -- logp(x) = 0.668 +/- 0.024\n",
      "epoch  40 / 200, step    0 / 100; loss -0.4810\n",
      "Evaluate (epoch 40) -- logp(x) = 0.582 +/- 0.023\n",
      "epoch  41 / 200, step    0 / 100; loss -0.5026\n",
      "Evaluate (epoch 41) -- logp(x) = 0.710 +/- 0.021\n",
      "epoch  42 / 200, step    0 / 100; loss -0.7686\n",
      "Evaluate (epoch 42) -- logp(x) = 0.597 +/- 0.021\n",
      "epoch  43 / 200, step    0 / 100; loss -0.6519\n",
      "Evaluate (epoch 43) -- logp(x) = 0.697 +/- 0.021\n",
      "epoch  44 / 200, step    0 / 100; loss -0.8383\n",
      "Evaluate (epoch 44) -- logp(x) = 0.718 +/- 0.019\n",
      "epoch  45 / 200, step    0 / 100; loss -0.5388\n",
      "Evaluate (epoch 45) -- logp(x) = 0.625 +/- 0.021\n",
      "epoch  46 / 200, step    0 / 100; loss -0.6642\n",
      "Evaluate (epoch 46) -- logp(x) = 0.678 +/- 0.020\n",
      "epoch  47 / 200, step    0 / 100; loss -0.8249\n",
      "Evaluate (epoch 47) -- logp(x) = 0.540 +/- 0.025\n",
      "epoch  48 / 200, step    0 / 100; loss -0.5641\n",
      "Evaluate (epoch 48) -- logp(x) = 0.739 +/- 0.020\n",
      "epoch  49 / 200, step    0 / 100; loss -0.6341\n",
      "Evaluate (epoch 49) -- logp(x) = 0.737 +/- 0.021\n",
      "epoch  50 / 200, step    0 / 100; loss -0.7846\n",
      "Evaluate (epoch 50) -- logp(x) = 0.721 +/- 0.021\n",
      "epoch  51 / 200, step    0 / 100; loss -0.7184\n",
      "Evaluate (epoch 51) -- logp(x) = 0.716 +/- 0.021\n",
      "epoch  52 / 200, step    0 / 100; loss -0.6308\n",
      "Evaluate (epoch 52) -- logp(x) = 0.682 +/- 0.020\n",
      "epoch  53 / 200, step    0 / 100; loss -0.5580\n",
      "Evaluate (epoch 53) -- logp(x) = 0.761 +/- 0.020\n",
      "epoch  54 / 200, step    0 / 100; loss -0.6434\n",
      "Evaluate (epoch 54) -- logp(x) = 0.553 +/- 0.026\n",
      "epoch  55 / 200, step    0 / 100; loss -0.5171\n",
      "Evaluate (epoch 55) -- logp(x) = 0.745 +/- 0.021\n",
      "epoch  56 / 200, step    0 / 100; loss -0.6846\n",
      "Evaluate (epoch 56) -- logp(x) = 0.670 +/- 0.025\n",
      "epoch  57 / 200, step    0 / 100; loss -0.5120\n",
      "Evaluate (epoch 57) -- logp(x) = 0.724 +/- 0.020\n",
      "epoch  58 / 200, step    0 / 100; loss -0.7727\n",
      "Evaluate (epoch 58) -- logp(x) = 0.779 +/- 0.021\n",
      "epoch  59 / 200, step    0 / 100; loss -0.7586\n",
      "Evaluate (epoch 59) -- logp(x) = 0.479 +/- 0.022\n",
      "epoch  60 / 200, step    0 / 100; loss -0.5299\n",
      "Evaluate (epoch 60) -- logp(x) = 0.826 +/- 0.021\n",
      "epoch  61 / 200, step    0 / 100; loss -0.7974\n",
      "Evaluate (epoch 61) -- logp(x) = 0.686 +/- 0.024\n",
      "epoch  62 / 200, step    0 / 100; loss -0.8203\n",
      "Evaluate (epoch 62) -- logp(x) = 0.657 +/- 0.027\n",
      "epoch  63 / 200, step    0 / 100; loss -0.8122\n",
      "Evaluate (epoch 63) -- logp(x) = 0.770 +/- 0.026\n",
      "epoch  64 / 200, step    0 / 100; loss -0.7715\n",
      "Evaluate (epoch 64) -- logp(x) = 0.487 +/- 0.029\n",
      "epoch  65 / 200, step    0 / 100; loss -0.7268\n",
      "Evaluate (epoch 65) -- logp(x) = 0.717 +/- 0.020\n",
      "epoch  66 / 200, step    0 / 100; loss -0.7299\n",
      "Evaluate (epoch 66) -- logp(x) = 0.742 +/- 0.022\n",
      "epoch  67 / 200, step    0 / 100; loss -0.7722\n",
      "Evaluate (epoch 67) -- logp(x) = 0.809 +/- 0.021\n",
      "epoch  68 / 200, step    0 / 100; loss -0.8996\n",
      "Evaluate (epoch 68) -- logp(x) = 0.762 +/- 0.019\n",
      "epoch  69 / 200, step    0 / 100; loss -0.7482\n",
      "Evaluate (epoch 69) -- logp(x) = 0.798 +/- 0.020\n",
      "epoch  70 / 200, step    0 / 100; loss -0.7593\n",
      "Evaluate (epoch 70) -- logp(x) = 0.724 +/- 0.023\n",
      "epoch  71 / 200, step    0 / 100; loss -0.6046\n",
      "Evaluate (epoch 71) -- logp(x) = 0.781 +/- 0.022\n",
      "epoch  72 / 200, step    0 / 100; loss -0.7658\n",
      "Evaluate (epoch 72) -- logp(x) = 0.707 +/- 0.024\n",
      "epoch  73 / 200, step    0 / 100; loss -0.7153\n",
      "Evaluate (epoch 73) -- logp(x) = 0.723 +/- 0.020\n",
      "epoch  74 / 200, step    0 / 100; loss -0.8383\n",
      "Evaluate (epoch 74) -- logp(x) = 0.742 +/- 0.021\n",
      "epoch  75 / 200, step    0 / 100; loss -0.7197\n",
      "Evaluate (epoch 75) -- logp(x) = 0.779 +/- 0.023\n",
      "epoch  76 / 200, step    0 / 100; loss -0.8544\n",
      "Evaluate (epoch 76) -- logp(x) = 0.635 +/- 0.025\n",
      "epoch  77 / 200, step    0 / 100; loss -0.7670\n",
      "Evaluate (epoch 77) -- logp(x) = 0.687 +/- 0.020\n",
      "epoch  78 / 200, step    0 / 100; loss -0.7020\n",
      "Evaluate (epoch 78) -- logp(x) = 0.807 +/- 0.020\n",
      "epoch  79 / 200, step    0 / 100; loss -0.9052\n",
      "Evaluate (epoch 79) -- logp(x) = 0.775 +/- 0.021\n",
      "epoch  80 / 200, step    0 / 100; loss -0.8248\n",
      "Evaluate (epoch 80) -- logp(x) = 0.851 +/- 0.018\n",
      "epoch  81 / 200, step    0 / 100; loss -0.8737\n",
      "Evaluate (epoch 81) -- logp(x) = 0.858 +/- 0.020\n",
      "epoch  82 / 200, step    0 / 100; loss -0.8348\n",
      "Evaluate (epoch 82) -- logp(x) = 0.831 +/- 0.019\n",
      "epoch  83 / 200, step    0 / 100; loss -0.8157\n",
      "Evaluate (epoch 83) -- logp(x) = 0.682 +/- 0.025\n",
      "epoch  84 / 200, step    0 / 100; loss -0.4822\n",
      "Evaluate (epoch 84) -- logp(x) = 0.792 +/- 0.023\n",
      "epoch  85 / 200, step    0 / 100; loss -0.8147\n",
      "Evaluate (epoch 85) -- logp(x) = 0.762 +/- 0.020\n",
      "epoch  86 / 200, step    0 / 100; loss -0.7879\n",
      "Evaluate (epoch 86) -- logp(x) = 0.820 +/- 0.019\n",
      "epoch  87 / 200, step    0 / 100; loss -0.6992\n",
      "Evaluate (epoch 87) -- logp(x) = 0.882 +/- 0.020\n",
      "epoch  88 / 200, step    0 / 100; loss -0.8718\n",
      "Evaluate (epoch 88) -- logp(x) = 0.812 +/- 0.019\n",
      "epoch  89 / 200, step    0 / 100; loss -0.7403\n",
      "Evaluate (epoch 89) -- logp(x) = 0.908 +/- 0.020\n",
      "epoch  90 / 200, step    0 / 100; loss -0.9953\n",
      "Evaluate (epoch 90) -- logp(x) = 0.725 +/- 0.022\n",
      "epoch  91 / 200, step    0 / 100; loss -0.7971\n",
      "Evaluate (epoch 91) -- logp(x) = 0.788 +/- 0.018\n",
      "epoch  92 / 200, step    0 / 100; loss -0.8572\n",
      "Evaluate (epoch 92) -- logp(x) = 0.883 +/- 0.020\n",
      "epoch  93 / 200, step    0 / 100; loss -0.8342\n",
      "Evaluate (epoch 93) -- logp(x) = 0.902 +/- 0.020\n",
      "epoch  94 / 200, step    0 / 100; loss -0.8729\n",
      "Evaluate (epoch 94) -- logp(x) = 0.898 +/- 0.020\n",
      "epoch  95 / 200, step    0 / 100; loss -0.8410\n",
      "Evaluate (epoch 95) -- logp(x) = 0.740 +/- 0.022\n",
      "epoch  96 / 200, step    0 / 100; loss -0.8457\n",
      "Evaluate (epoch 96) -- logp(x) = 0.718 +/- 0.022\n",
      "epoch  97 / 200, step    0 / 100; loss -0.7547\n",
      "Evaluate (epoch 97) -- logp(x) = 0.920 +/- 0.020\n",
      "epoch  98 / 200, step    0 / 100; loss -0.8611\n",
      "Evaluate (epoch 98) -- logp(x) = 0.827 +/- 0.019\n",
      "epoch  99 / 200, step    0 / 100; loss -0.7903\n",
      "Evaluate (epoch 99) -- logp(x) = 0.826 +/- 0.022\n",
      "epoch 100 / 200, step    0 / 100; loss -0.9687\n",
      "Evaluate (epoch 100) -- logp(x) = 0.761 +/- 0.024\n",
      "epoch 101 / 200, step    0 / 100; loss -0.8801\n",
      "Evaluate (epoch 101) -- logp(x) = 0.849 +/- 0.021\n",
      "epoch 102 / 200, step    0 / 100; loss -0.8649\n",
      "Evaluate (epoch 102) -- logp(x) = 0.583 +/- 0.021\n",
      "epoch 103 / 200, step    0 / 100; loss -0.6198\n",
      "Evaluate (epoch 103) -- logp(x) = 0.843 +/- 0.022\n",
      "epoch 104 / 200, step    0 / 100; loss -0.9402\n",
      "Evaluate (epoch 104) -- logp(x) = 0.886 +/- 0.020\n",
      "epoch 105 / 200, step    0 / 100; loss -0.7528\n",
      "Evaluate (epoch 105) -- logp(x) = 0.793 +/- 0.019\n",
      "epoch 106 / 200, step    0 / 100; loss -0.7668\n",
      "Evaluate (epoch 106) -- logp(x) = 0.572 +/- 0.025\n",
      "epoch 107 / 200, step    0 / 100; loss -0.5629\n",
      "Evaluate (epoch 107) -- logp(x) = 0.814 +/- 0.020\n",
      "epoch 108 / 200, step    0 / 100; loss -0.8855\n",
      "Evaluate (epoch 108) -- logp(x) = 0.839 +/- 0.019\n",
      "epoch 109 / 200, step    0 / 100; loss -0.7813\n",
      "Evaluate (epoch 109) -- logp(x) = 0.876 +/- 0.020\n",
      "epoch 110 / 200, step    0 / 100; loss -0.8635\n",
      "Evaluate (epoch 110) -- logp(x) = 0.874 +/- 0.021\n",
      "epoch 111 / 200, step    0 / 100; loss -0.9605\n",
      "Evaluate (epoch 111) -- logp(x) = 0.806 +/- 0.022\n",
      "epoch 112 / 200, step    0 / 100; loss -0.7779\n",
      "Evaluate (epoch 112) -- logp(x) = 0.853 +/- 0.021\n",
      "epoch 113 / 200, step    0 / 100; loss -0.9383\n",
      "Evaluate (epoch 113) -- logp(x) = 0.888 +/- 0.021\n",
      "epoch 114 / 200, step    0 / 100; loss -1.0208\n",
      "Evaluate (epoch 114) -- logp(x) = 0.850 +/- 0.019\n",
      "epoch 115 / 200, step    0 / 100; loss -0.8561\n",
      "Evaluate (epoch 115) -- logp(x) = 0.813 +/- 0.020\n",
      "epoch 116 / 200, step    0 / 100; loss -0.7614\n",
      "Evaluate (epoch 116) -- logp(x) = 0.827 +/- 0.021\n",
      "epoch 117 / 200, step    0 / 100; loss -1.0296\n",
      "Evaluate (epoch 117) -- logp(x) = 0.887 +/- 0.021\n",
      "epoch 118 / 200, step    0 / 100; loss -1.0034\n",
      "Evaluate (epoch 118) -- logp(x) = 0.906 +/- 0.020\n",
      "epoch 119 / 200, step    0 / 100; loss -0.9399\n",
      "Evaluate (epoch 119) -- logp(x) = 0.831 +/- 0.020\n",
      "epoch 120 / 200, step    0 / 100; loss -0.6358\n",
      "Evaluate (epoch 120) -- logp(x) = 0.770 +/- 0.024\n",
      "epoch 121 / 200, step    0 / 100; loss -0.6911\n",
      "Evaluate (epoch 121) -- logp(x) = 0.696 +/- 0.019\n",
      "epoch 122 / 200, step    0 / 100; loss -0.7207\n",
      "Evaluate (epoch 122) -- logp(x) = 0.891 +/- 0.021\n",
      "epoch 123 / 200, step    0 / 100; loss -0.9679\n",
      "Evaluate (epoch 123) -- logp(x) = 0.861 +/- 0.022\n",
      "epoch 124 / 200, step    0 / 100; loss -0.8276\n",
      "Evaluate (epoch 124) -- logp(x) = 0.915 +/- 0.020\n",
      "epoch 125 / 200, step    0 / 100; loss -0.8261\n",
      "Evaluate (epoch 125) -- logp(x) = 0.890 +/- 0.020\n",
      "epoch 126 / 200, step    0 / 100; loss -0.7723\n",
      "Evaluate (epoch 126) -- logp(x) = 0.934 +/- 0.019\n",
      "epoch 127 / 200, step    0 / 100; loss -1.0389\n",
      "Evaluate (epoch 127) -- logp(x) = 0.853 +/- 0.018\n",
      "epoch 128 / 200, step    0 / 100; loss -0.8618\n",
      "Evaluate (epoch 128) -- logp(x) = 0.908 +/- 0.021\n",
      "epoch 129 / 200, step    0 / 100; loss -0.7734\n",
      "Evaluate (epoch 129) -- logp(x) = 0.756 +/- 0.020\n",
      "epoch 130 / 200, step    0 / 100; loss -0.7986\n",
      "Evaluate (epoch 130) -- logp(x) = 0.945 +/- 0.020\n",
      "epoch 131 / 200, step    0 / 100; loss -0.9928\n",
      "Evaluate (epoch 131) -- logp(x) = 0.923 +/- 0.020\n",
      "epoch 132 / 200, step    0 / 100; loss -0.8957\n",
      "Evaluate (epoch 132) -- logp(x) = 0.915 +/- 0.019\n",
      "epoch 133 / 200, step    0 / 100; loss -0.8895\n",
      "Evaluate (epoch 133) -- logp(x) = 0.811 +/- 0.024\n",
      "epoch 134 / 200, step    0 / 100; loss -0.8563\n",
      "Evaluate (epoch 134) -- logp(x) = 0.945 +/- 0.019\n",
      "epoch 135 / 200, step    0 / 100; loss -1.0216\n",
      "Evaluate (epoch 135) -- logp(x) = 0.929 +/- 0.019\n",
      "epoch 136 / 200, step    0 / 100; loss -0.8025\n",
      "Evaluate (epoch 136) -- logp(x) = 0.861 +/- 0.023\n",
      "epoch 137 / 200, step    0 / 100; loss -0.8331\n",
      "Evaluate (epoch 137) -- logp(x) = 0.925 +/- 0.020\n",
      "epoch 138 / 200, step    0 / 100; loss -0.9310\n",
      "Evaluate (epoch 138) -- logp(x) = 0.841 +/- 0.023\n",
      "epoch 139 / 200, step    0 / 100; loss -0.8041\n",
      "Evaluate (epoch 139) -- logp(x) = 0.961 +/- 0.019\n",
      "epoch 140 / 200, step    0 / 100; loss -0.9642\n",
      "Evaluate (epoch 140) -- logp(x) = 0.971 +/- 0.019\n",
      "epoch 141 / 200, step    0 / 100; loss -0.9549\n",
      "Evaluate (epoch 141) -- logp(x) = 0.897 +/- 0.020\n",
      "epoch 142 / 200, step    0 / 100; loss -1.0087\n",
      "Evaluate (epoch 142) -- logp(x) = 0.820 +/- 0.025\n",
      "epoch 143 / 200, step    0 / 100; loss -0.8034\n",
      "Evaluate (epoch 143) -- logp(x) = 0.898 +/- 0.022\n",
      "epoch 144 / 200, step    0 / 100; loss -0.9156\n",
      "Evaluate (epoch 144) -- logp(x) = 0.947 +/- 0.020\n",
      "epoch 145 / 200, step    0 / 100; loss -0.8240\n",
      "Evaluate (epoch 145) -- logp(x) = 0.967 +/- 0.018\n",
      "epoch 146 / 200, step    0 / 100; loss -0.8790\n",
      "Evaluate (epoch 146) -- logp(x) = 0.947 +/- 0.019\n",
      "epoch 147 / 200, step    0 / 100; loss -1.0566\n",
      "Evaluate (epoch 147) -- logp(x) = 0.971 +/- 0.020\n",
      "epoch 148 / 200, step    0 / 100; loss -0.8953\n",
      "Evaluate (epoch 148) -- logp(x) = 0.893 +/- 0.022\n",
      "epoch 149 / 200, step    0 / 100; loss -0.8734\n",
      "Evaluate (epoch 149) -- logp(x) = 0.936 +/- 0.019\n",
      "epoch 150 / 200, step    0 / 100; loss -0.9833\n",
      "Evaluate (epoch 150) -- logp(x) = 0.898 +/- 0.020\n",
      "epoch 151 / 200, step    0 / 100; loss -0.9240\n",
      "Evaluate (epoch 151) -- logp(x) = 0.921 +/- 0.019\n",
      "epoch 152 / 200, step    0 / 100; loss -0.9070\n",
      "Evaluate (epoch 152) -- logp(x) = 0.929 +/- 0.020\n",
      "epoch 153 / 200, step    0 / 100; loss -0.9480\n",
      "Evaluate (epoch 153) -- logp(x) = 0.963 +/- 0.021\n",
      "epoch 154 / 200, step    0 / 100; loss -0.9643\n",
      "Evaluate (epoch 154) -- logp(x) = 0.996 +/- 0.018\n",
      "epoch 155 / 200, step    0 / 100; loss -1.0896\n",
      "Evaluate (epoch 155) -- logp(x) = 0.917 +/- 0.020\n",
      "epoch 156 / 200, step    0 / 100; loss -1.0381\n",
      "Evaluate (epoch 156) -- logp(x) = 0.937 +/- 0.019\n",
      "epoch 157 / 200, step    0 / 100; loss -0.7582\n",
      "Evaluate (epoch 157) -- logp(x) = 0.971 +/- 0.019\n",
      "epoch 158 / 200, step    0 / 100; loss -0.9504\n",
      "Evaluate (epoch 158) -- logp(x) = 0.813 +/- 0.022\n",
      "epoch 159 / 200, step    0 / 100; loss -0.7224\n",
      "Evaluate (epoch 159) -- logp(x) = 0.952 +/- 0.020\n",
      "epoch 160 / 200, step    0 / 100; loss -0.9452\n",
      "Evaluate (epoch 160) -- logp(x) = 1.007 +/- 0.018\n",
      "epoch 161 / 200, step    0 / 100; loss -1.0077\n",
      "Evaluate (epoch 161) -- logp(x) = 0.973 +/- 0.019\n",
      "epoch 162 / 200, step    0 / 100; loss -0.9154\n",
      "Evaluate (epoch 162) -- logp(x) = 0.933 +/- 0.019\n",
      "epoch 163 / 200, step    0 / 100; loss -0.9423\n",
      "Evaluate (epoch 163) -- logp(x) = 0.805 +/- 0.021\n",
      "epoch 164 / 200, step    0 / 100; loss -0.8591\n",
      "Evaluate (epoch 164) -- logp(x) = 0.990 +/- 0.020\n",
      "epoch 165 / 200, step    0 / 100; loss -1.0924\n",
      "Evaluate (epoch 165) -- logp(x) = 0.967 +/- 0.019\n",
      "epoch 166 / 200, step    0 / 100; loss -1.1188\n",
      "Evaluate (epoch 166) -- logp(x) = 0.870 +/- 0.023\n",
      "epoch 167 / 200, step    0 / 100; loss -0.9828\n",
      "Evaluate (epoch 167) -- logp(x) = 0.831 +/- 0.024\n",
      "epoch 168 / 200, step    0 / 100; loss -0.9446\n",
      "Evaluate (epoch 168) -- logp(x) = 0.886 +/- 0.018\n",
      "epoch 169 / 200, step    0 / 100; loss -0.9928\n",
      "Evaluate (epoch 169) -- logp(x) = 0.921 +/- 0.019\n",
      "epoch 170 / 200, step    0 / 100; loss -0.9899\n",
      "Evaluate (epoch 170) -- logp(x) = 1.000 +/- 0.019\n",
      "epoch 171 / 200, step    0 / 100; loss -0.9210\n",
      "Evaluate (epoch 171) -- logp(x) = 0.964 +/- 0.020\n",
      "epoch 172 / 200, step    0 / 100; loss -1.0073\n",
      "Evaluate (epoch 172) -- logp(x) = 1.003 +/- 0.020\n",
      "epoch 173 / 200, step    0 / 100; loss -1.0346\n",
      "Evaluate (epoch 173) -- logp(x) = 0.951 +/- 0.022\n",
      "epoch 174 / 200, step    0 / 100; loss -0.7717\n",
      "Evaluate (epoch 174) -- logp(x) = 0.962 +/- 0.020\n",
      "epoch 175 / 200, step    0 / 100; loss -0.8957\n",
      "Evaluate (epoch 175) -- logp(x) = 1.024 +/- 0.018\n",
      "epoch 176 / 200, step    0 / 100; loss -0.9146\n",
      "Evaluate (epoch 176) -- logp(x) = 0.917 +/- 0.019\n",
      "epoch 177 / 200, step    0 / 100; loss -0.7944\n",
      "Evaluate (epoch 177) -- logp(x) = 0.983 +/- 0.019\n",
      "epoch 178 / 200, step    0 / 100; loss -1.0234\n",
      "Evaluate (epoch 178) -- logp(x) = 0.981 +/- 0.020\n",
      "epoch 179 / 200, step    0 / 100; loss -0.9178\n",
      "Evaluate (epoch 179) -- logp(x) = 1.018 +/- 0.017\n",
      "epoch 180 / 200, step    0 / 100; loss -1.0045\n",
      "Evaluate (epoch 180) -- logp(x) = 0.945 +/- 0.018\n",
      "epoch 181 / 200, step    0 / 100; loss -0.8879\n",
      "Evaluate (epoch 181) -- logp(x) = 0.925 +/- 0.020\n",
      "epoch 182 / 200, step    0 / 100; loss -0.8342\n",
      "Evaluate (epoch 182) -- logp(x) = 0.929 +/- 0.020\n",
      "epoch 183 / 200, step    0 / 100; loss -0.8634\n",
      "Evaluate (epoch 183) -- logp(x) = 0.927 +/- 0.022\n",
      "epoch 184 / 200, step    0 / 100; loss -1.1621\n",
      "Evaluate (epoch 184) -- logp(x) = 0.942 +/- 0.021\n",
      "epoch 185 / 200, step    0 / 100; loss -0.8935\n",
      "Evaluate (epoch 185) -- logp(x) = 0.902 +/- 0.032\n",
      "epoch 186 / 200, step    0 / 100; loss -0.8380\n",
      "Evaluate (epoch 186) -- logp(x) = 0.918 +/- 0.022\n",
      "epoch 187 / 200, step    0 / 100; loss -0.7019\n",
      "Evaluate (epoch 187) -- logp(x) = 0.942 +/- 0.020\n",
      "epoch 188 / 200, step    0 / 100; loss -0.9997\n",
      "Evaluate (epoch 188) -- logp(x) = 1.009 +/- 0.019\n",
      "epoch 189 / 200, step    0 / 100; loss -0.8954\n",
      "Evaluate (epoch 189) -- logp(x) = 0.816 +/- 0.025\n",
      "epoch 190 / 200, step    0 / 100; loss -1.0572\n",
      "Evaluate (epoch 190) -- logp(x) = 0.824 +/- 0.023\n",
      "epoch 191 / 200, step    0 / 100; loss -0.7305\n",
      "Evaluate (epoch 191) -- logp(x) = 0.926 +/- 0.023\n",
      "epoch 192 / 200, step    0 / 100; loss -1.0235\n",
      "Evaluate (epoch 192) -- logp(x) = 1.031 +/- 0.018\n",
      "epoch 193 / 200, step    0 / 100; loss -1.0414\n",
      "Evaluate (epoch 193) -- logp(x) = 0.997 +/- 0.019\n",
      "epoch 194 / 200, step    0 / 100; loss -0.9019\n",
      "Evaluate (epoch 194) -- logp(x) = 1.016 +/- 0.018\n",
      "epoch 195 / 200, step    0 / 100; loss -1.1191\n",
      "Evaluate (epoch 195) -- logp(x) = 0.988 +/- 0.021\n",
      "epoch 196 / 200, step    0 / 100; loss -1.0363\n",
      "Evaluate (epoch 196) -- logp(x) = 0.881 +/- 0.026\n",
      "epoch 197 / 200, step    0 / 100; loss -0.9140\n",
      "Evaluate (epoch 197) -- logp(x) = 0.998 +/- 0.018\n",
      "epoch 198 / 200, step    0 / 100; loss -0.8964\n",
      "Evaluate (epoch 198) -- logp(x) = 0.965 +/- 0.020\n",
      "epoch 199 / 200, step    0 / 100; loss -0.8913\n",
      "Evaluate (epoch 199) -- logp(x) = 1.025 +/- 0.022\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'CIRCLE',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': True,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 2,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 2,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'realnvp',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 50,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': True,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/realnvp',\n",
      " 'restore_file': './results/realnvp/best_model_checkpoint.pt',\n",
      " 'results_file': './results/realnvp\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 193,\n",
      " 'train': False}\n",
      "RealNVP(\n",
      "  (net): FlowSequential(\n",
      "    (0): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'TORUS',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': False,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 3,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 3,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'realnvp',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 200,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': True,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/realnvp',\n",
      " 'restore_file': None,\n",
      " 'results_file': './results/realnvp\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 0,\n",
      " 'train': True}\n",
      "RealNVP(\n",
      "  (net): FlowSequential(\n",
      "    (0): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch   0 / 200, step    0 / 100; loss 3.6304\n",
      "Evaluate (epoch 0) -- logp(x) = -2.653 +/- 0.014\n",
      "epoch   1 / 200, step    0 / 100; loss 2.6617\n",
      "Evaluate (epoch 1) -- logp(x) = -2.629 +/- 0.014\n",
      "epoch   2 / 200, step    0 / 100; loss 2.6646\n",
      "Evaluate (epoch 2) -- logp(x) = -2.594 +/- 0.013\n",
      "epoch   3 / 200, step    0 / 100; loss 2.5243\n",
      "Evaluate (epoch 3) -- logp(x) = -2.546 +/- 0.013\n",
      "epoch   4 / 200, step    0 / 100; loss 2.5241\n",
      "Evaluate (epoch 4) -- logp(x) = -2.490 +/- 0.012\n",
      "epoch   5 / 200, step    0 / 100; loss 2.4352\n",
      "Evaluate (epoch 5) -- logp(x) = -2.423 +/- 0.014\n",
      "epoch   6 / 200, step    0 / 100; loss 2.4255\n",
      "Evaluate (epoch 6) -- logp(x) = -2.330 +/- 0.013\n",
      "epoch   7 / 200, step    0 / 100; loss 2.1673\n",
      "Evaluate (epoch 7) -- logp(x) = -2.270 +/- 0.012\n",
      "epoch   8 / 200, step    0 / 100; loss 2.3738\n",
      "Evaluate (epoch 8) -- logp(x) = -2.225 +/- 0.012\n",
      "epoch   9 / 200, step    0 / 100; loss 2.2420\n",
      "Evaluate (epoch 9) -- logp(x) = -2.185 +/- 0.013\n",
      "epoch  10 / 200, step    0 / 100; loss 2.1091\n",
      "Evaluate (epoch 10) -- logp(x) = -2.183 +/- 0.013\n",
      "epoch  11 / 200, step    0 / 100; loss 2.2408\n",
      "Evaluate (epoch 11) -- logp(x) = -2.103 +/- 0.014\n",
      "epoch  12 / 200, step    0 / 100; loss 2.1026\n",
      "Evaluate (epoch 12) -- logp(x) = -2.093 +/- 0.014\n",
      "epoch  13 / 200, step    0 / 100; loss 2.0480\n",
      "Evaluate (epoch 13) -- logp(x) = -2.039 +/- 0.013\n",
      "epoch  14 / 200, step    0 / 100; loss 2.0559\n",
      "Evaluate (epoch 14) -- logp(x) = -1.999 +/- 0.015\n",
      "epoch  15 / 200, step    0 / 100; loss 2.0670\n",
      "Evaluate (epoch 15) -- logp(x) = -1.950 +/- 0.015\n",
      "epoch  16 / 200, step    0 / 100; loss 1.9325\n",
      "Evaluate (epoch 16) -- logp(x) = -1.946 +/- 0.015\n",
      "epoch  17 / 200, step    0 / 100; loss 1.8746\n",
      "Evaluate (epoch 17) -- logp(x) = -1.943 +/- 0.017\n",
      "epoch  18 / 200, step    0 / 100; loss 1.7926\n",
      "Evaluate (epoch 18) -- logp(x) = -1.928 +/- 0.016\n",
      "epoch  19 / 200, step    0 / 100; loss 1.9495\n",
      "Evaluate (epoch 19) -- logp(x) = -1.871 +/- 0.016\n",
      "epoch  20 / 200, step    0 / 100; loss 1.8518\n",
      "Evaluate (epoch 20) -- logp(x) = -1.908 +/- 0.016\n",
      "epoch  21 / 200, step    0 / 100; loss 1.9731\n",
      "Evaluate (epoch 21) -- logp(x) = -1.847 +/- 0.016\n",
      "epoch  22 / 200, step    0 / 100; loss 1.8583\n",
      "Evaluate (epoch 22) -- logp(x) = -1.814 +/- 0.019\n",
      "epoch  23 / 200, step    0 / 100; loss 1.7718\n",
      "Evaluate (epoch 23) -- logp(x) = -1.862 +/- 0.025\n",
      "epoch  24 / 200, step    0 / 100; loss 1.9009\n",
      "Evaluate (epoch 24) -- logp(x) = -1.844 +/- 0.015\n",
      "epoch  25 / 200, step    0 / 100; loss 1.7755\n",
      "Evaluate (epoch 25) -- logp(x) = -1.763 +/- 0.018\n",
      "epoch  26 / 200, step    0 / 100; loss 1.7884\n",
      "Evaluate (epoch 26) -- logp(x) = -1.753 +/- 0.017\n",
      "epoch  27 / 200, step    0 / 100; loss 1.7575\n",
      "Evaluate (epoch 27) -- logp(x) = -1.721 +/- 0.018\n",
      "epoch  28 / 200, step    0 / 100; loss 1.6602\n",
      "Evaluate (epoch 28) -- logp(x) = -1.786 +/- 0.018\n",
      "epoch  29 / 200, step    0 / 100; loss 1.7439\n",
      "Evaluate (epoch 29) -- logp(x) = -1.816 +/- 0.018\n",
      "epoch  30 / 200, step    0 / 100; loss 1.8245\n",
      "Evaluate (epoch 30) -- logp(x) = -1.773 +/- 0.017\n",
      "epoch  31 / 200, step    0 / 100; loss 1.6311\n",
      "Evaluate (epoch 31) -- logp(x) = -1.769 +/- 0.022\n",
      "epoch  32 / 200, step    0 / 100; loss 1.7076\n",
      "Evaluate (epoch 32) -- logp(x) = -1.894 +/- 0.052\n",
      "epoch  33 / 200, step    0 / 100; loss 1.9413\n",
      "Evaluate (epoch 33) -- logp(x) = -1.674 +/- 0.018\n",
      "epoch  34 / 200, step    0 / 100; loss 1.6222\n",
      "Evaluate (epoch 34) -- logp(x) = -1.650 +/- 0.019\n",
      "epoch  35 / 200, step    0 / 100; loss 1.7426\n",
      "Evaluate (epoch 35) -- logp(x) = -1.672 +/- 0.018\n",
      "epoch  36 / 200, step    0 / 100; loss 1.6114\n",
      "Evaluate (epoch 36) -- logp(x) = -1.761 +/- 0.015\n",
      "epoch  37 / 200, step    0 / 100; loss 1.7988\n",
      "Evaluate (epoch 37) -- logp(x) = -1.638 +/- 0.019\n",
      "epoch  38 / 200, step    0 / 100; loss 1.5495\n",
      "Evaluate (epoch 38) -- logp(x) = -1.623 +/- 0.018\n",
      "epoch  39 / 200, step    0 / 100; loss 1.5303\n",
      "Evaluate (epoch 39) -- logp(x) = -1.600 +/- 0.019\n",
      "epoch  40 / 200, step    0 / 100; loss 1.6719\n",
      "Evaluate (epoch 40) -- logp(x) = -1.617 +/- 0.020\n",
      "epoch  41 / 200, step    0 / 100; loss 1.5839\n",
      "Evaluate (epoch 41) -- logp(x) = -1.600 +/- 0.019\n",
      "epoch  42 / 200, step    0 / 100; loss 1.5226\n",
      "Evaluate (epoch 42) -- logp(x) = -1.729 +/- 0.020\n",
      "epoch  43 / 200, step    0 / 100; loss 1.6860\n",
      "Evaluate (epoch 43) -- logp(x) = -1.601 +/- 0.021\n",
      "epoch  44 / 200, step    0 / 100; loss 1.5669\n",
      "Evaluate (epoch 44) -- logp(x) = -1.604 +/- 0.020\n",
      "epoch  45 / 200, step    0 / 100; loss 1.4809\n",
      "Evaluate (epoch 45) -- logp(x) = -1.605 +/- 0.019\n",
      "epoch  46 / 200, step    0 / 100; loss 1.6467\n",
      "Evaluate (epoch 46) -- logp(x) = -1.548 +/- 0.023\n",
      "epoch  47 / 200, step    0 / 100; loss 1.7038\n",
      "Evaluate (epoch 47) -- logp(x) = -1.582 +/- 0.020\n",
      "epoch  48 / 200, step    0 / 100; loss 1.6800\n",
      "Evaluate (epoch 48) -- logp(x) = -1.562 +/- 0.021\n",
      "epoch  49 / 200, step    0 / 100; loss 1.7238\n",
      "Evaluate (epoch 49) -- logp(x) = -1.531 +/- 0.027\n",
      "epoch  50 / 200, step    0 / 100; loss 1.3743\n",
      "Evaluate (epoch 50) -- logp(x) = -1.529 +/- 0.020\n",
      "epoch  51 / 200, step    0 / 100; loss 1.5294\n",
      "Evaluate (epoch 51) -- logp(x) = -1.494 +/- 0.021\n",
      "epoch  52 / 200, step    0 / 100; loss 1.5803\n",
      "Evaluate (epoch 52) -- logp(x) = -1.533 +/- 0.020\n",
      "epoch  53 / 200, step    0 / 100; loss 1.5739\n",
      "Evaluate (epoch 53) -- logp(x) = -1.461 +/- 0.021\n",
      "epoch  54 / 200, step    0 / 100; loss 1.4435\n",
      "Evaluate (epoch 54) -- logp(x) = -1.474 +/- 0.020\n",
      "epoch  55 / 200, step    0 / 100; loss 1.4046\n",
      "Evaluate (epoch 55) -- logp(x) = -1.568 +/- 0.021\n",
      "epoch  56 / 200, step    0 / 100; loss 1.6754\n",
      "Evaluate (epoch 56) -- logp(x) = -1.433 +/- 0.022\n",
      "epoch  57 / 200, step    0 / 100; loss 1.4404\n",
      "Evaluate (epoch 57) -- logp(x) = -1.541 +/- 0.020\n",
      "epoch  58 / 200, step    0 / 100; loss 1.5836\n",
      "Evaluate (epoch 58) -- logp(x) = -1.459 +/- 0.020\n",
      "epoch  59 / 200, step    0 / 100; loss 1.4748\n",
      "Evaluate (epoch 59) -- logp(x) = -1.483 +/- 0.022\n",
      "epoch  60 / 200, step    0 / 100; loss 1.1799\n",
      "Evaluate (epoch 60) -- logp(x) = -1.450 +/- 0.021\n",
      "epoch  61 / 200, step    0 / 100; loss 1.5153\n",
      "Evaluate (epoch 61) -- logp(x) = -1.455 +/- 0.021\n",
      "epoch  62 / 200, step    0 / 100; loss 1.5311\n",
      "Evaluate (epoch 62) -- logp(x) = -1.413 +/- 0.021\n",
      "epoch  63 / 200, step    0 / 100; loss 1.5553\n",
      "Evaluate (epoch 63) -- logp(x) = -1.461 +/- 0.023\n",
      "epoch  64 / 200, step    0 / 100; loss 1.5376\n",
      "Evaluate (epoch 64) -- logp(x) = -1.380 +/- 0.023\n",
      "epoch  65 / 200, step    0 / 100; loss 1.6519\n",
      "Evaluate (epoch 65) -- logp(x) = -1.433 +/- 0.021\n",
      "epoch  66 / 200, step    0 / 100; loss 1.4979\n",
      "Evaluate (epoch 66) -- logp(x) = -1.370 +/- 0.021\n",
      "epoch  67 / 200, step    0 / 100; loss 1.3133\n",
      "Evaluate (epoch 67) -- logp(x) = -1.377 +/- 0.023\n",
      "epoch  68 / 200, step    0 / 100; loss 1.6606\n",
      "Evaluate (epoch 68) -- logp(x) = -1.563 +/- 0.020\n",
      "epoch  69 / 200, step    0 / 100; loss 1.5007\n",
      "Evaluate (epoch 69) -- logp(x) = -1.854 +/- 0.021\n",
      "epoch  70 / 200, step    0 / 100; loss 1.6842\n",
      "Evaluate (epoch 70) -- logp(x) = -1.576 +/- 0.020\n",
      "epoch  71 / 200, step    0 / 100; loss 1.6314\n",
      "Evaluate (epoch 71) -- logp(x) = -1.415 +/- 0.021\n",
      "epoch  72 / 200, step    0 / 100; loss 1.4891\n",
      "Evaluate (epoch 72) -- logp(x) = -1.357 +/- 0.022\n",
      "epoch  73 / 200, step    0 / 100; loss 1.3416\n",
      "Evaluate (epoch 73) -- logp(x) = -1.361 +/- 0.021\n",
      "epoch  74 / 200, step    0 / 100; loss 1.2169\n",
      "Evaluate (epoch 74) -- logp(x) = -1.360 +/- 0.022\n",
      "epoch  75 / 200, step    0 / 100; loss 1.4081\n",
      "Evaluate (epoch 75) -- logp(x) = -1.420 +/- 0.022\n",
      "epoch  76 / 200, step    0 / 100; loss 1.4275\n",
      "Evaluate (epoch 76) -- logp(x) = -1.320 +/- 0.022\n",
      "epoch  77 / 200, step    0 / 100; loss 1.2573\n",
      "Evaluate (epoch 77) -- logp(x) = -1.335 +/- 0.021\n",
      "epoch  78 / 200, step    0 / 100; loss 1.2797\n",
      "Evaluate (epoch 78) -- logp(x) = -1.338 +/- 0.024\n",
      "epoch  79 / 200, step    0 / 100; loss 1.3737\n",
      "Evaluate (epoch 79) -- logp(x) = -1.343 +/- 0.022\n",
      "epoch  80 / 200, step    0 / 100; loss 1.2282\n",
      "Evaluate (epoch 80) -- logp(x) = -1.334 +/- 0.023\n",
      "epoch  81 / 200, step    0 / 100; loss 1.1429\n",
      "Evaluate (epoch 81) -- logp(x) = -1.300 +/- 0.023\n",
      "epoch  82 / 200, step    0 / 100; loss 1.6157\n",
      "Evaluate (epoch 82) -- logp(x) = -1.312 +/- 0.023\n",
      "epoch  83 / 200, step    0 / 100; loss 1.1422\n",
      "Evaluate (epoch 83) -- logp(x) = -1.378 +/- 0.024\n",
      "epoch  84 / 200, step    0 / 100; loss 1.3857\n",
      "Evaluate (epoch 84) -- logp(x) = -1.322 +/- 0.025\n",
      "epoch  85 / 200, step    0 / 100; loss 1.1677\n",
      "Evaluate (epoch 85) -- logp(x) = -1.324 +/- 0.023\n",
      "epoch  86 / 200, step    0 / 100; loss 1.3018\n",
      "Evaluate (epoch 86) -- logp(x) = -1.344 +/- 0.024\n",
      "epoch  87 / 200, step    0 / 100; loss 1.3822\n",
      "Evaluate (epoch 87) -- logp(x) = -1.266 +/- 0.022\n",
      "epoch  88 / 200, step    0 / 100; loss 1.1946\n",
      "Evaluate (epoch 88) -- logp(x) = -1.274 +/- 0.023\n",
      "epoch  89 / 200, step    0 / 100; loss 1.2537\n",
      "Evaluate (epoch 89) -- logp(x) = -1.359 +/- 0.022\n",
      "epoch  90 / 200, step    0 / 100; loss 1.3901\n",
      "Evaluate (epoch 90) -- logp(x) = -1.314 +/- 0.025\n",
      "epoch  91 / 200, step    0 / 100; loss 1.4043\n",
      "Evaluate (epoch 91) -- logp(x) = -1.305 +/- 0.021\n",
      "epoch  92 / 200, step    0 / 100; loss 1.2085\n",
      "Evaluate (epoch 92) -- logp(x) = -1.406 +/- 0.022\n",
      "epoch  93 / 200, step    0 / 100; loss 1.4890\n",
      "Evaluate (epoch 93) -- logp(x) = -1.246 +/- 0.022\n",
      "epoch  94 / 200, step    0 / 100; loss 1.2022\n",
      "Evaluate (epoch 94) -- logp(x) = -1.250 +/- 0.022\n",
      "epoch  95 / 200, step    0 / 100; loss 1.1359\n",
      "Evaluate (epoch 95) -- logp(x) = -1.314 +/- 0.024\n",
      "epoch  96 / 200, step    0 / 100; loss 1.3788\n",
      "Evaluate (epoch 96) -- logp(x) = -1.306 +/- 0.023\n",
      "epoch  97 / 200, step    0 / 100; loss 1.1868\n",
      "Evaluate (epoch 97) -- logp(x) = -1.290 +/- 0.023\n",
      "epoch  98 / 200, step    0 / 100; loss 1.5121\n",
      "Evaluate (epoch 98) -- logp(x) = -1.223 +/- 0.023\n",
      "epoch  99 / 200, step    0 / 100; loss 1.4545\n",
      "Evaluate (epoch 99) -- logp(x) = -1.278 +/- 0.022\n",
      "epoch 100 / 200, step    0 / 100; loss 1.3131\n",
      "Evaluate (epoch 100) -- logp(x) = -1.248 +/- 0.023\n",
      "epoch 101 / 200, step    0 / 100; loss 1.3221\n",
      "Evaluate (epoch 101) -- logp(x) = -1.315 +/- 0.024\n",
      "epoch 102 / 200, step    0 / 100; loss 1.4434\n",
      "Evaluate (epoch 102) -- logp(x) = -1.268 +/- 0.023\n",
      "epoch 103 / 200, step    0 / 100; loss 1.2283\n",
      "Evaluate (epoch 103) -- logp(x) = -1.227 +/- 0.023\n",
      "epoch 104 / 200, step    0 / 100; loss 1.2009\n",
      "Evaluate (epoch 104) -- logp(x) = -1.279 +/- 0.023\n",
      "epoch 105 / 200, step    0 / 100; loss 1.2648\n",
      "Evaluate (epoch 105) -- logp(x) = -1.218 +/- 0.023\n",
      "epoch 106 / 200, step    0 / 100; loss 1.1913\n",
      "Evaluate (epoch 106) -- logp(x) = -1.242 +/- 0.023\n",
      "epoch 107 / 200, step    0 / 100; loss 1.2425\n",
      "Evaluate (epoch 107) -- logp(x) = -1.258 +/- 0.022\n",
      "epoch 108 / 200, step    0 / 100; loss 1.5566\n",
      "Evaluate (epoch 108) -- logp(x) = -1.241 +/- 0.024\n",
      "epoch 109 / 200, step    0 / 100; loss 1.2635\n",
      "Evaluate (epoch 109) -- logp(x) = -1.246 +/- 0.023\n",
      "epoch 110 / 200, step    0 / 100; loss 1.1699\n",
      "Evaluate (epoch 110) -- logp(x) = -1.280 +/- 0.027\n",
      "epoch 111 / 200, step    0 / 100; loss 1.1348\n",
      "Evaluate (epoch 111) -- logp(x) = -1.315 +/- 0.022\n",
      "epoch 112 / 200, step    0 / 100; loss 1.3562\n",
      "Evaluate (epoch 112) -- logp(x) = -1.209 +/- 0.022\n",
      "epoch 113 / 200, step    0 / 100; loss 1.2349\n",
      "Evaluate (epoch 113) -- logp(x) = -1.244 +/- 0.023\n",
      "epoch 114 / 200, step    0 / 100; loss 1.1930\n",
      "Evaluate (epoch 114) -- logp(x) = -1.220 +/- 0.023\n",
      "epoch 115 / 200, step    0 / 100; loss 1.1758\n",
      "Evaluate (epoch 115) -- logp(x) = -1.497 +/- 0.023\n",
      "epoch 116 / 200, step    0 / 100; loss 1.5593\n",
      "Evaluate (epoch 116) -- logp(x) = -1.303 +/- 0.022\n",
      "epoch 117 / 200, step    0 / 100; loss 1.3833\n",
      "Evaluate (epoch 117) -- logp(x) = -1.224 +/- 0.023\n",
      "epoch 118 / 200, step    0 / 100; loss 1.1941\n",
      "Evaluate (epoch 118) -- logp(x) = -1.290 +/- 0.023\n",
      "epoch 119 / 200, step    0 / 100; loss 1.1250\n",
      "Evaluate (epoch 119) -- logp(x) = -1.183 +/- 0.022\n",
      "epoch 120 / 200, step    0 / 100; loss 1.1390\n",
      "Evaluate (epoch 120) -- logp(x) = -1.207 +/- 0.023\n",
      "epoch 121 / 200, step    0 / 100; loss 1.3121\n",
      "Evaluate (epoch 121) -- logp(x) = -1.197 +/- 0.023\n",
      "epoch 122 / 200, step    0 / 100; loss 1.2337\n",
      "Evaluate (epoch 122) -- logp(x) = -1.244 +/- 0.022\n",
      "epoch 123 / 200, step    0 / 100; loss 1.2732\n",
      "Evaluate (epoch 123) -- logp(x) = -1.245 +/- 0.022\n",
      "epoch 124 / 200, step    0 / 100; loss 1.2662\n",
      "Evaluate (epoch 124) -- logp(x) = -1.164 +/- 0.023\n",
      "epoch 125 / 200, step    0 / 100; loss 1.1360\n",
      "Evaluate (epoch 125) -- logp(x) = -1.178 +/- 0.023\n",
      "epoch 126 / 200, step    0 / 100; loss 1.2043\n",
      "Evaluate (epoch 126) -- logp(x) = -1.209 +/- 0.023\n",
      "epoch 127 / 200, step    0 / 100; loss 1.3148\n",
      "Evaluate (epoch 127) -- logp(x) = -1.193 +/- 0.023\n",
      "epoch 128 / 200, step    0 / 100; loss 1.1035\n",
      "Evaluate (epoch 128) -- logp(x) = -1.174 +/- 0.024\n",
      "epoch 129 / 200, step    0 / 100; loss 1.1695\n",
      "Evaluate (epoch 129) -- logp(x) = -1.167 +/- 0.024\n",
      "epoch 130 / 200, step    0 / 100; loss 1.2563\n",
      "Evaluate (epoch 130) -- logp(x) = -1.257 +/- 0.030\n",
      "epoch 131 / 200, step    0 / 100; loss 1.2931\n",
      "Evaluate (epoch 131) -- logp(x) = -1.184 +/- 0.024\n",
      "epoch 132 / 200, step    0 / 100; loss 1.3733\n",
      "Evaluate (epoch 132) -- logp(x) = -1.264 +/- 0.026\n",
      "epoch 133 / 200, step    0 / 100; loss 1.2544\n",
      "Evaluate (epoch 133) -- logp(x) = -1.204 +/- 0.023\n",
      "epoch 134 / 200, step    0 / 100; loss 1.2151\n",
      "Evaluate (epoch 134) -- logp(x) = -1.208 +/- 0.023\n",
      "epoch 135 / 200, step    0 / 100; loss 1.3704\n",
      "Evaluate (epoch 135) -- logp(x) = -1.142 +/- 0.023\n",
      "epoch 136 / 200, step    0 / 100; loss 1.1994\n",
      "Evaluate (epoch 136) -- logp(x) = -1.257 +/- 0.022\n",
      "epoch 137 / 200, step    0 / 100; loss 1.1669\n",
      "Evaluate (epoch 137) -- logp(x) = -1.137 +/- 0.023\n",
      "epoch 138 / 200, step    0 / 100; loss 1.2049\n",
      "Evaluate (epoch 138) -- logp(x) = -1.175 +/- 0.024\n",
      "epoch 139 / 200, step    0 / 100; loss 1.1843\n",
      "Evaluate (epoch 139) -- logp(x) = -1.342 +/- 0.023\n",
      "epoch 140 / 200, step    0 / 100; loss 1.2095\n",
      "Evaluate (epoch 140) -- logp(x) = -1.161 +/- 0.023\n",
      "epoch 141 / 200, step    0 / 100; loss 1.2781\n",
      "Evaluate (epoch 141) -- logp(x) = -1.191 +/- 0.027\n",
      "epoch 142 / 200, step    0 / 100; loss 1.0935\n",
      "Evaluate (epoch 142) -- logp(x) = -1.136 +/- 0.023\n",
      "epoch 143 / 200, step    0 / 100; loss 1.2025\n",
      "Evaluate (epoch 143) -- logp(x) = -1.148 +/- 0.023\n",
      "epoch 144 / 200, step    0 / 100; loss 1.2065\n",
      "Evaluate (epoch 144) -- logp(x) = -1.168 +/- 0.023\n",
      "epoch 145 / 200, step    0 / 100; loss 1.1976\n",
      "Evaluate (epoch 145) -- logp(x) = -1.192 +/- 0.023\n",
      "epoch 146 / 200, step    0 / 100; loss 1.1326\n",
      "Evaluate (epoch 146) -- logp(x) = -1.187 +/- 0.024\n",
      "epoch 147 / 200, step    0 / 100; loss 1.1466\n",
      "Evaluate (epoch 147) -- logp(x) = -1.134 +/- 0.023\n",
      "epoch 148 / 200, step    0 / 100; loss 1.2354\n",
      "Evaluate (epoch 148) -- logp(x) = -1.245 +/- 0.027\n",
      "epoch 149 / 200, step    0 / 100; loss 1.1485\n",
      "Evaluate (epoch 149) -- logp(x) = -1.106 +/- 0.024\n",
      "epoch 150 / 200, step    0 / 100; loss 0.8066\n",
      "Evaluate (epoch 150) -- logp(x) = -1.140 +/- 0.025\n",
      "epoch 151 / 200, step    0 / 100; loss 1.0905\n",
      "Evaluate (epoch 151) -- logp(x) = -1.165 +/- 0.024\n",
      "epoch 152 / 200, step    0 / 100; loss 1.1428\n",
      "Evaluate (epoch 152) -- logp(x) = -1.141 +/- 0.023\n",
      "epoch 153 / 200, step    0 / 100; loss 1.1099\n",
      "Evaluate (epoch 153) -- logp(x) = -1.177 +/- 0.023\n",
      "epoch 154 / 200, step    0 / 100; loss 1.0583\n",
      "Evaluate (epoch 154) -- logp(x) = -1.142 +/- 0.023\n",
      "epoch 155 / 200, step    0 / 100; loss 1.1127\n",
      "Evaluate (epoch 155) -- logp(x) = -1.217 +/- 0.024\n",
      "epoch 156 / 200, step    0 / 100; loss 1.3313\n",
      "Evaluate (epoch 156) -- logp(x) = -1.128 +/- 0.024\n",
      "epoch 157 / 200, step    0 / 100; loss 1.0245\n",
      "Evaluate (epoch 157) -- logp(x) = -1.150 +/- 0.023\n",
      "epoch 158 / 200, step    0 / 100; loss 1.0867\n",
      "Evaluate (epoch 158) -- logp(x) = -1.146 +/- 0.023\n",
      "epoch 159 / 200, step    0 / 100; loss 0.9399\n",
      "Evaluate (epoch 159) -- logp(x) = -1.098 +/- 0.023\n",
      "epoch 160 / 200, step    0 / 100; loss 0.9528\n",
      "Evaluate (epoch 160) -- logp(x) = -1.092 +/- 0.024\n",
      "epoch 161 / 200, step    0 / 100; loss 1.1937\n",
      "Evaluate (epoch 161) -- logp(x) = -1.114 +/- 0.024\n",
      "epoch 162 / 200, step    0 / 100; loss 1.0110\n",
      "Evaluate (epoch 162) -- logp(x) = -1.104 +/- 0.023\n",
      "epoch 163 / 200, step    0 / 100; loss 1.0787\n",
      "Evaluate (epoch 163) -- logp(x) = -1.175 +/- 0.024\n",
      "epoch 164 / 200, step    0 / 100; loss 1.1414\n",
      "Evaluate (epoch 164) -- logp(x) = -1.102 +/- 0.023\n",
      "epoch 165 / 200, step    0 / 100; loss 1.0773\n",
      "Evaluate (epoch 165) -- logp(x) = -1.072 +/- 0.024\n",
      "epoch 166 / 200, step    0 / 100; loss 0.9425\n",
      "Evaluate (epoch 166) -- logp(x) = -1.266 +/- 0.021\n",
      "epoch 167 / 200, step    0 / 100; loss 1.3009\n",
      "Evaluate (epoch 167) -- logp(x) = -1.106 +/- 0.024\n",
      "epoch 168 / 200, step    0 / 100; loss 1.2731\n",
      "Evaluate (epoch 168) -- logp(x) = -1.114 +/- 0.023\n",
      "epoch 169 / 200, step    0 / 100; loss 1.1765\n",
      "Evaluate (epoch 169) -- logp(x) = -1.113 +/- 0.025\n",
      "epoch 170 / 200, step    0 / 100; loss 0.9919\n",
      "Evaluate (epoch 170) -- logp(x) = -1.145 +/- 0.023\n",
      "epoch 171 / 200, step    0 / 100; loss 1.0405\n",
      "Evaluate (epoch 171) -- logp(x) = -1.121 +/- 0.023\n",
      "epoch 172 / 200, step    0 / 100; loss 1.1814\n",
      "Evaluate (epoch 172) -- logp(x) = -1.106 +/- 0.023\n",
      "epoch 173 / 200, step    0 / 100; loss 1.2676\n",
      "Evaluate (epoch 173) -- logp(x) = -1.240 +/- 0.022\n",
      "epoch 174 / 200, step    0 / 100; loss 1.3001\n",
      "Evaluate (epoch 174) -- logp(x) = -1.089 +/- 0.023\n",
      "epoch 175 / 200, step    0 / 100; loss 1.2033\n",
      "Evaluate (epoch 175) -- logp(x) = -1.073 +/- 0.023\n",
      "epoch 176 / 200, step    0 / 100; loss 1.1363\n",
      "Evaluate (epoch 176) -- logp(x) = -1.113 +/- 0.023\n",
      "epoch 177 / 200, step    0 / 100; loss 1.2413\n",
      "Evaluate (epoch 177) -- logp(x) = -1.089 +/- 0.023\n",
      "epoch 178 / 200, step    0 / 100; loss 0.9641\n",
      "Evaluate (epoch 178) -- logp(x) = -1.051 +/- 0.024\n",
      "epoch 179 / 200, step    0 / 100; loss 1.2274\n",
      "Evaluate (epoch 179) -- logp(x) = -1.147 +/- 0.025\n",
      "epoch 180 / 200, step    0 / 100; loss 1.1700\n",
      "Evaluate (epoch 180) -- logp(x) = -1.075 +/- 0.025\n",
      "epoch 181 / 200, step    0 / 100; loss 0.8811\n",
      "Evaluate (epoch 181) -- logp(x) = -1.055 +/- 0.024\n",
      "epoch 182 / 200, step    0 / 100; loss 1.2089\n",
      "Evaluate (epoch 182) -- logp(x) = -1.122 +/- 0.023\n",
      "epoch 183 / 200, step    0 / 100; loss 1.0300\n",
      "Evaluate (epoch 183) -- logp(x) = -1.071 +/- 0.024\n",
      "epoch 184 / 200, step    0 / 100; loss 1.1909\n",
      "Evaluate (epoch 184) -- logp(x) = -1.083 +/- 0.025\n",
      "epoch 185 / 200, step    0 / 100; loss 1.1287\n",
      "Evaluate (epoch 185) -- logp(x) = -1.114 +/- 0.022\n",
      "epoch 186 / 200, step    0 / 100; loss 1.1563\n",
      "Evaluate (epoch 186) -- logp(x) = -1.199 +/- 0.022\n",
      "epoch 187 / 200, step    0 / 100; loss 1.3219\n",
      "Evaluate (epoch 187) -- logp(x) = -1.049 +/- 0.024\n",
      "epoch 188 / 200, step    0 / 100; loss 1.0660\n",
      "Evaluate (epoch 188) -- logp(x) = -1.101 +/- 0.023\n",
      "epoch 189 / 200, step    0 / 100; loss 1.1252\n",
      "Evaluate (epoch 189) -- logp(x) = -1.108 +/- 0.023\n",
      "epoch 190 / 200, step    0 / 100; loss 1.0446\n",
      "Evaluate (epoch 190) -- logp(x) = -1.129 +/- 0.025\n",
      "epoch 191 / 200, step    0 / 100; loss 1.2855\n",
      "Evaluate (epoch 191) -- logp(x) = -1.061 +/- 0.024\n",
      "epoch 192 / 200, step    0 / 100; loss 1.2358\n",
      "Evaluate (epoch 192) -- logp(x) = -1.150 +/- 0.028\n",
      "epoch 193 / 200, step    0 / 100; loss 1.1502\n",
      "Evaluate (epoch 193) -- logp(x) = -1.076 +/- 0.024\n",
      "epoch 194 / 200, step    0 / 100; loss 1.2241\n",
      "Evaluate (epoch 194) -- logp(x) = -1.041 +/- 0.023\n",
      "epoch 195 / 200, step    0 / 100; loss 1.0858\n",
      "Evaluate (epoch 195) -- logp(x) = -1.073 +/- 0.024\n",
      "epoch 196 / 200, step    0 / 100; loss 0.9592\n",
      "Evaluate (epoch 196) -- logp(x) = -1.111 +/- 0.024\n",
      "epoch 197 / 200, step    0 / 100; loss 0.8958\n",
      "Evaluate (epoch 197) -- logp(x) = -1.050 +/- 0.024\n",
      "epoch 198 / 200, step    0 / 100; loss 1.0628\n",
      "Evaluate (epoch 198) -- logp(x) = -1.028 +/- 0.024\n",
      "epoch 199 / 200, step    0 / 100; loss 1.0428\n",
      "Evaluate (epoch 199) -- logp(x) = -1.147 +/- 0.024\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'TORUS',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': True,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 3,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 3,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'realnvp',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 50,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': True,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/realnvp',\n",
      " 'restore_file': './results/realnvp/best_model_checkpoint.pt',\n",
      " 'results_file': './results/realnvp\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 199,\n",
      " 'train': False}\n",
      "RealNVP(\n",
      "  (net): FlowSequential(\n",
      "    (0): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'INVOLUTE',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': False,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 2,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 2,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'realnvp',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 200,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': True,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/realnvp',\n",
      " 'restore_file': None,\n",
      " 'results_file': './results/realnvp\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 0,\n",
      " 'train': True}\n",
      "RealNVP(\n",
      "  (net): FlowSequential(\n",
      "    (0): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch   0 / 200, step    0 / 100; loss 2.0048\n",
      "Evaluate (epoch 0) -- logp(x) = -1.042 +/- 0.019\n",
      "epoch   1 / 200, step    0 / 100; loss 1.1514\n",
      "Evaluate (epoch 1) -- logp(x) = -1.035 +/- 0.018\n",
      "epoch   2 / 200, step    0 / 100; loss 1.1046\n",
      "Evaluate (epoch 2) -- logp(x) = -1.033 +/- 0.019\n",
      "epoch   3 / 200, step    0 / 100; loss 1.0245\n",
      "Evaluate (epoch 3) -- logp(x) = -1.026 +/- 0.017\n",
      "epoch   4 / 200, step    0 / 100; loss 0.9594\n",
      "Evaluate (epoch 4) -- logp(x) = -1.023 +/- 0.018\n",
      "epoch   5 / 200, step    0 / 100; loss 0.9735\n",
      "Evaluate (epoch 5) -- logp(x) = -1.022 +/- 0.016\n",
      "epoch   6 / 200, step    0 / 100; loss 1.0305\n",
      "Evaluate (epoch 6) -- logp(x) = -1.017 +/- 0.016\n",
      "epoch   7 / 200, step    0 / 100; loss 1.0457\n",
      "Evaluate (epoch 7) -- logp(x) = -1.009 +/- 0.017\n",
      "epoch   8 / 200, step    0 / 100; loss 0.8977\n",
      "Evaluate (epoch 8) -- logp(x) = -1.006 +/- 0.016\n",
      "epoch   9 / 200, step    0 / 100; loss 0.8721\n",
      "Evaluate (epoch 9) -- logp(x) = -1.002 +/- 0.016\n",
      "epoch  10 / 200, step    0 / 100; loss 1.0113\n",
      "Evaluate (epoch 10) -- logp(x) = -0.980 +/- 0.015\n",
      "epoch  11 / 200, step    0 / 100; loss 1.0001\n",
      "Evaluate (epoch 11) -- logp(x) = -0.949 +/- 0.015\n",
      "epoch  12 / 200, step    0 / 100; loss 0.9089\n",
      "Evaluate (epoch 12) -- logp(x) = -0.943 +/- 0.016\n",
      "epoch  13 / 200, step    0 / 100; loss 0.7700\n",
      "Evaluate (epoch 13) -- logp(x) = -0.902 +/- 0.015\n",
      "epoch  14 / 200, step    0 / 100; loss 0.9218\n",
      "Evaluate (epoch 14) -- logp(x) = -0.897 +/- 0.014\n",
      "epoch  15 / 200, step    0 / 100; loss 0.7650\n",
      "Evaluate (epoch 15) -- logp(x) = -0.879 +/- 0.015\n",
      "epoch  16 / 200, step    0 / 100; loss 0.8271\n",
      "Evaluate (epoch 16) -- logp(x) = -0.846 +/- 0.014\n",
      "epoch  17 / 200, step    0 / 100; loss 0.8781\n",
      "Evaluate (epoch 17) -- logp(x) = -0.827 +/- 0.015\n",
      "epoch  18 / 200, step    0 / 100; loss 0.8775\n",
      "Evaluate (epoch 18) -- logp(x) = -0.812 +/- 0.015\n",
      "epoch  19 / 200, step    0 / 100; loss 0.9507\n",
      "Evaluate (epoch 19) -- logp(x) = -0.791 +/- 0.016\n",
      "epoch  20 / 200, step    0 / 100; loss 0.6520\n",
      "Evaluate (epoch 20) -- logp(x) = -0.792 +/- 0.017\n",
      "epoch  21 / 200, step    0 / 100; loss 0.7461\n",
      "Evaluate (epoch 21) -- logp(x) = -0.827 +/- 0.016\n",
      "epoch  22 / 200, step    0 / 100; loss 0.8508\n",
      "Evaluate (epoch 22) -- logp(x) = -0.992 +/- 0.018\n",
      "epoch  23 / 200, step    0 / 100; loss 1.1888\n",
      "Evaluate (epoch 23) -- logp(x) = -0.867 +/- 0.017\n",
      "epoch  24 / 200, step    0 / 100; loss 0.9136\n",
      "Evaluate (epoch 24) -- logp(x) = -0.832 +/- 0.017\n",
      "epoch  25 / 200, step    0 / 100; loss 0.8078\n",
      "Evaluate (epoch 25) -- logp(x) = -0.792 +/- 0.017\n",
      "epoch  26 / 200, step    0 / 100; loss 0.7643\n",
      "Evaluate (epoch 26) -- logp(x) = -0.755 +/- 0.016\n",
      "epoch  27 / 200, step    0 / 100; loss 0.7704\n",
      "Evaluate (epoch 27) -- logp(x) = -0.749 +/- 0.017\n",
      "epoch  28 / 200, step    0 / 100; loss 0.8689\n",
      "Evaluate (epoch 28) -- logp(x) = -0.698 +/- 0.017\n",
      "epoch  29 / 200, step    0 / 100; loss 0.6281\n",
      "Evaluate (epoch 29) -- logp(x) = -0.708 +/- 0.017\n",
      "epoch  30 / 200, step    0 / 100; loss 0.6449\n",
      "Evaluate (epoch 30) -- logp(x) = -0.667 +/- 0.019\n",
      "epoch  31 / 200, step    0 / 100; loss 0.5454\n",
      "Evaluate (epoch 31) -- logp(x) = -0.663 +/- 0.019\n",
      "epoch  32 / 200, step    0 / 100; loss 0.6977\n",
      "Evaluate (epoch 32) -- logp(x) = -0.652 +/- 0.019\n",
      "epoch  33 / 200, step    0 / 100; loss 0.7901\n",
      "Evaluate (epoch 33) -- logp(x) = -0.649 +/- 0.019\n",
      "epoch  34 / 200, step    0 / 100; loss 0.5965\n",
      "Evaluate (epoch 34) -- logp(x) = -0.611 +/- 0.020\n",
      "epoch  35 / 200, step    0 / 100; loss 0.9764\n",
      "Evaluate (epoch 35) -- logp(x) = -0.619 +/- 0.019\n",
      "epoch  36 / 200, step    0 / 100; loss 0.5519\n",
      "Evaluate (epoch 36) -- logp(x) = -0.608 +/- 0.020\n",
      "epoch  37 / 200, step    0 / 100; loss 0.6919\n",
      "Evaluate (epoch 37) -- logp(x) = -0.727 +/- 0.019\n",
      "epoch  38 / 200, step    0 / 100; loss 0.7600\n",
      "Evaluate (epoch 38) -- logp(x) = -0.616 +/- 0.019\n",
      "epoch  39 / 200, step    0 / 100; loss 0.6546\n",
      "Evaluate (epoch 39) -- logp(x) = -0.622 +/- 0.022\n",
      "epoch  40 / 200, step    0 / 100; loss 0.4881\n",
      "Evaluate (epoch 40) -- logp(x) = -0.608 +/- 0.020\n",
      "epoch  41 / 200, step    0 / 100; loss 0.6564\n",
      "Evaluate (epoch 41) -- logp(x) = -0.579 +/- 0.020\n",
      "epoch  42 / 200, step    0 / 100; loss 0.6182\n",
      "Evaluate (epoch 42) -- logp(x) = -0.556 +/- 0.020\n",
      "epoch  43 / 200, step    0 / 100; loss 0.5787\n",
      "Evaluate (epoch 43) -- logp(x) = -0.535 +/- 0.020\n",
      "epoch  44 / 200, step    0 / 100; loss 0.6871\n",
      "Evaluate (epoch 44) -- logp(x) = -0.522 +/- 0.021\n",
      "epoch  45 / 200, step    0 / 100; loss 0.5300\n",
      "Evaluate (epoch 45) -- logp(x) = -0.495 +/- 0.022\n",
      "epoch  46 / 200, step    0 / 100; loss 0.5186\n",
      "Evaluate (epoch 46) -- logp(x) = -0.535 +/- 0.021\n",
      "epoch  47 / 200, step    0 / 100; loss 0.4394\n",
      "Evaluate (epoch 47) -- logp(x) = -0.520 +/- 0.022\n",
      "epoch  48 / 200, step    0 / 100; loss 0.6011\n",
      "Evaluate (epoch 48) -- logp(x) = -0.484 +/- 0.023\n",
      "epoch  49 / 200, step    0 / 100; loss 0.7039\n",
      "Evaluate (epoch 49) -- logp(x) = -0.545 +/- 0.024\n",
      "epoch  50 / 200, step    0 / 100; loss 0.5767\n",
      "Evaluate (epoch 50) -- logp(x) = -0.505 +/- 0.022\n",
      "epoch  51 / 200, step    0 / 100; loss 0.5349\n",
      "Evaluate (epoch 51) -- logp(x) = -0.432 +/- 0.024\n",
      "epoch  52 / 200, step    0 / 100; loss 0.3185\n",
      "Evaluate (epoch 52) -- logp(x) = -0.568 +/- 0.029\n",
      "epoch  53 / 200, step    0 / 100; loss 0.5035\n",
      "Evaluate (epoch 53) -- logp(x) = -0.414 +/- 0.024\n",
      "epoch  54 / 200, step    0 / 100; loss 0.5642\n",
      "Evaluate (epoch 54) -- logp(x) = -0.410 +/- 0.024\n",
      "epoch  55 / 200, step    0 / 100; loss 0.6521\n",
      "Evaluate (epoch 55) -- logp(x) = -0.429 +/- 0.025\n",
      "epoch  56 / 200, step    0 / 100; loss 0.2635\n",
      "Evaluate (epoch 56) -- logp(x) = -0.411 +/- 0.025\n",
      "epoch  57 / 200, step    0 / 100; loss 0.2473\n",
      "Evaluate (epoch 57) -- logp(x) = -0.468 +/- 0.023\n",
      "epoch  58 / 200, step    0 / 100; loss 0.4515\n",
      "Evaluate (epoch 58) -- logp(x) = -0.384 +/- 0.024\n",
      "epoch  59 / 200, step    0 / 100; loss 0.3033\n",
      "Evaluate (epoch 59) -- logp(x) = -0.388 +/- 0.025\n",
      "epoch  60 / 200, step    0 / 100; loss 0.2568\n",
      "Evaluate (epoch 60) -- logp(x) = -0.466 +/- 0.024\n",
      "epoch  61 / 200, step    0 / 100; loss 0.7624\n",
      "Evaluate (epoch 61) -- logp(x) = -0.394 +/- 0.024\n",
      "epoch  62 / 200, step    0 / 100; loss 0.4317\n",
      "Evaluate (epoch 62) -- logp(x) = -0.377 +/- 0.025\n",
      "epoch  63 / 200, step    0 / 100; loss 0.3246\n",
      "Evaluate (epoch 63) -- logp(x) = -0.391 +/- 0.024\n",
      "epoch  64 / 200, step    0 / 100; loss 0.4362\n",
      "Evaluate (epoch 64) -- logp(x) = -0.452 +/- 0.026\n",
      "epoch  65 / 200, step    0 / 100; loss 0.4931\n",
      "Evaluate (epoch 65) -- logp(x) = -0.430 +/- 0.025\n",
      "epoch  66 / 200, step    0 / 100; loss 0.4029\n",
      "Evaluate (epoch 66) -- logp(x) = -0.362 +/- 0.024\n",
      "epoch  67 / 200, step    0 / 100; loss 0.3787\n",
      "Evaluate (epoch 67) -- logp(x) = -0.421 +/- 0.025\n",
      "epoch  68 / 200, step    0 / 100; loss 0.4903\n",
      "Evaluate (epoch 68) -- logp(x) = -0.408 +/- 0.025\n",
      "epoch  69 / 200, step    0 / 100; loss 0.6073\n",
      "Evaluate (epoch 69) -- logp(x) = -0.418 +/- 0.027\n",
      "epoch  70 / 200, step    0 / 100; loss 0.3679\n",
      "Evaluate (epoch 70) -- logp(x) = -0.549 +/- 0.026\n",
      "epoch  71 / 200, step    0 / 100; loss 0.7847\n",
      "Evaluate (epoch 71) -- logp(x) = -0.354 +/- 0.026\n",
      "epoch  72 / 200, step    0 / 100; loss 0.3363\n",
      "Evaluate (epoch 72) -- logp(x) = -0.365 +/- 0.027\n",
      "epoch  73 / 200, step    0 / 100; loss 0.3266\n",
      "Evaluate (epoch 73) -- logp(x) = -0.393 +/- 0.028\n",
      "epoch  74 / 200, step    0 / 100; loss 0.3371\n",
      "Evaluate (epoch 74) -- logp(x) = -0.364 +/- 0.029\n",
      "epoch  75 / 200, step    0 / 100; loss 0.2960\n",
      "Evaluate (epoch 75) -- logp(x) = -0.424 +/- 0.026\n",
      "epoch  76 / 200, step    0 / 100; loss 0.4707\n",
      "Evaluate (epoch 76) -- logp(x) = -0.331 +/- 0.024\n",
      "epoch  77 / 200, step    0 / 100; loss 0.3530\n",
      "Evaluate (epoch 77) -- logp(x) = -0.375 +/- 0.027\n",
      "epoch  78 / 200, step    0 / 100; loss 0.2056\n",
      "Evaluate (epoch 78) -- logp(x) = -0.379 +/- 0.026\n",
      "epoch  79 / 200, step    0 / 100; loss 0.3374\n",
      "Evaluate (epoch 79) -- logp(x) = -0.300 +/- 0.025\n",
      "epoch  80 / 200, step    0 / 100; loss 0.0926\n",
      "Evaluate (epoch 80) -- logp(x) = -0.337 +/- 0.027\n",
      "epoch  81 / 200, step    0 / 100; loss 0.5997\n",
      "Evaluate (epoch 81) -- logp(x) = -0.305 +/- 0.025\n",
      "epoch  82 / 200, step    0 / 100; loss 0.3161\n",
      "Evaluate (epoch 82) -- logp(x) = -0.307 +/- 0.027\n",
      "epoch  83 / 200, step    0 / 100; loss 0.4011\n",
      "Evaluate (epoch 83) -- logp(x) = -0.478 +/- 0.027\n",
      "epoch  84 / 200, step    0 / 100; loss 0.4599\n",
      "Evaluate (epoch 84) -- logp(x) = -0.380 +/- 0.025\n",
      "epoch  85 / 200, step    0 / 100; loss 0.5501\n",
      "Evaluate (epoch 85) -- logp(x) = -0.308 +/- 0.026\n",
      "epoch  86 / 200, step    0 / 100; loss 0.3453\n",
      "Evaluate (epoch 86) -- logp(x) = -0.301 +/- 0.025\n",
      "epoch  87 / 200, step    0 / 100; loss 0.1075\n",
      "Evaluate (epoch 87) -- logp(x) = -0.321 +/- 0.025\n",
      "epoch  88 / 200, step    0 / 100; loss 0.4516\n",
      "Evaluate (epoch 88) -- logp(x) = -0.339 +/- 0.026\n",
      "epoch  89 / 200, step    0 / 100; loss 0.2645\n",
      "Evaluate (epoch 89) -- logp(x) = -0.277 +/- 0.026\n",
      "epoch  90 / 200, step    0 / 100; loss 0.3995\n",
      "Evaluate (epoch 90) -- logp(x) = -0.295 +/- 0.026\n",
      "epoch  91 / 200, step    0 / 100; loss 0.0107\n",
      "Evaluate (epoch 91) -- logp(x) = -0.294 +/- 0.024\n",
      "epoch  92 / 200, step    0 / 100; loss 0.2128\n",
      "Evaluate (epoch 92) -- logp(x) = -0.289 +/- 0.024\n",
      "epoch  93 / 200, step    0 / 100; loss 0.2417\n",
      "Evaluate (epoch 93) -- logp(x) = -0.274 +/- 0.025\n",
      "epoch  94 / 200, step    0 / 100; loss 0.2742\n",
      "Evaluate (epoch 94) -- logp(x) = -0.298 +/- 0.026\n",
      "epoch  95 / 200, step    0 / 100; loss 0.2377\n",
      "Evaluate (epoch 95) -- logp(x) = -0.262 +/- 0.026\n",
      "epoch  96 / 200, step    0 / 100; loss 0.2044\n",
      "Evaluate (epoch 96) -- logp(x) = -0.250 +/- 0.027\n",
      "epoch  97 / 200, step    0 / 100; loss 0.3621\n",
      "Evaluate (epoch 97) -- logp(x) = -0.299 +/- 0.026\n",
      "epoch  98 / 200, step    0 / 100; loss 0.3260\n",
      "Evaluate (epoch 98) -- logp(x) = -0.300 +/- 0.024\n",
      "epoch  99 / 200, step    0 / 100; loss 0.3042\n",
      "Evaluate (epoch 99) -- logp(x) = -0.264 +/- 0.025\n",
      "epoch 100 / 200, step    0 / 100; loss 0.2695\n",
      "Evaluate (epoch 100) -- logp(x) = -0.343 +/- 0.024\n",
      "epoch 101 / 200, step    0 / 100; loss 0.2362\n",
      "Evaluate (epoch 101) -- logp(x) = -0.307 +/- 0.023\n",
      "epoch 102 / 200, step    0 / 100; loss 0.4718\n",
      "Evaluate (epoch 102) -- logp(x) = -0.267 +/- 0.025\n",
      "epoch 103 / 200, step    0 / 100; loss 0.4129\n",
      "Evaluate (epoch 103) -- logp(x) = -0.328 +/- 0.027\n",
      "epoch 104 / 200, step    0 / 100; loss 0.2407\n",
      "Evaluate (epoch 104) -- logp(x) = -0.333 +/- 0.031\n",
      "epoch 105 / 200, step    0 / 100; loss 0.5160\n",
      "Evaluate (epoch 105) -- logp(x) = -0.288 +/- 0.026\n",
      "epoch 106 / 200, step    0 / 100; loss 0.1191\n",
      "Evaluate (epoch 106) -- logp(x) = -0.303 +/- 0.024\n",
      "epoch 107 / 200, step    0 / 100; loss 0.3000\n",
      "Evaluate (epoch 107) -- logp(x) = -0.350 +/- 0.027\n",
      "epoch 108 / 200, step    0 / 100; loss 0.3590\n",
      "Evaluate (epoch 108) -- logp(x) = -0.283 +/- 0.025\n",
      "epoch 109 / 200, step    0 / 100; loss 0.1983\n",
      "Evaluate (epoch 109) -- logp(x) = -0.382 +/- 0.028\n",
      "epoch 110 / 200, step    0 / 100; loss 0.4046\n",
      "Evaluate (epoch 110) -- logp(x) = -0.331 +/- 0.027\n",
      "epoch 111 / 200, step    0 / 100; loss 0.1793\n",
      "Evaluate (epoch 111) -- logp(x) = -0.271 +/- 0.025\n",
      "epoch 112 / 200, step    0 / 100; loss 0.2709\n",
      "Evaluate (epoch 112) -- logp(x) = -0.263 +/- 0.025\n",
      "epoch 113 / 200, step    0 / 100; loss 0.1671\n",
      "Evaluate (epoch 113) -- logp(x) = -0.350 +/- 0.027\n",
      "epoch 114 / 200, step    0 / 100; loss 0.2611\n",
      "Evaluate (epoch 114) -- logp(x) = -0.275 +/- 0.026\n",
      "epoch 115 / 200, step    0 / 100; loss 0.4797\n",
      "Evaluate (epoch 115) -- logp(x) = -0.239 +/- 0.028\n",
      "epoch 116 / 200, step    0 / 100; loss 0.4443\n",
      "Evaluate (epoch 116) -- logp(x) = -0.245 +/- 0.028\n",
      "epoch 117 / 200, step    0 / 100; loss 0.0460\n",
      "Evaluate (epoch 117) -- logp(x) = -0.267 +/- 0.025\n",
      "epoch 118 / 200, step    0 / 100; loss 0.1802\n",
      "Evaluate (epoch 118) -- logp(x) = -0.231 +/- 0.026\n",
      "epoch 119 / 200, step    0 / 100; loss 0.6084\n",
      "Evaluate (epoch 119) -- logp(x) = -0.242 +/- 0.027\n",
      "epoch 120 / 200, step    0 / 100; loss 0.3732\n",
      "Evaluate (epoch 120) -- logp(x) = -0.242 +/- 0.028\n",
      "epoch 121 / 200, step    0 / 100; loss 0.3408\n",
      "Evaluate (epoch 121) -- logp(x) = -0.239 +/- 0.025\n",
      "epoch 122 / 200, step    0 / 100; loss 0.3456\n",
      "Evaluate (epoch 122) -- logp(x) = -0.273 +/- 0.025\n",
      "epoch 123 / 200, step    0 / 100; loss 0.2198\n",
      "Evaluate (epoch 123) -- logp(x) = -0.223 +/- 0.025\n",
      "epoch 124 / 200, step    0 / 100; loss 0.2690\n",
      "Evaluate (epoch 124) -- logp(x) = -0.238 +/- 0.026\n",
      "epoch 125 / 200, step    0 / 100; loss 0.2833\n",
      "Evaluate (epoch 125) -- logp(x) = -0.220 +/- 0.026\n",
      "epoch 126 / 200, step    0 / 100; loss 0.2101\n",
      "Evaluate (epoch 126) -- logp(x) = -0.312 +/- 0.027\n",
      "epoch 127 / 200, step    0 / 100; loss 0.2124\n",
      "Evaluate (epoch 127) -- logp(x) = -0.212 +/- 0.026\n",
      "epoch 128 / 200, step    0 / 100; loss 0.3331\n",
      "Evaluate (epoch 128) -- logp(x) = -0.292 +/- 0.026\n",
      "epoch 129 / 200, step    0 / 100; loss 0.3679\n",
      "Evaluate (epoch 129) -- logp(x) = -0.224 +/- 0.026\n",
      "epoch 130 / 200, step    0 / 100; loss 0.3053\n",
      "Evaluate (epoch 130) -- logp(x) = -0.210 +/- 0.026\n",
      "epoch 131 / 200, step    0 / 100; loss 0.1038\n",
      "Evaluate (epoch 131) -- logp(x) = -0.220 +/- 0.026\n",
      "epoch 132 / 200, step    0 / 100; loss 0.4404\n",
      "Evaluate (epoch 132) -- logp(x) = -0.227 +/- 0.025\n",
      "epoch 133 / 200, step    0 / 100; loss 0.2202\n",
      "Evaluate (epoch 133) -- logp(x) = -0.190 +/- 0.027\n",
      "epoch 134 / 200, step    0 / 100; loss 0.1311\n",
      "Evaluate (epoch 134) -- logp(x) = -0.338 +/- 0.025\n",
      "epoch 135 / 200, step    0 / 100; loss 0.3270\n",
      "Evaluate (epoch 135) -- logp(x) = -0.206 +/- 0.025\n",
      "epoch 136 / 200, step    0 / 100; loss 0.2954\n",
      "Evaluate (epoch 136) -- logp(x) = -0.185 +/- 0.026\n",
      "epoch 137 / 200, step    0 / 100; loss 0.2123\n",
      "Evaluate (epoch 137) -- logp(x) = -0.225 +/- 0.026\n",
      "epoch 138 / 200, step    0 / 100; loss 0.3851\n",
      "Evaluate (epoch 138) -- logp(x) = -0.241 +/- 0.025\n",
      "epoch 139 / 200, step    0 / 100; loss 0.2182\n",
      "Evaluate (epoch 139) -- logp(x) = -0.207 +/- 0.026\n",
      "epoch 140 / 200, step    0 / 100; loss 0.0956\n",
      "Evaluate (epoch 140) -- logp(x) = -0.176 +/- 0.026\n",
      "epoch 141 / 200, step    0 / 100; loss 0.1239\n",
      "Evaluate (epoch 141) -- logp(x) = -0.205 +/- 0.026\n",
      "epoch 142 / 200, step    0 / 100; loss 0.1359\n",
      "Evaluate (epoch 142) -- logp(x) = -0.177 +/- 0.028\n",
      "epoch 143 / 200, step    0 / 100; loss 0.0439\n",
      "Evaluate (epoch 143) -- logp(x) = -0.175 +/- 0.028\n",
      "epoch 144 / 200, step    0 / 100; loss 0.2558\n",
      "Evaluate (epoch 144) -- logp(x) = -0.218 +/- 0.026\n",
      "epoch 145 / 200, step    0 / 100; loss 0.2926\n",
      "Evaluate (epoch 145) -- logp(x) = -0.194 +/- 0.027\n",
      "epoch 146 / 200, step    0 / 100; loss 0.1589\n",
      "Evaluate (epoch 146) -- logp(x) = -0.220 +/- 0.027\n",
      "epoch 147 / 200, step    0 / 100; loss 0.1184\n",
      "Evaluate (epoch 147) -- logp(x) = -0.168 +/- 0.027\n",
      "epoch 148 / 200, step    0 / 100; loss 0.2193\n",
      "Evaluate (epoch 148) -- logp(x) = -0.264 +/- 0.027\n",
      "epoch 149 / 200, step    0 / 100; loss 0.3518\n",
      "Evaluate (epoch 149) -- logp(x) = -0.261 +/- 0.026\n",
      "epoch 150 / 200, step    0 / 100; loss 0.3791\n",
      "Evaluate (epoch 150) -- logp(x) = -0.259 +/- 0.030\n",
      "epoch 151 / 200, step    0 / 100; loss 0.1695\n",
      "Evaluate (epoch 151) -- logp(x) = -0.225 +/- 0.025\n",
      "epoch 152 / 200, step    0 / 100; loss 0.2053\n",
      "Evaluate (epoch 152) -- logp(x) = -0.231 +/- 0.027\n",
      "epoch 153 / 200, step    0 / 100; loss 0.2426\n",
      "Evaluate (epoch 153) -- logp(x) = -0.308 +/- 0.027\n",
      "epoch 154 / 200, step    0 / 100; loss 0.2479\n",
      "Evaluate (epoch 154) -- logp(x) = -0.294 +/- 0.028\n",
      "epoch 155 / 200, step    0 / 100; loss 0.3076\n",
      "Evaluate (epoch 155) -- logp(x) = -0.213 +/- 0.026\n",
      "epoch 156 / 200, step    0 / 100; loss 0.2647\n",
      "Evaluate (epoch 156) -- logp(x) = -0.201 +/- 0.027\n",
      "epoch 157 / 200, step    0 / 100; loss 0.0036\n",
      "Evaluate (epoch 157) -- logp(x) = -0.195 +/- 0.027\n",
      "epoch 158 / 200, step    0 / 100; loss 0.0795\n",
      "Evaluate (epoch 158) -- logp(x) = -0.331 +/- 0.027\n",
      "epoch 159 / 200, step    0 / 100; loss 0.3129\n",
      "Evaluate (epoch 159) -- logp(x) = -0.216 +/- 0.028\n",
      "epoch 160 / 200, step    0 / 100; loss 0.1930\n",
      "Evaluate (epoch 160) -- logp(x) = -0.243 +/- 0.025\n",
      "epoch 161 / 200, step    0 / 100; loss 0.1063\n",
      "Evaluate (epoch 161) -- logp(x) = -0.296 +/- 0.026\n",
      "epoch 162 / 200, step    0 / 100; loss 0.0378\n",
      "Evaluate (epoch 162) -- logp(x) = -0.150 +/- 0.028\n",
      "epoch 163 / 200, step    0 / 100; loss 0.4909\n",
      "Evaluate (epoch 163) -- logp(x) = -0.177 +/- 0.027\n",
      "epoch 164 / 200, step    0 / 100; loss 0.2398\n",
      "Evaluate (epoch 164) -- logp(x) = -0.177 +/- 0.027\n",
      "epoch 165 / 200, step    0 / 100; loss 0.0345\n",
      "Evaluate (epoch 165) -- logp(x) = -0.195 +/- 0.027\n",
      "epoch 166 / 200, step    0 / 100; loss 0.3998\n",
      "Evaluate (epoch 166) -- logp(x) = -0.266 +/- 0.027\n",
      "epoch 167 / 200, step    0 / 100; loss 0.1734\n",
      "Evaluate (epoch 167) -- logp(x) = -0.260 +/- 0.027\n",
      "epoch 168 / 200, step    0 / 100; loss 0.1924\n",
      "Evaluate (epoch 168) -- logp(x) = -0.166 +/- 0.028\n",
      "epoch 169 / 200, step    0 / 100; loss 0.4922\n",
      "Evaluate (epoch 169) -- logp(x) = -0.142 +/- 0.028\n",
      "epoch 170 / 200, step    0 / 100; loss 0.0246\n",
      "Evaluate (epoch 170) -- logp(x) = -0.155 +/- 0.030\n",
      "epoch 171 / 200, step    0 / 100; loss 0.0456\n",
      "Evaluate (epoch 171) -- logp(x) = -0.175 +/- 0.028\n",
      "epoch 172 / 200, step    0 / 100; loss 0.0755\n",
      "Evaluate (epoch 172) -- logp(x) = -0.220 +/- 0.027\n",
      "epoch 173 / 200, step    0 / 100; loss 0.1166\n",
      "Evaluate (epoch 173) -- logp(x) = -0.160 +/- 0.026\n",
      "epoch 174 / 200, step    0 / 100; loss 0.2117\n",
      "Evaluate (epoch 174) -- logp(x) = -0.196 +/- 0.027\n",
      "epoch 175 / 200, step    0 / 100; loss 0.1372\n",
      "Evaluate (epoch 175) -- logp(x) = -0.166 +/- 0.026\n",
      "epoch 176 / 200, step    0 / 100; loss 0.4709\n",
      "Evaluate (epoch 176) -- logp(x) = -0.189 +/- 0.029\n",
      "epoch 177 / 200, step    0 / 100; loss 0.1418\n",
      "Evaluate (epoch 177) -- logp(x) = -0.180 +/- 0.028\n",
      "epoch 178 / 200, step    0 / 100; loss -0.0563\n",
      "Evaluate (epoch 178) -- logp(x) = -0.153 +/- 0.026\n",
      "epoch 179 / 200, step    0 / 100; loss 0.0397\n",
      "Evaluate (epoch 179) -- logp(x) = -0.192 +/- 0.030\n",
      "epoch 180 / 200, step    0 / 100; loss 0.0902\n",
      "Evaluate (epoch 180) -- logp(x) = -0.143 +/- 0.027\n",
      "epoch 181 / 200, step    0 / 100; loss 0.0894\n",
      "Evaluate (epoch 181) -- logp(x) = -0.161 +/- 0.027\n",
      "epoch 182 / 200, step    0 / 100; loss 0.3211\n",
      "Evaluate (epoch 182) -- logp(x) = -0.337 +/- 0.027\n",
      "epoch 183 / 200, step    0 / 100; loss 0.3480\n",
      "Evaluate (epoch 183) -- logp(x) = -0.281 +/- 0.029\n",
      "epoch 184 / 200, step    0 / 100; loss 0.2949\n",
      "Evaluate (epoch 184) -- logp(x) = -0.268 +/- 0.027\n",
      "epoch 185 / 200, step    0 / 100; loss 0.1039\n",
      "Evaluate (epoch 185) -- logp(x) = -0.257 +/- 0.031\n",
      "epoch 186 / 200, step    0 / 100; loss 0.1901\n",
      "Evaluate (epoch 186) -- logp(x) = -0.315 +/- 0.030\n",
      "epoch 187 / 200, step    0 / 100; loss 0.5299\n",
      "Evaluate (epoch 187) -- logp(x) = -0.199 +/- 0.029\n",
      "epoch 188 / 200, step    0 / 100; loss 0.2481\n",
      "Evaluate (epoch 188) -- logp(x) = -0.263 +/- 0.027\n",
      "epoch 189 / 200, step    0 / 100; loss 0.6394\n",
      "Evaluate (epoch 189) -- logp(x) = -0.289 +/- 0.033\n",
      "epoch 190 / 200, step    0 / 100; loss 0.1972\n",
      "Evaluate (epoch 190) -- logp(x) = -0.232 +/- 0.026\n",
      "epoch 191 / 200, step    0 / 100; loss 0.3478\n",
      "Evaluate (epoch 191) -- logp(x) = -0.211 +/- 0.028\n",
      "epoch 192 / 200, step    0 / 100; loss -0.0370\n",
      "Evaluate (epoch 192) -- logp(x) = -0.210 +/- 0.028\n",
      "epoch 193 / 200, step    0 / 100; loss 0.2199\n",
      "Evaluate (epoch 193) -- logp(x) = -0.188 +/- 0.031\n",
      "epoch 194 / 200, step    0 / 100; loss 0.1752\n",
      "Evaluate (epoch 194) -- logp(x) = -0.199 +/- 0.028\n",
      "epoch 195 / 200, step    0 / 100; loss 0.1544\n",
      "Evaluate (epoch 195) -- logp(x) = -0.194 +/- 0.029\n",
      "epoch 196 / 200, step    0 / 100; loss 0.3532\n",
      "Evaluate (epoch 196) -- logp(x) = -0.190 +/- 0.032\n",
      "epoch 197 / 200, step    0 / 100; loss 0.1663\n",
      "Evaluate (epoch 197) -- logp(x) = -0.188 +/- 0.027\n",
      "epoch 198 / 200, step    0 / 100; loss 0.1107\n",
      "Evaluate (epoch 198) -- logp(x) = -0.189 +/- 0.029\n",
      "epoch 199 / 200, step    0 / 100; loss 0.3097\n",
      "Evaluate (epoch 199) -- logp(x) = -0.230 +/- 0.028\n",
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'INVOLUTE',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': True,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 2,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 2,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'realnvp',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 50,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': True,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/realnvp',\n",
      " 'restore_file': './results/realnvp/best_model_checkpoint.pt',\n",
      " 'results_file': './results/realnvp\\\\results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 170,\n",
      " 'train': False}\n",
      "RealNVP(\n",
      "  (net): FlowSequential(\n",
      "    (0): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): LinearMaskedCoupling(\n",
      "      (s_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "      (t_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "!python normalizing_flows/maf.py --train --model realnvp --dataset CIRCLE --n_epochs 200 --no_batch_norm\n",
    "!python normalizing_flows/maf.py --generate --model realnvp --dataset CIRCLE --no_batch_norm --restore_file ./results/realnvp/best_model_checkpoint.pt\n",
    "!python normalizing_flows/maf.py --train --model realnvp --dataset TORUS --n_epochs 200 --no_batch_norm\n",
    "!python normalizing_flows/maf.py --generate --model realnvp --dataset TORUS --no_batch_norm --restore_file ./results/realnvp/best_model_checkpoint.pt\n",
    "!python normalizing_flows/maf.py --train --model realnvp --dataset INVOLUTE --n_epochs 200 --no_batch_norm\n",
    "!python normalizing_flows/maf.py --generate --model realnvp --dataset INVOLUTE --no_batch_norm --restore_file ./results/realnvp/best_model_checkpoint.pt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model RoundTrip:\n",
    "\n",
    "This model is installed from pip, and based on tensorflow. There are lots of dependencies, so it is better to install it in a virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyroundtrip in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from pyroundtrip) (1.1.3)\n",
      "Requirement already satisfied: python-dateutil in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from pyroundtrip) (2.8.2)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from pyroundtrip) (1.5.2)\n",
      "Requirement already satisfied: tensorflow>=2.8.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from pyroundtrip) (2.10.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (0.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (65.5.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (0.31.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (2.10.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (3.19.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (2.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (1.14.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (1.54.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (1.6.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (16.0.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (1.1.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (1.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (4.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (3.7.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (2.10.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (22.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (20210226132247)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorflow>=2.8.0->pyroundtrip) (1.23.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from pandas->pyroundtrip) (2022.7)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from scikit-learn->pyroundtrip) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from scikit-learn->pyroundtrip) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from scikit-learn->pyroundtrip) (1.9.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from astunparse>=1.6.0->tensorflow>=2.8.0->pyroundtrip) (0.35.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (2.28.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (2.20.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (0.4.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (0.2.8)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (1.26.13)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (4.11.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\envs\\py39-cuda117\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow>=2.8.0->pyroundtrip) (3.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyroundtrip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only tested with RountTrip v2.0.1\n",
    "# Revise the evaluation part in class Roundtrip\n",
    "import os\n",
    "import inspect\n",
    "import fileinput\n",
    "import importlib\n",
    "import pyroundtrip as pyrt\n",
    "module_path = inspect.getfile(pyrt)\n",
    "\n",
    "# Get the directory path\n",
    "directory_path = os.path.dirname(module_path)\n",
    "file_path = os.path.join(directory_path,'roundtrip.py')\n",
    "\n",
    "line_count = 0\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        line_count += 1\n",
    "\n",
    "replacement_code = \"        np.save('{}/data_z_at_{}.npy'.format(self.save_dir,batch_idx),data_z_)\\n        data_x_ = self.g_net(self.z_sampler.get_batch(10000))\\n        np.save('{}/data_x_at_{}.npy'.format(self.save_dir,batch_idx),data_x_)\\n\"\n",
    "if not pyrt.__version__ == \"2.0.1\":\n",
    "    print(\"warning: Only tested with RountTrip v2.0.1\")\n",
    "\n",
    "if line_count<1510:\n",
    "    with fileinput.FileInput(file_path, inplace=True) as f:\n",
    "        for line in f:\n",
    "            if f.filelineno() == 210:\n",
    "                print(replacement_code, end='')\n",
    "            else:\n",
    "                # Print the original line as it is\n",
    "                print(line, end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel to import pyrt again\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently use version v2.0.1 of Roundtrip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyroundtrip as pyrt\n",
    "print(\"Currently use version v%s of Roundtrip.\"%pyrt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [0] : g_loss_adv [0.7556], e_loss_adv [0.7920],                l2_loss_x [0.1658], l2_loss_z [1.3156], g_e_loss [16.3612], dx_loss [0.4067], dz_loss [0.4071], d_loss [0.8138]\n",
      "Iteration [10000] : g_loss_adv [0.1536], e_loss_adv [0.1656],                l2_loss_x [0.0155], l2_loss_z [0.0004], g_e_loss [0.4783], dx_loss [0.1596], dz_loss [0.1583], d_loss [0.3179]\n",
      "Iteration [20000] : g_loss_adv [0.2227], e_loss_adv [0.1842],                l2_loss_x [0.0142], l2_loss_z [0.0038], g_e_loss [0.5863], dx_loss [0.1494], dz_loss [0.1594], d_loss [0.3088]\n",
      "Iteration [30000] : g_loss_adv [0.3248], e_loss_adv [0.1551],                l2_loss_x [0.0163], l2_loss_z [0.0020], g_e_loss [0.6627], dx_loss [0.0875], dz_loss [0.1521], d_loss [0.2396]\n",
      "Iteration [40000] : g_loss_adv [0.2735], e_loss_adv [0.1943],                l2_loss_x [0.0096], l2_loss_z [0.0021], g_e_loss [0.5851], dx_loss [0.1388], dz_loss [0.1515], d_loss [0.2903]\n",
      "Iteration [50000] : g_loss_adv [0.2973], e_loss_adv [0.1651],                l2_loss_x [0.0308], l2_loss_z [0.0216], g_e_loss [0.9861], dx_loss [0.1214], dz_loss [0.1585], d_loss [0.2800]\n",
      "Iteration [60000] : g_loss_adv [0.2126], e_loss_adv [0.1684],                l2_loss_x [0.0054], l2_loss_z [0.0045], g_e_loss [0.4796], dx_loss [0.1411], dz_loss [0.1591], d_loss [0.3002]\n",
      "Iteration [70000] : g_loss_adv [0.2474], e_loss_adv [0.1541],                l2_loss_x [0.0169], l2_loss_z [0.0037], g_e_loss [0.6080], dx_loss [0.1331], dz_loss [0.1616], d_loss [0.2947]\n",
      "Iteration [80000] : g_loss_adv [0.2283], e_loss_adv [0.1564],                l2_loss_x [0.0215], l2_loss_z [0.0008], g_e_loss [0.6077], dx_loss [0.1352], dz_loss [0.1583], d_loss [0.2935]\n",
      "Iteration [90000] : g_loss_adv [0.2330], e_loss_adv [0.1569],                l2_loss_x [0.0355], l2_loss_z [0.0022], g_e_loss [0.7673], dx_loss [0.1380], dz_loss [0.1600], d_loss [0.2980]\n",
      "Iteration [100000] : g_loss_adv [0.2720], e_loss_adv [0.1701],                l2_loss_x [0.0103], l2_loss_z [0.0006], g_e_loss [0.5513], dx_loss [0.1357], dz_loss [0.1537], d_loss [0.2894]\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'dataset': 'Involute',\n",
    "    'output_dir': './RoundTrip_623/',\n",
    "    'x_dim': 2,\n",
    "    'z_dim': 1,\n",
    "    'lr': 0.0002,\n",
    "    'alpha': 10,\n",
    "    'beta': 1,\n",
    "    'gamma': 0,\n",
    "    'g_d_freq': 1,\n",
    "    'g_units': [512, 512, 512, 512, 512, 512, 512, 512, 512, 512],\n",
    "    'e_units': [256, 256, 256, 256, 256, 256, 256, 256, 256, 256],\n",
    "    'dz_units': [128, 128],\n",
    "    'dx_units': [256, 256, 256, 256],\n",
    "    'save_model': False,\n",
    "    'sd_x': 0.05,\n",
    "    'scale': 0.5,\n",
    "    'sample_size': 10000\n",
    "}\n",
    "\n",
    "model = pyrt.Roundtrip(params=params,random_seed=123)\n",
    "\n",
    "data = pd.read_csv('datasets/involute/B.csv', header=None).values\n",
    "data = np.array(data, dtype=np.float32)\n",
    "\n",
    "model.train(data=data, save_format='csv', n_iter=100000, batches_per_eval=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [0] : g_loss_adv [0.7568], e_loss_adv [0.7920],                l2_loss_x [0.4979], l2_loss_z [1.3156], g_e_loss [19.6838], dx_loss [0.4178], dz_loss [0.4071], d_loss [0.8249]\n",
      "Iteration [10000] : g_loss_adv [0.1522], e_loss_adv [0.1552],                l2_loss_x [0.0027], l2_loss_z [0.0024], g_e_loss [0.3585], dx_loss [0.1604], dz_loss [0.1614], d_loss [0.3218]\n",
      "Iteration [20000] : g_loss_adv [0.2312], e_loss_adv [0.1595],                l2_loss_x [0.0017], l2_loss_z [0.0072], g_e_loss [0.4806], dx_loss [0.1487], dz_loss [0.1625], d_loss [0.3112]\n",
      "Iteration [30000] : g_loss_adv [0.1729], e_loss_adv [0.1916],                l2_loss_x [0.0064], l2_loss_z [0.0021], g_e_loss [0.4493], dx_loss [0.1454], dz_loss [0.1480], d_loss [0.2934]\n",
      "Iteration [40000] : g_loss_adv [0.1622], e_loss_adv [0.1656],                l2_loss_x [0.0021], l2_loss_z [0.0134], g_e_loss [0.4830], dx_loss [0.1539], dz_loss [0.1638], d_loss [0.3177]\n",
      "Iteration [50000] : g_loss_adv [0.1626], e_loss_adv [0.1612],                l2_loss_x [0.0010], l2_loss_z [0.0011], g_e_loss [0.3448], dx_loss [0.1589], dz_loss [0.1596], d_loss [0.3185]\n",
      "Iteration [60000] : g_loss_adv [0.1560], e_loss_adv [0.1622],                l2_loss_x [0.0011], l2_loss_z [0.0010], g_e_loss [0.3390], dx_loss [0.1593], dz_loss [0.1617], d_loss [0.3210]\n",
      "Iteration [70000] : g_loss_adv [0.1566], e_loss_adv [0.1670],                l2_loss_x [0.0017], l2_loss_z [0.0052], g_e_loss [0.3926], dx_loss [0.1622], dz_loss [0.1600], d_loss [0.3222]\n",
      "Iteration [80000] : g_loss_adv [0.1723], e_loss_adv [0.1596],                l2_loss_x [0.0080], l2_loss_z [0.0064], g_e_loss [0.4756], dx_loss [0.1487], dz_loss [0.1618], d_loss [0.3105]\n",
      "Iteration [90000] : g_loss_adv [0.1902], e_loss_adv [0.1562],                l2_loss_x [0.0017], l2_loss_z [0.0003], g_e_loss [0.3666], dx_loss [0.1510], dz_loss [0.1551], d_loss [0.3061]\n",
      "Iteration [100000] : g_loss_adv [0.1685], e_loss_adv [0.1617],                l2_loss_x [0.0008], l2_loss_z [0.0003], g_e_loss [0.3419], dx_loss [0.1619], dz_loss [0.1655], d_loss [0.3275]\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'dataset': 'Circle',\n",
    "    'output_dir': './RoundTrip_623',\n",
    "    'x_dim': 2,\n",
    "    'z_dim': 1,\n",
    "    'lr': 0.0002,\n",
    "    'alpha': 10,\n",
    "    'beta': 1,\n",
    "    'gamma': 0,\n",
    "    'g_d_freq': 1,\n",
    "    'g_units': [512, 512, 512, 512, 512, 512, 512, 512, 512, 512],\n",
    "    'e_units': [256, 256, 256, 256, 256, 256, 256, 256, 256, 256],\n",
    "    'dz_units': [128, 128],\n",
    "    'dx_units': [256, 256, 256, 256],\n",
    "    'save_model': False,\n",
    "    'sd_x': 0.05,\n",
    "    'scale': 0.5,\n",
    "    'sample_size': 10000\n",
    "}\n",
    "\n",
    "model = pyrt.Roundtrip(params=params,random_seed=123)\n",
    "\n",
    "data = pd.read_csv('datasets/circle/B.csv', header=None).values\n",
    "data = np.array(data, dtype=np.float32)\n",
    "\n",
    "model.train(data=data, save_format='csv', n_iter=100000, batches_per_eval=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [0] : g_loss_adv [0.7581], e_loss_adv [0.7924],                l2_loss_x [0.3736], l2_loss_z [0.8799], g_e_loss [14.0855], dx_loss [0.3854], dz_loss [0.4096], d_loss [0.7951]\n",
      "Iteration [10000] : g_loss_adv [0.1622], e_loss_adv [0.1646],                l2_loss_x [0.0084], l2_loss_z [0.0071], g_e_loss [0.4818], dx_loss [0.1524], dz_loss [0.1628], d_loss [0.3152]\n",
      "Iteration [20000] : g_loss_adv [0.1835], e_loss_adv [0.1766],                l2_loss_x [0.0044], l2_loss_z [0.0071], g_e_loss [0.4759], dx_loss [0.1552], dz_loss [0.1584], d_loss [0.3137]\n",
      "Iteration [30000] : g_loss_adv [0.1682], e_loss_adv [0.1697],                l2_loss_x [0.0015], l2_loss_z [0.0104], g_e_loss [0.4564], dx_loss [0.1497], dz_loss [0.1606], d_loss [0.3103]\n",
      "Iteration [40000] : g_loss_adv [0.1780], e_loss_adv [0.1603],                l2_loss_x [0.0052], l2_loss_z [0.0038], g_e_loss [0.4286], dx_loss [0.1477], dz_loss [0.1598], d_loss [0.3075]\n",
      "Iteration [50000] : g_loss_adv [0.1984], e_loss_adv [0.1670],                l2_loss_x [0.0045], l2_loss_z [0.0053], g_e_loss [0.4631], dx_loss [0.1591], dz_loss [0.1539], d_loss [0.3130]\n",
      "Iteration [60000] : g_loss_adv [0.1930], e_loss_adv [0.1561],                l2_loss_x [0.0020], l2_loss_z [0.0032], g_e_loss [0.4016], dx_loss [0.1384], dz_loss [0.1572], d_loss [0.2956]\n",
      "Iteration [70000] : g_loss_adv [0.1679], e_loss_adv [0.1657],                l2_loss_x [0.0137], l2_loss_z [0.0040], g_e_loss [0.5099], dx_loss [0.1409], dz_loss [0.1606], d_loss [0.3014]\n",
      "Iteration [80000] : g_loss_adv [0.1385], e_loss_adv [0.1609],                l2_loss_x [0.0010], l2_loss_z [0.0011], g_e_loss [0.3197], dx_loss [0.1556], dz_loss [0.1548], d_loss [0.3105]\n",
      "Iteration [90000] : g_loss_adv [0.2128], e_loss_adv [0.1570],                l2_loss_x [0.0009], l2_loss_z [0.0013], g_e_loss [0.3923], dx_loss [0.1544], dz_loss [0.1609], d_loss [0.3153]\n",
      "Iteration [100000] : g_loss_adv [0.1855], e_loss_adv [0.1588],                l2_loss_x [0.0009], l2_loss_z [0.0023], g_e_loss [0.3766], dx_loss [0.1592], dz_loss [0.1574], d_loss [0.3167]\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'dataset': 'Torus',\n",
    "    'output_dir': './RoundTrip_623',\n",
    "    'x_dim': 3,\n",
    "    'z_dim': 2,\n",
    "    'lr': 0.0002,\n",
    "    'alpha': 10,\n",
    "    'beta': 1,\n",
    "    'gamma': 0,\n",
    "    'g_d_freq': 1,\n",
    "    'g_units': [512, 512, 512, 512, 512, 512, 512, 512, 512, 512],\n",
    "    'e_units': [256, 256, 256, 256, 256, 256, 256, 256, 256, 256],\n",
    "    'dz_units': [128, 128],\n",
    "    'dx_units': [256, 256, 256, 256],\n",
    "    'save_model': False,\n",
    "    'sd_x': 0.05,\n",
    "    'scale': 0.5,\n",
    "    'sample_size': 10000\n",
    "}\n",
    "\n",
    "model = pyrt.Roundtrip(params=params,random_seed=123)\n",
    "\n",
    "data = pd.read_csv('datasets/torus/B.csv', header=None).values\n",
    "data = np.array(data, dtype=np.float32)\n",
    "\n",
    "model.train(data=data, save_format='csv', n_iter=100000, batches_per_eval=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
